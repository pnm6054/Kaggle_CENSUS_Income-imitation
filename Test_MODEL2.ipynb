{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, QuantileTransformer, RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./data/train.csv')\n",
    "test = pd.read_csv('./data/test.csv')\n",
    "train_len = len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train['income']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all = pd.concat([train.iloc[:, :-1], test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## marital-status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bigBro\\Anaconda3\\envs\\deeplearning\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\Users\\bigBro\\Anaconda3\\envs\\deeplearning\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "data_all['marital-status'][data_all['marital-status']!=' Married-civ-spouse'] = 0\n",
    "data_all['marital-status'][data_all['marital-status']==' Married-civ-spouse'] = 1\n",
    "data_all['marital-status'] = data_all['marital-status'].astype('int64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bigBro\\Anaconda3\\envs\\deeplearning\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\Users\\bigBro\\Anaconda3\\envs\\deeplearning\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "data_all['relationship'][(data_all['relationship']!=' Husband') & (data_all['relationship']!=' Wife')] = 0\n",
    "data_all['relationship'][(data_all['relationship']==' Husband') | (data_all['relationship']==' Wife')] = 1\n",
    "data_all['relationship'] = data_all['relationship'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>Private</td>\n",
       "      <td>219199</td>\n",
       "      <td>7</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>39</td>\n",
       "      <td>Private</td>\n",
       "      <td>52978</td>\n",
       "      <td>10</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Other-service</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>1721</td>\n",
       "      <td>55</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>35</td>\n",
       "      <td>Private</td>\n",
       "      <td>196899</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>Asian-Pac-Islander</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>Haiti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>64</td>\n",
       "      <td>Private</td>\n",
       "      <td>135527</td>\n",
       "      <td>11</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Tech-support</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24</td>\n",
       "      <td>Private</td>\n",
       "      <td>60783</td>\n",
       "      <td>10</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Transport-moving</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>70</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age workclass  fnlwgt  education-num       marital-status  \\\n",
       "0   25   Private  219199              7             Divorced   \n",
       "1   39   Private   52978             10             Divorced   \n",
       "2   35   Private  196899             13        Never-married   \n",
       "3   64   Private  135527             11             Divorced   \n",
       "4   24   Private   60783             10   Married-civ-spouse   \n",
       "\n",
       "           occupation    relationship                 race      sex  \\\n",
       "0   Machine-op-inspct   Not-in-family                White     Male   \n",
       "1       Other-service   Not-in-family                White   Female   \n",
       "2   Handlers-cleaners   Not-in-family   Asian-Pac-Islander   Female   \n",
       "3        Tech-support   Not-in-family                White   Female   \n",
       "4    Transport-moving         Husband                White     Male   \n",
       "\n",
       "   capital-gain  capital-loss  hours-per-week  native-country  \n",
       "0             0             0              40   United-States  \n",
       "1             0          1721              55   United-States  \n",
       "2             0             0              50           Haiti  \n",
       "3             0             0              40   United-States  \n",
       "4             0             0              70   United-States  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## drop un-important column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data_all['education']\n",
    "del data_all['native-country']\n",
    "#del data_all['workclass']\n",
    "del data_all['no']\n",
    "del data_all['race']\n",
    "del one_hot_encoding['fnlwgt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## one-hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 48842 entries, 0 to 19536\n",
      "Data columns (total 34 columns):\n",
      "age                              48842 non-null int64\n",
      "fnlwgt                           48842 non-null int64\n",
      "education-num                    48842 non-null int64\n",
      "marital-status                   48842 non-null int64\n",
      "relationship                     48842 non-null int64\n",
      "capital-gain                     48842 non-null int64\n",
      "capital-loss                     48842 non-null int64\n",
      "hours-per-week                   48842 non-null int64\n",
      "workclass_ ?                     48842 non-null uint8\n",
      "workclass_ Federal-gov           48842 non-null uint8\n",
      "workclass_ Local-gov             48842 non-null uint8\n",
      "workclass_ Never-worked          48842 non-null uint8\n",
      "workclass_ Private               48842 non-null uint8\n",
      "workclass_ Self-emp-inc          48842 non-null uint8\n",
      "workclass_ Self-emp-not-inc      48842 non-null uint8\n",
      "workclass_ State-gov             48842 non-null uint8\n",
      "workclass_ Without-pay           48842 non-null uint8\n",
      "occupation_ ?                    48842 non-null uint8\n",
      "occupation_ Adm-clerical         48842 non-null uint8\n",
      "occupation_ Armed-Forces         48842 non-null uint8\n",
      "occupation_ Craft-repair         48842 non-null uint8\n",
      "occupation_ Exec-managerial      48842 non-null uint8\n",
      "occupation_ Farming-fishing      48842 non-null uint8\n",
      "occupation_ Handlers-cleaners    48842 non-null uint8\n",
      "occupation_ Machine-op-inspct    48842 non-null uint8\n",
      "occupation_ Other-service        48842 non-null uint8\n",
      "occupation_ Priv-house-serv      48842 non-null uint8\n",
      "occupation_ Prof-specialty       48842 non-null uint8\n",
      "occupation_ Protective-serv      48842 non-null uint8\n",
      "occupation_ Sales                48842 non-null uint8\n",
      "occupation_ Tech-support         48842 non-null uint8\n",
      "occupation_ Transport-moving     48842 non-null uint8\n",
      "sex_ Female                      48842 non-null uint8\n",
      "sex_ Male                        48842 non-null uint8\n",
      "dtypes: int64(8), uint8(26)\n",
      "memory usage: 4.6 MB\n"
     ]
    }
   ],
   "source": [
    "one_hot_encoding = pd.get_dummies(data_all)\n",
    "one_hot_encoding.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['age', 'fnlwgt', 'education-num', 'marital-status', 'relationship',\n",
       "       'capital-gain', 'capital-loss', 'hours-per-week', 'workclass_ ?',\n",
       "       'workclass_ Federal-gov', 'workclass_ Local-gov',\n",
       "       'workclass_ Never-worked', 'workclass_ Private',\n",
       "       'workclass_ Self-emp-inc', 'workclass_ Self-emp-not-inc',\n",
       "       'workclass_ State-gov', 'workclass_ Without-pay', 'occupation_ ?',\n",
       "       'occupation_ Adm-clerical', 'occupation_ Armed-Forces',\n",
       "       'occupation_ Craft-repair', 'occupation_ Exec-managerial',\n",
       "       'occupation_ Farming-fishing', 'occupation_ Handlers-cleaners',\n",
       "       'occupation_ Machine-op-inspct', 'occupation_ Other-service',\n",
       "       'occupation_ Priv-house-serv', 'occupation_ Prof-specialty',\n",
       "       'occupation_ Protective-serv', 'occupation_ Sales',\n",
       "       'occupation_ Tech-support', 'occupation_ Transport-moving',\n",
       "       'sex_ Female', 'sex_ Male'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_encoding.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "del one_hot_encoding['fnlwgt']\n",
    "del one_hot_encoding['occupation_ ?']\n",
    "del one_hot_encoding['occupation_ Adm-clerical']\n",
    "del one_hot_encoding['occupation_ Armed-Forces']\n",
    "del one_hot_encoding['occupation_ Craft-repair']\n",
    "del one_hot_encoding['occupation_ Farming-fishing']\n",
    "del one_hot_encoding['occupation_ Handlers-cleaners']\n",
    "del one_hot_encoding['occupation_ Machine-op-inspct']\n",
    "del one_hot_encoding['occupation_ Other-service']\n",
    "del one_hot_encoding['occupation_ Priv-house-serv']\n",
    "del one_hot_encoding['occupation_ Protective-serv']\n",
    "del one_hot_encoding['occupation_ Sales']\n",
    "del one_hot_encoding['occupation_ Tech-support']\n",
    "del one_hot_encoding['occupation_ Transport-moving']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = one_hot_encoding[:train_len]\n",
    "X_test = one_hot_encoding[train_len:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MinMaxScaler(copy=True, feature_range=(0, 1))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = MinMaxScaler()\n",
    "sc.fit(one_hot_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr = sc.transform(X)\n",
    "X_te = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC(C=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_tr, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ccc8a5425c5460cae118442db66c5b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "hyperparam_list = []\n",
    "C_list = [100000]\n",
    "G_list = [0.1, 0.01]\n",
    "for c in tqdm_notebook(C_list):\n",
    "    for g in G_list:\n",
    "        svc = SVC(C=c, gamma=g)\n",
    "        svc.fit(X_train, y_train)\n",
    "        score = cross_val_score(svc ,X_test, y_test).mean()\n",
    "        hyperparam_list.append([c, g, score])\n",
    "\n",
    "hp = pd.DataFrame(hyperparam_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-118-c5658c92c5b3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msvc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\svm\\base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m         \u001b[0mseed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrnd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'i'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 209\u001b[1;33m         \u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    210\u001b[0m         \u001b[1;31m# see comment on the other call to np.iinfo in this file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\svm\\base.py\u001b[0m in \u001b[0;36m_dense_fit\u001b[1;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[0m\n\u001b[0;32m    266\u001b[0m                 \u001b[0mcache_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcache_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcoef0\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m                 \u001b[0mgamma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 268\u001b[1;33m                 max_iter=self.max_iter, random_seed=random_seed)\n\u001b[0m\u001b[0;32m    269\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    270\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_warn_from_fit_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "svc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.84851586, 0.84544524, 0.84792627])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(svc ,X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>100000</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.847808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>100000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.847126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.845419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10000</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.845078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.844737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.844396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.844225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.836719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.834842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.828018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        0     1         2\n",
       "8  100000  0.10  0.847808\n",
       "9  100000  0.01  0.847126\n",
       "2     100  0.10  0.845419\n",
       "6   10000  0.10  0.845078\n",
       "4    1000  0.10  0.844737\n",
       "7   10000  0.01  0.844396\n",
       "5    1000  0.01  0.844225\n",
       "3     100  0.01  0.836719\n",
       "0      10  0.10  0.834842\n",
       "1      10  0.01  0.828018"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hp.sort_values(by=2, ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(random_state=42)\n",
    "param_grid_rf = { \n",
    "    'n_estimators': [500,700,1100,1300,1500],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'max_depth' : [13,15,17,19,21],\n",
    "    'criterion' :['gini', 'entropy']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-7716d2a94c21>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mCV_rfc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrfc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparam_grid_rf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mCV_rfc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    685\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    686\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 687\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    689\u001b[0m         \u001b[1;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1146\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1147\u001b[0m         \u001b[1;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1148\u001b[1;33m         \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1149\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params)\u001b[0m\n\u001b[0;32m    664\u001b[0m                                \u001b[1;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    665\u001b[0m                                in product(candidate_params,\n\u001b[1;32m--> 666\u001b[1;33m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[0;32m    667\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    668\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    922\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    923\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 924\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    925\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    926\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    757\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    758\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 759\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    760\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    761\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    714\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    715\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 716\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    717\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    718\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    180\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 182\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    183\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    547\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    548\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 549\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    550\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    551\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 225\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 225\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    512\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    513\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 514\u001b[1;33m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    515\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    516\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\ensemble\\forest.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    328\u001b[0m                     \u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrees\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    329\u001b[0m                     verbose=self.verbose, class_weight=self.class_weight)\n\u001b[1;32m--> 330\u001b[1;33m                 for i, t in enumerate(trees))\n\u001b[0m\u001b[0;32m    331\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    332\u001b[0m             \u001b[1;31m# Collect newly grown trees\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    922\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    923\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 924\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    925\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    926\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    757\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    758\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 759\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    760\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    761\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    714\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    715\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 716\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    717\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    718\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    180\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 182\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    183\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    547\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    548\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 549\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    550\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    551\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 225\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 225\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\ensemble\\forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight)\u001b[0m\n\u001b[0;32m    116\u001b[0m             \u001b[0mcurr_sample_weight\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'balanced'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 118\u001b[1;33m         \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    119\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m         \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\tree\\tree.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    814\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    815\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 816\u001b[1;33m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[0;32m    817\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\tree\\tree.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    378\u001b[0m                                            min_impurity_split)\n\u001b[0;32m    379\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 380\u001b[1;33m         \u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    381\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    382\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "CV_rfc = GridSearchCV(estimator=rfc, param_grid=param_grid_rf, cv= 5)\n",
    "CV_rfc.fit(X_train, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'criterion': 'gini', 'max_depth': 13, 'max_features': 'auto', 'n_estimators': 500}\n"
     ]
    }
   ],
   "source": [
    "print(CV_rfc.best_params_)\n",
    "model_rf = CV_rfc.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                       max_depth=13, max_features='auto', max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=500,\n",
       "                       n_jobs=None, oob_score=False, random_state=42, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_rf = RandomForestClassifier(random_state=42, criterion='gini', max_depth=13, max_features='auto',n_estimators=500)\n",
    "model_rf.fit(X_tr, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.84379159, 0.85810811, 0.85524161])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(model_rf, X_tr, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbc = xgb.XGBClassifier(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=0.9,\n",
       "              colsample_bynode=1, colsample_bytree=0.8, gamma=0,\n",
       "              learning_rate=0.1, max_delta_step=0, max_depth=8,\n",
       "              min_child_weight=10, missing=None, n_estimators=500, n_jobs=-1,\n",
       "              nthread=None, objective='binary:logistic', random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "              silent=None, subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid={'booster' :['gbtree'],\n",
    "                 'silent':[True],\n",
    "                 'max_depth':[5,6,8],\n",
    "                 'min_child_weight':[1,3,5],\n",
    "                 'gamma':[0,1,2,3],\n",
    "                 'nthread':[4],\n",
    "                 'colsample_bytree':[0.5,0.8],\n",
    "                 'colsample_bylevel':[0.9],\n",
    "                 'n_estimators':[50, 100, 200, 300],\n",
    "                 'objective':['binary:logistic'],\n",
    "                 'random_state':[2]}\n",
    "cv=KFold(n_splits=6, random_state=1)\n",
    "gcv=GridSearchCV(xgbc, param_grid=param_grid, cv=cv, scoring='f1', n_jobs=-1)\n",
    "\n",
    "gcv.fit(X_train, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(gcv.best_params_)\n",
    "model_xgbc = XGBClassifier(booster='gbtree', colsample_bylevel=1, colsample_bytree=0.5, gamma=0, max_depth=5, min_child_weight=1, n_estimators=500, objective='binary:logistic',random_state=42, silent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=0.9,\n",
       "              colsample_bynode=1, colsample_bytree=0.5, gamma=0,\n",
       "              learning_rate=0.1, max_delta_step=0, max_depth=5,\n",
       "              min_child_weight=1, missing=None, n_estimators=300, n_jobs=1,\n",
       "              nthread=None, objective='binary:logistic', random_state=2,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "              silent=True, subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_xgbc.fit(X_tr, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7201584907849622"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(model_xgbc, X_tr, y).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ada = AdaBoostClassifier(random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "             estimator=AdaBoostClassifier(algorithm='SAMME.R',\n",
       "                                          base_estimator=None,\n",
       "                                          learning_rate=1.0, n_estimators=50,\n",
       "                                          random_state=0),\n",
       "             iid='warn', n_jobs=-1, param_grid={'n_estimators': [16, 32, 64]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='f1', verbose=0)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid={'n_estimators':[16, 32, 64]}\n",
    "gcv_ada=GridSearchCV(model_ada, param_grid=param_grid, cv=5, scoring='f1', n_jobs=-1)\n",
    "\n",
    "gcv_ada.fit(X_tr, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ada = gcv_ada.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.2404545 , 0.85677723, 0.85135135])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(model_ada, X_tr, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LogisticRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score='raise-deprecating',\n",
       "             estimator=LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                          fit_intercept=True,\n",
       "                                          intercept_scaling=1, l1_ratio=None,\n",
       "                                          max_iter=100, multi_class='warn',\n",
       "                                          n_jobs=None, penalty='l2',\n",
       "                                          random_state=None, solver='warn',\n",
       "                                          tol=0.0001, verbose=0,\n",
       "                                          warm_start=False),\n",
       "             iid='warn', n_jobs=None,\n",
       "             param_grid={'C': array([1.e-03, 1.e-02, 1.e-01, 1.e+00, 1.e+01, 1.e+02, 1.e+03]),\n",
       "                         'penalty': ['l1', 'l2']},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid={\"C\":np.logspace(-3,3,7), \"penalty\":[\"l1\",\"l2\"]}# l1 lasso l2 ridge\n",
    "logreg=LogisticRegression()\n",
    "logreg_cv=GridSearchCV(logreg,grid,cv=10)\n",
    "logreg_cv.fit(X_tr,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tuned hpyerparameters :(best parameters)  {'C': 100.0, 'penalty': 'l2'}\n",
      "accuracy : 0.8493089916396519\n"
     ]
    }
   ],
   "source": [
    "print(\"tuned hpyerparameters :(best parameters) \",logreg_cv.best_params_)\n",
    "print(\"accuracy :\",logreg_cv.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = one_hot_encoding[:train_len]\n",
    "X_test = one_hot_encoding[train_len:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MinMaxScaler(copy=True, feature_range=(0, 1))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = MinMaxScaler()\n",
    "sc.fit(one_hot_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_D = sc.transform(X)\n",
    "X_test_D = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_td, X_val, y_td, y_val = train_test_split(X_train_D, y, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20513, 92)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_td.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(256, kernel_initializer='normal', activation='relu', input_shape=(92,)))\n",
    "model.add(layers.Dense(64, kernel_initializer='normal', activation='relu'))\n",
    "model.add(layers.Dense(32, kernel_initializer='normal', activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer ='rmsprop',loss='binary_crossentropy', metrics =['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20513 samples, validate on 8792 samples\n",
      "Epoch 1/500\n",
      "20513/20513 [==============================] - ETA: 5s - loss: 0.2577 - acc: 0.875 - ETA: 1s - loss: 0.2686 - acc: 0.875 - ETA: 0s - loss: 0.2744 - acc: 0.870 - ETA: 0s - loss: 0.2847 - acc: 0.861 - ETA: 0s - loss: 0.2779 - acc: 0.865 - ETA: 0s - loss: 0.2785 - acc: 0.866 - ETA: 0s - loss: 0.2765 - acc: 0.867 - ETA: 0s - loss: 0.2757 - acc: 0.868 - ETA: 0s - loss: 0.2758 - acc: 0.867 - ETA: 0s - loss: 0.2763 - acc: 0.868 - ETA: 0s - loss: 0.2771 - acc: 0.869 - ETA: 0s - loss: 0.2762 - acc: 0.870 - ETA: 0s - loss: 0.2767 - acc: 0.869 - ETA: 0s - loss: 0.2774 - acc: 0.869 - ETA: 0s - loss: 0.2783 - acc: 0.869 - ETA: 0s - loss: 0.2799 - acc: 0.867 - ETA: 0s - loss: 0.2793 - acc: 0.868 - ETA: 0s - loss: 0.2791 - acc: 0.869 - 1s 52us/step - loss: 0.2789 - acc: 0.8694 - val_loss: 0.4073 - val_acc: 0.8462\n",
      "Epoch 2/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.2869 - acc: 0.875 - ETA: 1s - loss: 0.2870 - acc: 0.867 - ETA: 0s - loss: 0.2831 - acc: 0.861 - ETA: 0s - loss: 0.2784 - acc: 0.864 - ETA: 0s - loss: 0.2744 - acc: 0.867 - ETA: 0s - loss: 0.2781 - acc: 0.868 - ETA: 0s - loss: 0.2799 - acc: 0.867 - ETA: 0s - loss: 0.2826 - acc: 0.866 - ETA: 0s - loss: 0.2826 - acc: 0.866 - ETA: 0s - loss: 0.2825 - acc: 0.867 - ETA: 0s - loss: 0.2799 - acc: 0.869 - ETA: 0s - loss: 0.2802 - acc: 0.868 - ETA: 0s - loss: 0.2823 - acc: 0.868 - ETA: 0s - loss: 0.2830 - acc: 0.867 - ETA: 0s - loss: 0.2819 - acc: 0.868 - ETA: 0s - loss: 0.2810 - acc: 0.868 - ETA: 0s - loss: 0.2811 - acc: 0.869 - ETA: 0s - loss: 0.2804 - acc: 0.869 - 1s 49us/step - loss: 0.2802 - acc: 0.8694 - val_loss: 0.4105 - val_acc: 0.8261\n",
      "Epoch 3/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.2757 - acc: 0.828 - ETA: 0s - loss: 0.2867 - acc: 0.859 - ETA: 0s - loss: 0.2702 - acc: 0.872 - ETA: 0s - loss: 0.2598 - acc: 0.879 - ETA: 0s - loss: 0.2602 - acc: 0.878 - ETA: 0s - loss: 0.2618 - acc: 0.877 - ETA: 0s - loss: 0.2608 - acc: 0.877 - ETA: 0s - loss: 0.2627 - acc: 0.876 - ETA: 0s - loss: 0.2680 - acc: 0.873 - ETA: 0s - loss: 0.2710 - acc: 0.871 - ETA: 0s - loss: 0.2716 - acc: 0.871 - ETA: 0s - loss: 0.2741 - acc: 0.870 - ETA: 0s - loss: 0.2754 - acc: 0.870 - ETA: 0s - loss: 0.2771 - acc: 0.870 - ETA: 0s - loss: 0.2787 - acc: 0.869 - ETA: 0s - loss: 0.2791 - acc: 0.869 - ETA: 0s - loss: 0.2804 - acc: 0.868 - ETA: 0s - loss: 0.2804 - acc: 0.868 - 1s 50us/step - loss: 0.2817 - acc: 0.8680 - val_loss: 0.3638 - val_acc: 0.8441\n",
      "Epoch 4/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.3033 - acc: 0.859 - ETA: 0s - loss: 0.2679 - acc: 0.879 - ETA: 0s - loss: 0.2761 - acc: 0.873 - ETA: 0s - loss: 0.2743 - acc: 0.876 - ETA: 0s - loss: 0.2734 - acc: 0.876 - ETA: 0s - loss: 0.2737 - acc: 0.875 - ETA: 0s - loss: 0.2790 - acc: 0.873 - ETA: 0s - loss: 0.2791 - acc: 0.873 - ETA: 0s - loss: 0.2766 - acc: 0.872 - ETA: 0s - loss: 0.2782 - acc: 0.871 - ETA: 0s - loss: 0.2805 - acc: 0.870 - ETA: 0s - loss: 0.2811 - acc: 0.870 - ETA: 0s - loss: 0.2819 - acc: 0.869 - ETA: 0s - loss: 0.2821 - acc: 0.869 - ETA: 0s - loss: 0.2803 - acc: 0.869 - ETA: 0s - loss: 0.2801 - acc: 0.869 - ETA: 0s - loss: 0.2795 - acc: 0.870 - ETA: 0s - loss: 0.2811 - acc: 0.869 - 1s 50us/step - loss: 0.2806 - acc: 0.8697 - val_loss: 0.3646 - val_acc: 0.8482\n",
      "Epoch 5/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.1838 - acc: 0.921 - ETA: 0s - loss: 0.2605 - acc: 0.878 - ETA: 0s - loss: 0.2664 - acc: 0.879 - ETA: 0s - loss: 0.2669 - acc: 0.878 - ETA: 0s - loss: 0.2688 - acc: 0.878 - ETA: 0s - loss: 0.2715 - acc: 0.874 - ETA: 0s - loss: 0.2765 - acc: 0.872 - ETA: 0s - loss: 0.2755 - acc: 0.873 - ETA: 0s - loss: 0.2761 - acc: 0.871 - ETA: 0s - loss: 0.2777 - acc: 0.871 - ETA: 0s - loss: 0.2769 - acc: 0.871 - ETA: 0s - loss: 0.2778 - acc: 0.870 - ETA: 0s - loss: 0.2787 - acc: 0.870 - ETA: 0s - loss: 0.2788 - acc: 0.870 - ETA: 0s - loss: 0.2807 - acc: 0.869 - ETA: 0s - loss: 0.2812 - acc: 0.869 - ETA: 0s - loss: 0.2813 - acc: 0.869 - ETA: 0s - loss: 0.2800 - acc: 0.869 - 1s 50us/step - loss: 0.2805 - acc: 0.8701 - val_loss: 0.3571 - val_acc: 0.8471\n",
      "Epoch 6/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.3199 - acc: 0.843 - ETA: 0s - loss: 0.2761 - acc: 0.881 - ETA: 0s - loss: 0.2672 - acc: 0.882 - ETA: 0s - loss: 0.2834 - acc: 0.871 - ETA: 0s - loss: 0.2815 - acc: 0.872 - ETA: 0s - loss: 0.2766 - acc: 0.874 - ETA: 0s - loss: 0.2817 - acc: 0.870 - ETA: 0s - loss: 0.2831 - acc: 0.870 - ETA: 0s - loss: 0.2829 - acc: 0.870 - ETA: 0s - loss: 0.2851 - acc: 0.868 - ETA: 0s - loss: 0.2821 - acc: 0.870 - ETA: 0s - loss: 0.2823 - acc: 0.870 - ETA: 0s - loss: 0.2823 - acc: 0.870 - ETA: 0s - loss: 0.2809 - acc: 0.870 - ETA: 0s - loss: 0.2808 - acc: 0.870 - ETA: 0s - loss: 0.2804 - acc: 0.870 - ETA: 0s - loss: 0.2803 - acc: 0.871 - ETA: 0s - loss: 0.2805 - acc: 0.871 - 1s 49us/step - loss: 0.2811 - acc: 0.8712 - val_loss: 0.3703 - val_acc: 0.8461\n",
      "Epoch 7/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.2607 - acc: 0.828 - ETA: 0s - loss: 0.2871 - acc: 0.877 - ETA: 0s - loss: 0.2801 - acc: 0.875 - ETA: 0s - loss: 0.2795 - acc: 0.874 - ETA: 0s - loss: 0.2855 - acc: 0.874 - ETA: 0s - loss: 0.2810 - acc: 0.874 - ETA: 0s - loss: 0.2819 - acc: 0.871 - ETA: 0s - loss: 0.2825 - acc: 0.871 - ETA: 0s - loss: 0.2818 - acc: 0.870 - ETA: 0s - loss: 0.2799 - acc: 0.871 - ETA: 0s - loss: 0.2785 - acc: 0.871 - ETA: 0s - loss: 0.2786 - acc: 0.871 - ETA: 0s - loss: 0.2793 - acc: 0.871 - ETA: 0s - loss: 0.2802 - acc: 0.871 - ETA: 0s - loss: 0.2787 - acc: 0.871 - ETA: 0s - loss: 0.2778 - acc: 0.872 - ETA: 0s - loss: 0.2789 - acc: 0.871 - ETA: 0s - loss: 0.2791 - acc: 0.871 - 1s 51us/step - loss: 0.2803 - acc: 0.8709 - val_loss: 0.3535 - val_acc: 0.8495\n",
      "Epoch 8/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.2581 - acc: 0.937 - ETA: 0s - loss: 0.2702 - acc: 0.861 - ETA: 0s - loss: 0.2716 - acc: 0.875 - ETA: 0s - loss: 0.2758 - acc: 0.871 - ETA: 0s - loss: 0.2737 - acc: 0.872 - ETA: 0s - loss: 0.2722 - acc: 0.874 - ETA: 0s - loss: 0.2718 - acc: 0.874 - ETA: 0s - loss: 0.2744 - acc: 0.872 - ETA: 0s - loss: 0.2727 - acc: 0.874 - ETA: 0s - loss: 0.2736 - acc: 0.872 - ETA: 0s - loss: 0.2751 - acc: 0.872 - ETA: 0s - loss: 0.2765 - acc: 0.872 - ETA: 0s - loss: 0.2766 - acc: 0.871 - ETA: 0s - loss: 0.2766 - acc: 0.871 - ETA: 0s - loss: 0.2772 - acc: 0.871 - ETA: 0s - loss: 0.2782 - acc: 0.871 - ETA: 0s - loss: 0.2796 - acc: 0.871 - ETA: 0s - loss: 0.2796 - acc: 0.871 - 1s 49us/step - loss: 0.2798 - acc: 0.8711 - val_loss: 0.3634 - val_acc: 0.8496\n",
      "Epoch 9/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.4071 - acc: 0.796 - ETA: 0s - loss: 0.2770 - acc: 0.870 - ETA: 0s - loss: 0.2669 - acc: 0.873 - ETA: 0s - loss: 0.2707 - acc: 0.873 - ETA: 0s - loss: 0.2696 - acc: 0.875 - ETA: 0s - loss: 0.2702 - acc: 0.875 - ETA: 0s - loss: 0.2750 - acc: 0.875 - ETA: 0s - loss: 0.2758 - acc: 0.874 - ETA: 0s - loss: 0.2774 - acc: 0.871 - ETA: 0s - loss: 0.2767 - acc: 0.871 - ETA: 0s - loss: 0.2765 - acc: 0.872 - ETA: 0s - loss: 0.2754 - acc: 0.873 - ETA: 0s - loss: 0.2757 - acc: 0.872 - ETA: 0s - loss: 0.2764 - acc: 0.872 - ETA: 0s - loss: 0.2784 - acc: 0.871 - ETA: 0s - loss: 0.2788 - acc: 0.871 - ETA: 0s - loss: 0.2794 - acc: 0.871 - ETA: 0s - loss: 0.2791 - acc: 0.870 - 1s 51us/step - loss: 0.2789 - acc: 0.8713 - val_loss: 0.3611 - val_acc: 0.8459\n",
      "Epoch 10/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.2764 - acc: 0.843 - ETA: 0s - loss: 0.3076 - acc: 0.855 - ETA: 0s - loss: 0.2979 - acc: 0.864 - ETA: 0s - loss: 0.2938 - acc: 0.862 - ETA: 0s - loss: 0.2890 - acc: 0.866 - ETA: 0s - loss: 0.2840 - acc: 0.870 - ETA: 0s - loss: 0.2822 - acc: 0.870 - ETA: 0s - loss: 0.2824 - acc: 0.873 - ETA: 0s - loss: 0.2818 - acc: 0.873 - ETA: 0s - loss: 0.2811 - acc: 0.873 - ETA: 0s - loss: 0.2784 - acc: 0.874 - ETA: 0s - loss: 0.2791 - acc: 0.873 - ETA: 0s - loss: 0.2782 - acc: 0.873 - ETA: 0s - loss: 0.2798 - acc: 0.872 - ETA: 0s - loss: 0.2804 - acc: 0.872 - ETA: 0s - loss: 0.2794 - acc: 0.872 - ETA: 0s - loss: 0.2783 - acc: 0.873 - ETA: 0s - loss: 0.2793 - acc: 0.872 - 1s 50us/step - loss: 0.2794 - acc: 0.8723 - val_loss: 0.3661 - val_acc: 0.8470\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.3039 - acc: 0.906 - ETA: 0s - loss: 0.2741 - acc: 0.882 - ETA: 0s - loss: 0.2799 - acc: 0.871 - ETA: 0s - loss: 0.2784 - acc: 0.872 - ETA: 0s - loss: 0.2769 - acc: 0.873 - ETA: 0s - loss: 0.2820 - acc: 0.868 - ETA: 0s - loss: 0.2791 - acc: 0.870 - ETA: 0s - loss: 0.2813 - acc: 0.869 - ETA: 0s - loss: 0.2803 - acc: 0.871 - ETA: 0s - loss: 0.2802 - acc: 0.869 - ETA: 0s - loss: 0.2799 - acc: 0.869 - ETA: 0s - loss: 0.2768 - acc: 0.871 - ETA: 0s - loss: 0.2757 - acc: 0.872 - ETA: 0s - loss: 0.2754 - acc: 0.873 - ETA: 0s - loss: 0.2773 - acc: 0.872 - ETA: 0s - loss: 0.2763 - acc: 0.872 - ETA: 0s - loss: 0.2771 - acc: 0.872 - ETA: 0s - loss: 0.2775 - acc: 0.871 - 1s 50us/step - loss: 0.2769 - acc: 0.8715 - val_loss: 0.3754 - val_acc: 0.8446\n",
      "Epoch 12/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.3906 - acc: 0.843 - ETA: 0s - loss: 0.2848 - acc: 0.877 - ETA: 0s - loss: 0.2862 - acc: 0.867 - ETA: 0s - loss: 0.2710 - acc: 0.876 - ETA: 0s - loss: 0.2746 - acc: 0.873 - ETA: 0s - loss: 0.2749 - acc: 0.875 - ETA: 0s - loss: 0.2800 - acc: 0.871 - ETA: 0s - loss: 0.2772 - acc: 0.873 - ETA: 0s - loss: 0.2755 - acc: 0.874 - ETA: 0s - loss: 0.2780 - acc: 0.874 - ETA: 0s - loss: 0.2741 - acc: 0.876 - ETA: 0s - loss: 0.2734 - acc: 0.876 - ETA: 0s - loss: 0.2743 - acc: 0.876 - ETA: 0s - loss: 0.2745 - acc: 0.875 - ETA: 0s - loss: 0.2753 - acc: 0.875 - ETA: 0s - loss: 0.2766 - acc: 0.874 - ETA: 0s - loss: 0.2769 - acc: 0.873 - ETA: 0s - loss: 0.2767 - acc: 0.873 - 1s 50us/step - loss: 0.2770 - acc: 0.8742 - val_loss: 0.3626 - val_acc: 0.8504\n",
      "Epoch 13/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.2284 - acc: 0.906 - ETA: 0s - loss: 0.2637 - acc: 0.882 - ETA: 0s - loss: 0.2772 - acc: 0.875 - ETA: 0s - loss: 0.2760 - acc: 0.874 - ETA: 0s - loss: 0.2725 - acc: 0.875 - ETA: 0s - loss: 0.2705 - acc: 0.875 - ETA: 0s - loss: 0.2720 - acc: 0.875 - ETA: 0s - loss: 0.2721 - acc: 0.876 - ETA: 0s - loss: 0.2724 - acc: 0.876 - ETA: 0s - loss: 0.2714 - acc: 0.877 - ETA: 0s - loss: 0.2740 - acc: 0.876 - ETA: 0s - loss: 0.2765 - acc: 0.874 - ETA: 0s - loss: 0.2761 - acc: 0.874 - ETA: 0s - loss: 0.2781 - acc: 0.873 - ETA: 0s - loss: 0.2774 - acc: 0.873 - ETA: 0s - loss: 0.2771 - acc: 0.873 - ETA: 0s - loss: 0.2779 - acc: 0.872 - ETA: 0s - loss: 0.2775 - acc: 0.871 - 1s 50us/step - loss: 0.2763 - acc: 0.8732 - val_loss: 0.3684 - val_acc: 0.8501\n",
      "Epoch 14/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.2544 - acc: 0.890 - ETA: 0s - loss: 0.2849 - acc: 0.866 - ETA: 0s - loss: 0.2847 - acc: 0.862 - ETA: 0s - loss: 0.2798 - acc: 0.869 - ETA: 0s - loss: 0.2832 - acc: 0.867 - ETA: 0s - loss: 0.2797 - acc: 0.867 - ETA: 0s - loss: 0.2813 - acc: 0.868 - ETA: 0s - loss: 0.2804 - acc: 0.870 - ETA: 0s - loss: 0.2784 - acc: 0.870 - ETA: 0s - loss: 0.2785 - acc: 0.871 - ETA: 0s - loss: 0.2747 - acc: 0.872 - ETA: 0s - loss: 0.2749 - acc: 0.873 - ETA: 0s - loss: 0.2731 - acc: 0.874 - ETA: 0s - loss: 0.2743 - acc: 0.874 - ETA: 0s - loss: 0.2739 - acc: 0.875 - ETA: 0s - loss: 0.2739 - acc: 0.875 - ETA: 0s - loss: 0.2744 - acc: 0.875 - ETA: 0s - loss: 0.2750 - acc: 0.874 - 1s 50us/step - loss: 0.2752 - acc: 0.8748 - val_loss: 0.3651 - val_acc: 0.8486\n",
      "Epoch 15/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.3245 - acc: 0.875 - ETA: 0s - loss: 0.2713 - acc: 0.868 - ETA: 0s - loss: 0.2643 - acc: 0.870 - ETA: 0s - loss: 0.2654 - acc: 0.874 - ETA: 0s - loss: 0.2698 - acc: 0.876 - ETA: 0s - loss: 0.2679 - acc: 0.876 - ETA: 0s - loss: 0.2691 - acc: 0.876 - ETA: 0s - loss: 0.2683 - acc: 0.878 - ETA: 0s - loss: 0.2675 - acc: 0.877 - ETA: 0s - loss: 0.2704 - acc: 0.876 - ETA: 0s - loss: 0.2710 - acc: 0.874 - ETA: 0s - loss: 0.2701 - acc: 0.874 - ETA: 0s - loss: 0.2707 - acc: 0.874 - ETA: 0s - loss: 0.2705 - acc: 0.874 - ETA: 0s - loss: 0.2742 - acc: 0.872 - ETA: 0s - loss: 0.2747 - acc: 0.872 - ETA: 0s - loss: 0.2746 - acc: 0.873 - ETA: 0s - loss: 0.2742 - acc: 0.873 - 1s 50us/step - loss: 0.2742 - acc: 0.8735 - val_loss: 0.3745 - val_acc: 0.8425\n",
      "Epoch 16/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.4784 - acc: 0.843 - ETA: 0s - loss: 0.3077 - acc: 0.868 - ETA: 0s - loss: 0.2813 - acc: 0.876 - ETA: 0s - loss: 0.2828 - acc: 0.874 - ETA: 0s - loss: 0.2817 - acc: 0.873 - ETA: 0s - loss: 0.2771 - acc: 0.873 - ETA: 0s - loss: 0.2781 - acc: 0.873 - ETA: 0s - loss: 0.2777 - acc: 0.872 - ETA: 0s - loss: 0.2768 - acc: 0.874 - ETA: 0s - loss: 0.2776 - acc: 0.874 - ETA: 0s - loss: 0.2767 - acc: 0.874 - ETA: 0s - loss: 0.2763 - acc: 0.874 - ETA: 0s - loss: 0.2749 - acc: 0.874 - ETA: 0s - loss: 0.2762 - acc: 0.873 - ETA: 0s - loss: 0.2741 - acc: 0.875 - ETA: 0s - loss: 0.2730 - acc: 0.875 - ETA: 0s - loss: 0.2718 - acc: 0.876 - ETA: 0s - loss: 0.2733 - acc: 0.874 - ETA: 0s - loss: 0.2734 - acc: 0.874 - 1s 51us/step - loss: 0.2735 - acc: 0.8746 - val_loss: 0.3696 - val_acc: 0.8508\n",
      "Epoch 17/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.2228 - acc: 0.937 - ETA: 0s - loss: 0.2629 - acc: 0.882 - ETA: 0s - loss: 0.2674 - acc: 0.877 - ETA: 0s - loss: 0.2642 - acc: 0.881 - ETA: 0s - loss: 0.2652 - acc: 0.880 - ETA: 0s - loss: 0.2689 - acc: 0.877 - ETA: 0s - loss: 0.2719 - acc: 0.877 - ETA: 0s - loss: 0.2740 - acc: 0.874 - ETA: 0s - loss: 0.2712 - acc: 0.876 - ETA: 0s - loss: 0.2679 - acc: 0.877 - ETA: 0s - loss: 0.2701 - acc: 0.875 - ETA: 0s - loss: 0.2697 - acc: 0.875 - ETA: 0s - loss: 0.2687 - acc: 0.876 - ETA: 0s - loss: 0.2688 - acc: 0.877 - ETA: 0s - loss: 0.2708 - acc: 0.876 - ETA: 0s - loss: 0.2712 - acc: 0.875 - ETA: 0s - loss: 0.2723 - acc: 0.875 - ETA: 0s - loss: 0.2734 - acc: 0.874 - 1s 50us/step - loss: 0.2739 - acc: 0.8745 - val_loss: 0.3709 - val_acc: 0.8425\n",
      "Epoch 18/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.2737 - acc: 0.906 - ETA: 1s - loss: 0.2414 - acc: 0.897 - ETA: 0s - loss: 0.2666 - acc: 0.880 - ETA: 0s - loss: 0.2684 - acc: 0.875 - ETA: 0s - loss: 0.2696 - acc: 0.874 - ETA: 0s - loss: 0.2722 - acc: 0.872 - ETA: 0s - loss: 0.2739 - acc: 0.872 - ETA: 0s - loss: 0.2747 - acc: 0.873 - ETA: 0s - loss: 0.2723 - acc: 0.875 - ETA: 0s - loss: 0.2710 - acc: 0.877 - ETA: 0s - loss: 0.2698 - acc: 0.878 - ETA: 0s - loss: 0.2708 - acc: 0.878 - ETA: 0s - loss: 0.2716 - acc: 0.877 - ETA: 0s - loss: 0.2723 - acc: 0.877 - ETA: 0s - loss: 0.2729 - acc: 0.876 - ETA: 0s - loss: 0.2727 - acc: 0.876 - ETA: 0s - loss: 0.2724 - acc: 0.876 - ETA: 0s - loss: 0.2721 - acc: 0.876 - 1s 50us/step - loss: 0.2722 - acc: 0.8769 - val_loss: 0.3737 - val_acc: 0.8536\n",
      "Epoch 19/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.2518 - acc: 0.921 - ETA: 0s - loss: 0.2517 - acc: 0.884 - ETA: 0s - loss: 0.2597 - acc: 0.880 - ETA: 0s - loss: 0.2532 - acc: 0.881 - ETA: 0s - loss: 0.2580 - acc: 0.882 - ETA: 0s - loss: 0.2607 - acc: 0.878 - ETA: 0s - loss: 0.2605 - acc: 0.878 - ETA: 0s - loss: 0.2642 - acc: 0.877 - ETA: 0s - loss: 0.2662 - acc: 0.877 - ETA: 0s - loss: 0.2660 - acc: 0.878 - ETA: 0s - loss: 0.2672 - acc: 0.877 - ETA: 0s - loss: 0.2681 - acc: 0.876 - ETA: 0s - loss: 0.2660 - acc: 0.877 - ETA: 0s - loss: 0.2669 - acc: 0.877 - ETA: 0s - loss: 0.2671 - acc: 0.876 - ETA: 0s - loss: 0.2673 - acc: 0.877 - ETA: 0s - loss: 0.2699 - acc: 0.876 - ETA: 0s - loss: 0.2716 - acc: 0.874 - 1s 51us/step - loss: 0.2714 - acc: 0.8743 - val_loss: 0.3852 - val_acc: 0.8484\n",
      "Epoch 20/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.3045 - acc: 0.828 - ETA: 0s - loss: 0.2684 - acc: 0.875 - ETA: 0s - loss: 0.2679 - acc: 0.876 - ETA: 0s - loss: 0.2672 - acc: 0.877 - ETA: 0s - loss: 0.2691 - acc: 0.875 - ETA: 0s - loss: 0.2676 - acc: 0.875 - ETA: 0s - loss: 0.2652 - acc: 0.876 - ETA: 0s - loss: 0.2651 - acc: 0.877 - ETA: 0s - loss: 0.2650 - acc: 0.878 - ETA: 0s - loss: 0.2653 - acc: 0.878 - ETA: 0s - loss: 0.2669 - acc: 0.876 - ETA: 0s - loss: 0.2676 - acc: 0.876 - ETA: 0s - loss: 0.2701 - acc: 0.875 - ETA: 0s - loss: 0.2712 - acc: 0.874 - ETA: 0s - loss: 0.2710 - acc: 0.875 - ETA: 0s - loss: 0.2708 - acc: 0.875 - ETA: 0s - loss: 0.2706 - acc: 0.875 - ETA: 0s - loss: 0.2715 - acc: 0.874 - 1s 51us/step - loss: 0.2712 - acc: 0.8754 - val_loss: 0.3741 - val_acc: 0.8479\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.1798 - acc: 0.906 - ETA: 0s - loss: 0.2569 - acc: 0.882 - ETA: 0s - loss: 0.2579 - acc: 0.877 - ETA: 0s - loss: 0.2640 - acc: 0.876 - ETA: 0s - loss: 0.2675 - acc: 0.875 - ETA: 0s - loss: 0.2707 - acc: 0.874 - ETA: 0s - loss: 0.2687 - acc: 0.875 - ETA: 0s - loss: 0.2717 - acc: 0.874 - ETA: 0s - loss: 0.2687 - acc: 0.876 - ETA: 0s - loss: 0.2691 - acc: 0.876 - ETA: 0s - loss: 0.2720 - acc: 0.876 - ETA: 0s - loss: 0.2717 - acc: 0.875 - ETA: 0s - loss: 0.2728 - acc: 0.875 - ETA: 0s - loss: 0.2708 - acc: 0.876 - ETA: 0s - loss: 0.2699 - acc: 0.877 - ETA: 0s - loss: 0.2708 - acc: 0.876 - ETA: 0s - loss: 0.2701 - acc: 0.876 - ETA: 0s - loss: 0.2706 - acc: 0.876 - 1s 49us/step - loss: 0.2710 - acc: 0.8769 - val_loss: 0.3780 - val_acc: 0.8371\n",
      "Epoch 22/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.3667 - acc: 0.828 - ETA: 0s - loss: 0.2472 - acc: 0.891 - ETA: 0s - loss: 0.2584 - acc: 0.885 - ETA: 0s - loss: 0.2679 - acc: 0.881 - ETA: 0s - loss: 0.2698 - acc: 0.876 - ETA: 0s - loss: 0.2690 - acc: 0.875 - ETA: 0s - loss: 0.2685 - acc: 0.873 - ETA: 0s - loss: 0.2693 - acc: 0.874 - ETA: 0s - loss: 0.2693 - acc: 0.874 - ETA: 0s - loss: 0.2660 - acc: 0.875 - ETA: 0s - loss: 0.2666 - acc: 0.875 - ETA: 0s - loss: 0.2694 - acc: 0.875 - ETA: 0s - loss: 0.2685 - acc: 0.876 - ETA: 0s - loss: 0.2674 - acc: 0.877 - ETA: 0s - loss: 0.2678 - acc: 0.878 - ETA: 0s - loss: 0.2673 - acc: 0.877 - ETA: 0s - loss: 0.2691 - acc: 0.876 - ETA: 0s - loss: 0.2690 - acc: 0.877 - 1s 50us/step - loss: 0.2677 - acc: 0.8783 - val_loss: 0.3766 - val_acc: 0.8508\n",
      "Epoch 23/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.3339 - acc: 0.843 - ETA: 0s - loss: 0.2482 - acc: 0.885 - ETA: 0s - loss: 0.2633 - acc: 0.874 - ETA: 0s - loss: 0.2678 - acc: 0.872 - ETA: 0s - loss: 0.2617 - acc: 0.875 - ETA: 0s - loss: 0.2664 - acc: 0.874 - ETA: 0s - loss: 0.2704 - acc: 0.873 - ETA: 0s - loss: 0.2681 - acc: 0.874 - ETA: 0s - loss: 0.2666 - acc: 0.876 - ETA: 0s - loss: 0.2684 - acc: 0.875 - ETA: 0s - loss: 0.2672 - acc: 0.876 - ETA: 0s - loss: 0.2669 - acc: 0.877 - ETA: 0s - loss: 0.2674 - acc: 0.876 - ETA: 0s - loss: 0.2671 - acc: 0.877 - ETA: 0s - loss: 0.2687 - acc: 0.877 - ETA: 0s - loss: 0.2676 - acc: 0.877 - ETA: 0s - loss: 0.2678 - acc: 0.877 - ETA: 0s - loss: 0.2690 - acc: 0.877 - 1s 49us/step - loss: 0.2687 - acc: 0.8780 - val_loss: 0.3721 - val_acc: 0.8430\n",
      "Epoch 24/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.2277 - acc: 0.890 - ETA: 0s - loss: 0.2657 - acc: 0.878 - ETA: 0s - loss: 0.2716 - acc: 0.873 - ETA: 0s - loss: 0.2689 - acc: 0.873 - ETA: 0s - loss: 0.2638 - acc: 0.880 - ETA: 0s - loss: 0.2572 - acc: 0.885 - ETA: 0s - loss: 0.2559 - acc: 0.884 - ETA: 0s - loss: 0.2547 - acc: 0.885 - ETA: 0s - loss: 0.2571 - acc: 0.883 - ETA: 0s - loss: 0.2558 - acc: 0.883 - ETA: 0s - loss: 0.2581 - acc: 0.883 - ETA: 0s - loss: 0.2574 - acc: 0.884 - ETA: 0s - loss: 0.2594 - acc: 0.884 - ETA: 0s - loss: 0.2604 - acc: 0.883 - ETA: 0s - loss: 0.2620 - acc: 0.881 - ETA: 0s - loss: 0.2620 - acc: 0.881 - ETA: 0s - loss: 0.2625 - acc: 0.880 - ETA: 0s - loss: 0.2659 - acc: 0.877 - 1s 51us/step - loss: 0.2667 - acc: 0.8773 - val_loss: 0.4077 - val_acc: 0.8317\n",
      "Epoch 25/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.2593 - acc: 0.875 - ETA: 0s - loss: 0.2425 - acc: 0.889 - ETA: 0s - loss: 0.2415 - acc: 0.884 - ETA: 0s - loss: 0.2516 - acc: 0.879 - ETA: 0s - loss: 0.2531 - acc: 0.880 - ETA: 0s - loss: 0.2540 - acc: 0.880 - ETA: 0s - loss: 0.2541 - acc: 0.880 - ETA: 0s - loss: 0.2554 - acc: 0.880 - ETA: 0s - loss: 0.2597 - acc: 0.879 - ETA: 0s - loss: 0.2596 - acc: 0.879 - ETA: 0s - loss: 0.2582 - acc: 0.880 - ETA: 0s - loss: 0.2614 - acc: 0.878 - ETA: 0s - loss: 0.2625 - acc: 0.878 - ETA: 0s - loss: 0.2626 - acc: 0.879 - ETA: 0s - loss: 0.2632 - acc: 0.879 - ETA: 0s - loss: 0.2635 - acc: 0.878 - ETA: 0s - loss: 0.2642 - acc: 0.878 - ETA: 0s - loss: 0.2666 - acc: 0.877 - 1s 51us/step - loss: 0.2662 - acc: 0.8777 - val_loss: 0.3660 - val_acc: 0.8493\n",
      "Epoch 26/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.2376 - acc: 0.890 - ETA: 0s - loss: 0.2534 - acc: 0.881 - ETA: 0s - loss: 0.2577 - acc: 0.879 - ETA: 0s - loss: 0.2563 - acc: 0.880 - ETA: 0s - loss: 0.2568 - acc: 0.880 - ETA: 0s - loss: 0.2599 - acc: 0.879 - ETA: 0s - loss: 0.2606 - acc: 0.879 - ETA: 0s - loss: 0.2628 - acc: 0.878 - ETA: 0s - loss: 0.2638 - acc: 0.879 - ETA: 0s - loss: 0.2645 - acc: 0.879 - ETA: 0s - loss: 0.2659 - acc: 0.878 - ETA: 0s - loss: 0.2649 - acc: 0.878 - ETA: 0s - loss: 0.2668 - acc: 0.877 - ETA: 0s - loss: 0.2669 - acc: 0.877 - ETA: 0s - loss: 0.2654 - acc: 0.878 - ETA: 0s - loss: 0.2654 - acc: 0.877 - ETA: 0s - loss: 0.2659 - acc: 0.877 - ETA: 0s - loss: 0.2656 - acc: 0.877 - 1s 50us/step - loss: 0.2660 - acc: 0.8776 - val_loss: 0.3763 - val_acc: 0.8500\n",
      "Epoch 27/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.2671 - acc: 0.875 - ETA: 0s - loss: 0.2651 - acc: 0.879 - ETA: 0s - loss: 0.2573 - acc: 0.881 - ETA: 0s - loss: 0.2550 - acc: 0.885 - ETA: 0s - loss: 0.2658 - acc: 0.880 - ETA: 0s - loss: 0.2635 - acc: 0.879 - ETA: 0s - loss: 0.2621 - acc: 0.880 - ETA: 0s - loss: 0.2624 - acc: 0.877 - ETA: 0s - loss: 0.2662 - acc: 0.876 - ETA: 0s - loss: 0.2657 - acc: 0.876 - ETA: 0s - loss: 0.2629 - acc: 0.878 - ETA: 0s - loss: 0.2658 - acc: 0.877 - ETA: 0s - loss: 0.2650 - acc: 0.877 - ETA: 0s - loss: 0.2649 - acc: 0.878 - ETA: 0s - loss: 0.2656 - acc: 0.878 - ETA: 0s - loss: 0.2671 - acc: 0.877 - ETA: 0s - loss: 0.2666 - acc: 0.877 - ETA: 0s - loss: 0.2663 - acc: 0.877 - 1s 50us/step - loss: 0.2658 - acc: 0.8781 - val_loss: 0.3707 - val_acc: 0.8513\n",
      "Epoch 28/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.1413 - acc: 0.921 - ETA: 0s - loss: 0.2467 - acc: 0.889 - ETA: 0s - loss: 0.2533 - acc: 0.889 - ETA: 0s - loss: 0.2580 - acc: 0.885 - ETA: 0s - loss: 0.2615 - acc: 0.880 - ETA: 0s - loss: 0.2608 - acc: 0.880 - ETA: 0s - loss: 0.2622 - acc: 0.878 - ETA: 0s - loss: 0.2604 - acc: 0.878 - ETA: 0s - loss: 0.2634 - acc: 0.878 - ETA: 0s - loss: 0.2640 - acc: 0.878 - ETA: 0s - loss: 0.2640 - acc: 0.879 - ETA: 0s - loss: 0.2653 - acc: 0.879 - ETA: 0s - loss: 0.2653 - acc: 0.878 - ETA: 0s - loss: 0.2648 - acc: 0.878 - ETA: 0s - loss: 0.2634 - acc: 0.879 - ETA: 0s - loss: 0.2646 - acc: 0.879 - ETA: 0s - loss: 0.2637 - acc: 0.880 - ETA: 0s - loss: 0.2631 - acc: 0.880 - 1s 51us/step - loss: 0.2642 - acc: 0.8798 - val_loss: 0.3730 - val_acc: 0.8472\n",
      "Epoch 29/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.2633 - acc: 0.875 - ETA: 0s - loss: 0.2387 - acc: 0.898 - ETA: 0s - loss: 0.2404 - acc: 0.894 - ETA: 0s - loss: 0.2427 - acc: 0.889 - ETA: 0s - loss: 0.2447 - acc: 0.887 - ETA: 0s - loss: 0.2444 - acc: 0.887 - ETA: 0s - loss: 0.2445 - acc: 0.886 - ETA: 0s - loss: 0.2504 - acc: 0.884 - ETA: 0s - loss: 0.2515 - acc: 0.883 - ETA: 0s - loss: 0.2565 - acc: 0.881 - ETA: 0s - loss: 0.2560 - acc: 0.881 - ETA: 0s - loss: 0.2547 - acc: 0.881 - ETA: 0s - loss: 0.2561 - acc: 0.881 - ETA: 0s - loss: 0.2562 - acc: 0.882 - ETA: 0s - loss: 0.2578 - acc: 0.882 - ETA: 0s - loss: 0.2593 - acc: 0.881 - ETA: 0s - loss: 0.2598 - acc: 0.881 - ETA: 0s - loss: 0.2615 - acc: 0.880 - 1s 50us/step - loss: 0.2633 - acc: 0.8792 - val_loss: 0.3809 - val_acc: 0.8494\n",
      "Epoch 30/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.2323 - acc: 0.890 - ETA: 0s - loss: 0.2513 - acc: 0.879 - ETA: 0s - loss: 0.2512 - acc: 0.883 - ETA: 0s - loss: 0.2582 - acc: 0.881 - ETA: 0s - loss: 0.2619 - acc: 0.880 - ETA: 0s - loss: 0.2620 - acc: 0.880 - ETA: 0s - loss: 0.2645 - acc: 0.877 - ETA: 0s - loss: 0.2593 - acc: 0.880 - ETA: 0s - loss: 0.2572 - acc: 0.881 - ETA: 0s - loss: 0.2585 - acc: 0.881 - ETA: 0s - loss: 0.2594 - acc: 0.881 - ETA: 0s - loss: 0.2592 - acc: 0.880 - ETA: 0s - loss: 0.2603 - acc: 0.880 - ETA: 0s - loss: 0.2596 - acc: 0.880 - ETA: 0s - loss: 0.2594 - acc: 0.880 - ETA: 0s - loss: 0.2587 - acc: 0.881 - ETA: 0s - loss: 0.2595 - acc: 0.881 - ETA: 0s - loss: 0.2607 - acc: 0.880 - 1s 50us/step - loss: 0.2626 - acc: 0.8803 - val_loss: 0.3992 - val_acc: 0.8414\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1999 - acc: 0.890 - ETA: 0s - loss: 0.2529 - acc: 0.893 - ETA: 0s - loss: 0.2642 - acc: 0.879 - ETA: 0s - loss: 0.2603 - acc: 0.881 - ETA: 0s - loss: 0.2589 - acc: 0.883 - ETA: 0s - loss: 0.2601 - acc: 0.881 - ETA: 0s - loss: 0.2626 - acc: 0.880 - ETA: 0s - loss: 0.2621 - acc: 0.880 - ETA: 0s - loss: 0.2609 - acc: 0.881 - ETA: 0s - loss: 0.2618 - acc: 0.880 - ETA: 0s - loss: 0.2625 - acc: 0.880 - ETA: 0s - loss: 0.2583 - acc: 0.881 - ETA: 0s - loss: 0.2592 - acc: 0.881 - ETA: 0s - loss: 0.2596 - acc: 0.881 - ETA: 0s - loss: 0.2603 - acc: 0.881 - ETA: 0s - loss: 0.2612 - acc: 0.881 - ETA: 0s - loss: 0.2628 - acc: 0.880 - ETA: 0s - loss: 0.2624 - acc: 0.880 - 1s 50us/step - loss: 0.2615 - acc: 0.8816 - val_loss: 0.3939 - val_acc: 0.8479\n",
      "Epoch 32/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1994 - acc: 0.906 - ETA: 0s - loss: 0.2541 - acc: 0.878 - ETA: 0s - loss: 0.2641 - acc: 0.874 - ETA: 0s - loss: 0.2627 - acc: 0.874 - ETA: 0s - loss: 0.2576 - acc: 0.878 - ETA: 0s - loss: 0.2558 - acc: 0.880 - ETA: 0s - loss: 0.2568 - acc: 0.879 - ETA: 0s - loss: 0.2543 - acc: 0.881 - ETA: 0s - loss: 0.2551 - acc: 0.880 - ETA: 0s - loss: 0.2585 - acc: 0.880 - ETA: 0s - loss: 0.2596 - acc: 0.880 - ETA: 0s - loss: 0.2585 - acc: 0.881 - ETA: 0s - loss: 0.2587 - acc: 0.881 - ETA: 0s - loss: 0.2587 - acc: 0.881 - ETA: 0s - loss: 0.2592 - acc: 0.881 - ETA: 0s - loss: 0.2592 - acc: 0.880 - ETA: 0s - loss: 0.2597 - acc: 0.881 - ETA: 0s - loss: 0.2601 - acc: 0.881 - 1s 50us/step - loss: 0.2613 - acc: 0.8803 - val_loss: 0.3738 - val_acc: 0.8509\n",
      "Epoch 33/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.2437 - acc: 0.875 - ETA: 0s - loss: 0.2432 - acc: 0.889 - ETA: 0s - loss: 0.2475 - acc: 0.884 - ETA: 0s - loss: 0.2494 - acc: 0.885 - ETA: 0s - loss: 0.2478 - acc: 0.887 - ETA: 0s - loss: 0.2534 - acc: 0.886 - ETA: 0s - loss: 0.2546 - acc: 0.885 - ETA: 0s - loss: 0.2570 - acc: 0.885 - ETA: 0s - loss: 0.2557 - acc: 0.885 - ETA: 0s - loss: 0.2555 - acc: 0.884 - ETA: 0s - loss: 0.2569 - acc: 0.882 - ETA: 0s - loss: 0.2599 - acc: 0.880 - ETA: 0s - loss: 0.2607 - acc: 0.880 - ETA: 0s - loss: 0.2608 - acc: 0.881 - ETA: 0s - loss: 0.2609 - acc: 0.881 - ETA: 0s - loss: 0.2627 - acc: 0.880 - ETA: 0s - loss: 0.2608 - acc: 0.880 - ETA: 0s - loss: 0.2599 - acc: 0.880 - 1s 50us/step - loss: 0.2599 - acc: 0.8804 - val_loss: 0.3866 - val_acc: 0.8490\n",
      "Epoch 34/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.2894 - acc: 0.875 - ETA: 0s - loss: 0.2714 - acc: 0.880 - ETA: 0s - loss: 0.2578 - acc: 0.884 - ETA: 0s - loss: 0.2707 - acc: 0.879 - ETA: 0s - loss: 0.2636 - acc: 0.882 - ETA: 0s - loss: 0.2646 - acc: 0.883 - ETA: 0s - loss: 0.2620 - acc: 0.883 - ETA: 0s - loss: 0.2592 - acc: 0.883 - ETA: 0s - loss: 0.2562 - acc: 0.884 - ETA: 0s - loss: 0.2569 - acc: 0.882 - ETA: 0s - loss: 0.2556 - acc: 0.883 - ETA: 0s - loss: 0.2560 - acc: 0.883 - ETA: 0s - loss: 0.2554 - acc: 0.884 - ETA: 0s - loss: 0.2560 - acc: 0.884 - ETA: 0s - loss: 0.2574 - acc: 0.883 - ETA: 0s - loss: 0.2587 - acc: 0.882 - ETA: 0s - loss: 0.2586 - acc: 0.882 - ETA: 0s - loss: 0.2581 - acc: 0.882 - 1s 49us/step - loss: 0.2578 - acc: 0.8824 - val_loss: 0.4015 - val_acc: 0.8503\n",
      "Epoch 35/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.3204 - acc: 0.843 - ETA: 0s - loss: 0.2539 - acc: 0.891 - ETA: 0s - loss: 0.2546 - acc: 0.889 - ETA: 0s - loss: 0.2555 - acc: 0.890 - ETA: 0s - loss: 0.2541 - acc: 0.889 - ETA: 0s - loss: 0.2520 - acc: 0.888 - ETA: 0s - loss: 0.2513 - acc: 0.888 - ETA: 0s - loss: 0.2526 - acc: 0.889 - ETA: 0s - loss: 0.2550 - acc: 0.888 - ETA: 0s - loss: 0.2576 - acc: 0.887 - ETA: 0s - loss: 0.2582 - acc: 0.886 - ETA: 0s - loss: 0.2587 - acc: 0.885 - ETA: 0s - loss: 0.2590 - acc: 0.884 - ETA: 0s - loss: 0.2588 - acc: 0.884 - ETA: 0s - loss: 0.2584 - acc: 0.884 - ETA: 0s - loss: 0.2590 - acc: 0.883 - ETA: 0s - loss: 0.2574 - acc: 0.884 - ETA: 0s - loss: 0.2584 - acc: 0.883 - 1s 49us/step - loss: 0.2588 - acc: 0.8828 - val_loss: 0.4021 - val_acc: 0.8409\n",
      "Epoch 36/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1799 - acc: 0.906 - ETA: 0s - loss: 0.2526 - acc: 0.892 - ETA: 0s - loss: 0.2503 - acc: 0.888 - ETA: 0s - loss: 0.2452 - acc: 0.894 - ETA: 0s - loss: 0.2501 - acc: 0.890 - ETA: 0s - loss: 0.2526 - acc: 0.886 - ETA: 0s - loss: 0.2511 - acc: 0.887 - ETA: 0s - loss: 0.2529 - acc: 0.885 - ETA: 0s - loss: 0.2541 - acc: 0.885 - ETA: 0s - loss: 0.2536 - acc: 0.885 - ETA: 0s - loss: 0.2544 - acc: 0.884 - ETA: 0s - loss: 0.2547 - acc: 0.884 - ETA: 0s - loss: 0.2554 - acc: 0.883 - ETA: 0s - loss: 0.2569 - acc: 0.883 - ETA: 0s - loss: 0.2581 - acc: 0.882 - ETA: 0s - loss: 0.2581 - acc: 0.882 - ETA: 0s - loss: 0.2572 - acc: 0.882 - ETA: 0s - loss: 0.2577 - acc: 0.881 - 1s 50us/step - loss: 0.2579 - acc: 0.8823 - val_loss: 0.3977 - val_acc: 0.8459\n",
      "Epoch 37/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1981 - acc: 0.953 - ETA: 0s - loss: 0.2482 - acc: 0.889 - ETA: 0s - loss: 0.2456 - acc: 0.889 - ETA: 0s - loss: 0.2484 - acc: 0.884 - ETA: 0s - loss: 0.2546 - acc: 0.880 - ETA: 0s - loss: 0.2526 - acc: 0.880 - ETA: 0s - loss: 0.2542 - acc: 0.881 - ETA: 0s - loss: 0.2551 - acc: 0.879 - ETA: 0s - loss: 0.2558 - acc: 0.878 - ETA: 0s - loss: 0.2599 - acc: 0.877 - ETA: 0s - loss: 0.2597 - acc: 0.878 - ETA: 0s - loss: 0.2599 - acc: 0.878 - ETA: 0s - loss: 0.2584 - acc: 0.879 - ETA: 0s - loss: 0.2565 - acc: 0.880 - ETA: 0s - loss: 0.2560 - acc: 0.881 - ETA: 0s - loss: 0.2555 - acc: 0.882 - ETA: 0s - loss: 0.2550 - acc: 0.882 - ETA: 0s - loss: 0.2565 - acc: 0.882 - 1s 50us/step - loss: 0.2580 - acc: 0.8819 - val_loss: 0.3817 - val_acc: 0.8452\n",
      "Epoch 38/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.3629 - acc: 0.828 - ETA: 0s - loss: 0.2446 - acc: 0.891 - ETA: 0s - loss: 0.2413 - acc: 0.891 - ETA: 0s - loss: 0.2487 - acc: 0.886 - ETA: 0s - loss: 0.2454 - acc: 0.888 - ETA: 0s - loss: 0.2505 - acc: 0.886 - ETA: 0s - loss: 0.2497 - acc: 0.885 - ETA: 0s - loss: 0.2520 - acc: 0.884 - ETA: 0s - loss: 0.2511 - acc: 0.884 - ETA: 0s - loss: 0.2526 - acc: 0.884 - ETA: 0s - loss: 0.2507 - acc: 0.886 - ETA: 0s - loss: 0.2538 - acc: 0.884 - ETA: 0s - loss: 0.2544 - acc: 0.884 - ETA: 0s - loss: 0.2524 - acc: 0.885 - ETA: 0s - loss: 0.2524 - acc: 0.886 - ETA: 0s - loss: 0.2543 - acc: 0.885 - ETA: 0s - loss: 0.2554 - acc: 0.884 - ETA: 0s - loss: 0.2561 - acc: 0.883 - 1s 50us/step - loss: 0.2558 - acc: 0.8837 - val_loss: 0.4170 - val_acc: 0.8414\n",
      "Epoch 39/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.3954 - acc: 0.828 - ETA: 0s - loss: 0.2595 - acc: 0.875 - ETA: 0s - loss: 0.2635 - acc: 0.878 - ETA: 0s - loss: 0.2561 - acc: 0.881 - ETA: 0s - loss: 0.2551 - acc: 0.882 - ETA: 0s - loss: 0.2555 - acc: 0.882 - ETA: 0s - loss: 0.2539 - acc: 0.883 - ETA: 0s - loss: 0.2558 - acc: 0.882 - ETA: 0s - loss: 0.2553 - acc: 0.883 - ETA: 0s - loss: 0.2545 - acc: 0.881 - ETA: 0s - loss: 0.2557 - acc: 0.880 - ETA: 0s - loss: 0.2526 - acc: 0.883 - ETA: 0s - loss: 0.2536 - acc: 0.882 - ETA: 0s - loss: 0.2544 - acc: 0.882 - ETA: 0s - loss: 0.2548 - acc: 0.882 - ETA: 0s - loss: 0.2539 - acc: 0.883 - ETA: 0s - loss: 0.2547 - acc: 0.883 - ETA: 0s - loss: 0.2550 - acc: 0.882 - 1s 50us/step - loss: 0.2544 - acc: 0.8833 - val_loss: 0.4151 - val_acc: 0.8483\n",
      "Epoch 40/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1820 - acc: 0.968 - ETA: 0s - loss: 0.2302 - acc: 0.896 - ETA: 0s - loss: 0.2434 - acc: 0.893 - ETA: 0s - loss: 0.2462 - acc: 0.889 - ETA: 0s - loss: 0.2520 - acc: 0.884 - ETA: 0s - loss: 0.2558 - acc: 0.883 - ETA: 0s - loss: 0.2572 - acc: 0.881 - ETA: 0s - loss: 0.2541 - acc: 0.883 - ETA: 0s - loss: 0.2541 - acc: 0.883 - ETA: 0s - loss: 0.2549 - acc: 0.884 - ETA: 0s - loss: 0.2560 - acc: 0.883 - ETA: 0s - loss: 0.2559 - acc: 0.883 - ETA: 0s - loss: 0.2551 - acc: 0.883 - ETA: 0s - loss: 0.2551 - acc: 0.884 - ETA: 0s - loss: 0.2552 - acc: 0.884 - ETA: 0s - loss: 0.2544 - acc: 0.884 - ETA: 0s - loss: 0.2544 - acc: 0.884 - ETA: 0s - loss: 0.2541 - acc: 0.884 - 1s 50us/step - loss: 0.2534 - acc: 0.8846 - val_loss: 0.4152 - val_acc: 0.8454\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1199 - acc: 0.953 - ETA: 0s - loss: 0.2222 - acc: 0.901 - ETA: 0s - loss: 0.2530 - acc: 0.885 - ETA: 0s - loss: 0.2591 - acc: 0.880 - ETA: 0s - loss: 0.2491 - acc: 0.884 - ETA: 0s - loss: 0.2483 - acc: 0.884 - ETA: 0s - loss: 0.2524 - acc: 0.884 - ETA: 0s - loss: 0.2522 - acc: 0.884 - ETA: 0s - loss: 0.2528 - acc: 0.883 - ETA: 0s - loss: 0.2517 - acc: 0.883 - ETA: 0s - loss: 0.2525 - acc: 0.884 - ETA: 0s - loss: 0.2531 - acc: 0.883 - ETA: 0s - loss: 0.2522 - acc: 0.884 - ETA: 0s - loss: 0.2526 - acc: 0.883 - ETA: 0s - loss: 0.2532 - acc: 0.883 - ETA: 0s - loss: 0.2532 - acc: 0.883 - ETA: 0s - loss: 0.2531 - acc: 0.883 - ETA: 0s - loss: 0.2535 - acc: 0.884 - 1s 51us/step - loss: 0.2530 - acc: 0.8846 - val_loss: 0.4105 - val_acc: 0.8475\n",
      "Epoch 42/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.3140 - acc: 0.875 - ETA: 0s - loss: 0.2299 - acc: 0.900 - ETA: 0s - loss: 0.2466 - acc: 0.891 - ETA: 0s - loss: 0.2487 - acc: 0.888 - ETA: 0s - loss: 0.2523 - acc: 0.884 - ETA: 0s - loss: 0.2525 - acc: 0.883 - ETA: 0s - loss: 0.2539 - acc: 0.883 - ETA: 0s - loss: 0.2542 - acc: 0.881 - ETA: 0s - loss: 0.2517 - acc: 0.882 - ETA: 0s - loss: 0.2516 - acc: 0.882 - ETA: 0s - loss: 0.2517 - acc: 0.883 - ETA: 0s - loss: 0.2525 - acc: 0.883 - ETA: 0s - loss: 0.2508 - acc: 0.883 - ETA: 0s - loss: 0.2517 - acc: 0.883 - ETA: 0s - loss: 0.2537 - acc: 0.882 - ETA: 0s - loss: 0.2527 - acc: 0.882 - ETA: 0s - loss: 0.2525 - acc: 0.882 - ETA: 0s - loss: 0.2514 - acc: 0.883 - 1s 50us/step - loss: 0.2517 - acc: 0.8830 - val_loss: 0.4154 - val_acc: 0.8417\n",
      "Epoch 43/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.2409 - acc: 0.890 - ETA: 0s - loss: 0.2394 - acc: 0.884 - ETA: 0s - loss: 0.2549 - acc: 0.878 - ETA: 0s - loss: 0.2541 - acc: 0.881 - ETA: 0s - loss: 0.2494 - acc: 0.885 - ETA: 0s - loss: 0.2502 - acc: 0.883 - ETA: 0s - loss: 0.2557 - acc: 0.883 - ETA: 0s - loss: 0.2548 - acc: 0.883 - ETA: 0s - loss: 0.2566 - acc: 0.881 - ETA: 0s - loss: 0.2536 - acc: 0.882 - ETA: 0s - loss: 0.2538 - acc: 0.883 - ETA: 0s - loss: 0.2526 - acc: 0.883 - ETA: 0s - loss: 0.2512 - acc: 0.884 - ETA: 0s - loss: 0.2496 - acc: 0.885 - ETA: 0s - loss: 0.2496 - acc: 0.884 - ETA: 0s - loss: 0.2490 - acc: 0.885 - ETA: 0s - loss: 0.2500 - acc: 0.885 - ETA: 0s - loss: 0.2499 - acc: 0.885 - 1s 50us/step - loss: 0.2506 - acc: 0.8853 - val_loss: 0.4144 - val_acc: 0.8420\n",
      "Epoch 44/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.2512 - acc: 0.890 - ETA: 0s - loss: 0.2376 - acc: 0.888 - ETA: 0s - loss: 0.2427 - acc: 0.890 - ETA: 0s - loss: 0.2371 - acc: 0.893 - ETA: 0s - loss: 0.2348 - acc: 0.894 - ETA: 0s - loss: 0.2325 - acc: 0.895 - ETA: 0s - loss: 0.2349 - acc: 0.894 - ETA: 0s - loss: 0.2408 - acc: 0.891 - ETA: 0s - loss: 0.2428 - acc: 0.891 - ETA: 0s - loss: 0.2435 - acc: 0.889 - ETA: 0s - loss: 0.2418 - acc: 0.889 - ETA: 0s - loss: 0.2453 - acc: 0.888 - ETA: 0s - loss: 0.2477 - acc: 0.887 - ETA: 0s - loss: 0.2475 - acc: 0.887 - ETA: 0s - loss: 0.2473 - acc: 0.887 - ETA: 0s - loss: 0.2460 - acc: 0.887 - ETA: 0s - loss: 0.2486 - acc: 0.886 - ETA: 0s - loss: 0.2488 - acc: 0.886 - 1s 51us/step - loss: 0.2497 - acc: 0.8856 - val_loss: 0.4190 - val_acc: 0.8466\n",
      "Epoch 45/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.2151 - acc: 0.906 - ETA: 0s - loss: 0.2466 - acc: 0.879 - ETA: 0s - loss: 0.2489 - acc: 0.884 - ETA: 0s - loss: 0.2466 - acc: 0.881 - ETA: 0s - loss: 0.2501 - acc: 0.879 - ETA: 0s - loss: 0.2537 - acc: 0.880 - ETA: 0s - loss: 0.2574 - acc: 0.880 - ETA: 0s - loss: 0.2557 - acc: 0.880 - ETA: 0s - loss: 0.2539 - acc: 0.882 - ETA: 0s - loss: 0.2540 - acc: 0.881 - ETA: 0s - loss: 0.2525 - acc: 0.881 - ETA: 0s - loss: 0.2512 - acc: 0.883 - ETA: 0s - loss: 0.2486 - acc: 0.884 - ETA: 0s - loss: 0.2478 - acc: 0.885 - ETA: 0s - loss: 0.2488 - acc: 0.885 - ETA: 0s - loss: 0.2505 - acc: 0.885 - ETA: 0s - loss: 0.2502 - acc: 0.884 - ETA: 0s - loss: 0.2482 - acc: 0.885 - ETA: 0s - loss: 0.2493 - acc: 0.885 - 1s 53us/step - loss: 0.2496 - acc: 0.8856 - val_loss: 0.4316 - val_acc: 0.8465\n",
      "Epoch 46/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.2958 - acc: 0.796 - ETA: 0s - loss: 0.2330 - acc: 0.894 - ETA: 0s - loss: 0.2398 - acc: 0.892 - ETA: 0s - loss: 0.2422 - acc: 0.891 - ETA: 0s - loss: 0.2449 - acc: 0.889 - ETA: 0s - loss: 0.2459 - acc: 0.887 - ETA: 0s - loss: 0.2441 - acc: 0.889 - ETA: 0s - loss: 0.2476 - acc: 0.887 - ETA: 0s - loss: 0.2471 - acc: 0.887 - ETA: 0s - loss: 0.2478 - acc: 0.887 - ETA: 0s - loss: 0.2483 - acc: 0.887 - ETA: 0s - loss: 0.2483 - acc: 0.887 - ETA: 0s - loss: 0.2464 - acc: 0.888 - ETA: 0s - loss: 0.2471 - acc: 0.887 - ETA: 0s - loss: 0.2470 - acc: 0.887 - ETA: 0s - loss: 0.2460 - acc: 0.887 - ETA: 0s - loss: 0.2470 - acc: 0.887 - ETA: 0s - loss: 0.2475 - acc: 0.887 - ETA: 0s - loss: 0.2479 - acc: 0.886 - 1s 51us/step - loss: 0.2482 - acc: 0.8862 - val_loss: 0.4421 - val_acc: 0.8432\n",
      "Epoch 47/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.2378 - acc: 0.875 - ETA: 0s - loss: 0.2396 - acc: 0.893 - ETA: 0s - loss: 0.2372 - acc: 0.891 - ETA: 0s - loss: 0.2418 - acc: 0.892 - ETA: 0s - loss: 0.2420 - acc: 0.891 - ETA: 0s - loss: 0.2449 - acc: 0.888 - ETA: 0s - loss: 0.2472 - acc: 0.887 - ETA: 0s - loss: 0.2465 - acc: 0.887 - ETA: 0s - loss: 0.2457 - acc: 0.887 - ETA: 0s - loss: 0.2461 - acc: 0.888 - ETA: 0s - loss: 0.2460 - acc: 0.888 - ETA: 0s - loss: 0.2465 - acc: 0.887 - ETA: 0s - loss: 0.2443 - acc: 0.889 - ETA: 0s - loss: 0.2453 - acc: 0.888 - ETA: 0s - loss: 0.2444 - acc: 0.888 - ETA: 0s - loss: 0.2436 - acc: 0.889 - ETA: 0s - loss: 0.2461 - acc: 0.888 - ETA: 0s - loss: 0.2475 - acc: 0.887 - 1s 50us/step - loss: 0.2476 - acc: 0.8875 - val_loss: 0.4431 - val_acc: 0.8391\n",
      "Epoch 48/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.2986 - acc: 0.812 - ETA: 0s - loss: 0.2634 - acc: 0.876 - ETA: 0s - loss: 0.2503 - acc: 0.882 - ETA: 0s - loss: 0.2462 - acc: 0.882 - ETA: 0s - loss: 0.2507 - acc: 0.879 - ETA: 0s - loss: 0.2459 - acc: 0.884 - ETA: 0s - loss: 0.2472 - acc: 0.886 - ETA: 0s - loss: 0.2445 - acc: 0.887 - ETA: 0s - loss: 0.2440 - acc: 0.888 - ETA: 0s - loss: 0.2432 - acc: 0.889 - ETA: 0s - loss: 0.2425 - acc: 0.889 - ETA: 0s - loss: 0.2429 - acc: 0.889 - ETA: 0s - loss: 0.2439 - acc: 0.889 - ETA: 0s - loss: 0.2431 - acc: 0.889 - ETA: 0s - loss: 0.2431 - acc: 0.889 - ETA: 0s - loss: 0.2441 - acc: 0.888 - ETA: 0s - loss: 0.2452 - acc: 0.888 - ETA: 0s - loss: 0.2464 - acc: 0.887 - 1s 50us/step - loss: 0.2467 - acc: 0.8869 - val_loss: 0.4392 - val_acc: 0.8426\n",
      "Epoch 49/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.2091 - acc: 0.875 - ETA: 0s - loss: 0.2395 - acc: 0.892 - ETA: 0s - loss: 0.2429 - acc: 0.887 - ETA: 0s - loss: 0.2427 - acc: 0.886 - ETA: 0s - loss: 0.2424 - acc: 0.889 - ETA: 0s - loss: 0.2434 - acc: 0.887 - ETA: 0s - loss: 0.2444 - acc: 0.887 - ETA: 0s - loss: 0.2453 - acc: 0.886 - ETA: 0s - loss: 0.2447 - acc: 0.886 - ETA: 0s - loss: 0.2409 - acc: 0.887 - ETA: 0s - loss: 0.2401 - acc: 0.888 - ETA: 0s - loss: 0.2434 - acc: 0.887 - ETA: 0s - loss: 0.2440 - acc: 0.886 - ETA: 0s - loss: 0.2438 - acc: 0.886 - ETA: 0s - loss: 0.2434 - acc: 0.887 - ETA: 0s - loss: 0.2449 - acc: 0.887 - ETA: 0s - loss: 0.2452 - acc: 0.887 - ETA: 0s - loss: 0.2458 - acc: 0.887 - 1s 50us/step - loss: 0.2471 - acc: 0.8868 - val_loss: 0.4221 - val_acc: 0.8470\n",
      "Epoch 50/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.2182 - acc: 0.921 - ETA: 0s - loss: 0.2208 - acc: 0.898 - ETA: 0s - loss: 0.2267 - acc: 0.896 - ETA: 0s - loss: 0.2335 - acc: 0.892 - ETA: 0s - loss: 0.2378 - acc: 0.892 - ETA: 0s - loss: 0.2359 - acc: 0.893 - ETA: 0s - loss: 0.2369 - acc: 0.893 - ETA: 0s - loss: 0.2396 - acc: 0.891 - ETA: 0s - loss: 0.2397 - acc: 0.891 - ETA: 0s - loss: 0.2430 - acc: 0.888 - ETA: 0s - loss: 0.2455 - acc: 0.887 - ETA: 0s - loss: 0.2443 - acc: 0.887 - ETA: 0s - loss: 0.2441 - acc: 0.888 - ETA: 0s - loss: 0.2439 - acc: 0.888 - ETA: 0s - loss: 0.2440 - acc: 0.888 - ETA: 0s - loss: 0.2452 - acc: 0.888 - ETA: 0s - loss: 0.2443 - acc: 0.888 - ETA: 0s - loss: 0.2451 - acc: 0.888 - 1s 50us/step - loss: 0.2461 - acc: 0.8875 - val_loss: 0.4568 - val_acc: 0.8346\n",
      "Epoch 51/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.1920 - acc: 0.906 - ETA: 0s - loss: 0.2318 - acc: 0.893 - ETA: 0s - loss: 0.2365 - acc: 0.888 - ETA: 0s - loss: 0.2360 - acc: 0.889 - ETA: 0s - loss: 0.2326 - acc: 0.892 - ETA: 0s - loss: 0.2349 - acc: 0.892 - ETA: 0s - loss: 0.2377 - acc: 0.893 - ETA: 0s - loss: 0.2369 - acc: 0.892 - ETA: 0s - loss: 0.2397 - acc: 0.890 - ETA: 0s - loss: 0.2408 - acc: 0.889 - ETA: 0s - loss: 0.2425 - acc: 0.888 - ETA: 0s - loss: 0.2433 - acc: 0.887 - ETA: 0s - loss: 0.2447 - acc: 0.886 - ETA: 0s - loss: 0.2437 - acc: 0.887 - ETA: 0s - loss: 0.2428 - acc: 0.888 - ETA: 0s - loss: 0.2440 - acc: 0.888 - ETA: 0s - loss: 0.2446 - acc: 0.887 - ETA: 0s - loss: 0.2442 - acc: 0.887 - 1s 50us/step - loss: 0.2448 - acc: 0.8872 - val_loss: 0.4429 - val_acc: 0.8439\n",
      "Epoch 52/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.2396 - acc: 0.906 - ETA: 0s - loss: 0.2393 - acc: 0.877 - ETA: 0s - loss: 0.2418 - acc: 0.883 - ETA: 0s - loss: 0.2468 - acc: 0.886 - ETA: 0s - loss: 0.2453 - acc: 0.889 - ETA: 0s - loss: 0.2475 - acc: 0.887 - ETA: 0s - loss: 0.2494 - acc: 0.884 - ETA: 0s - loss: 0.2469 - acc: 0.886 - ETA: 0s - loss: 0.2492 - acc: 0.885 - ETA: 0s - loss: 0.2478 - acc: 0.886 - ETA: 0s - loss: 0.2447 - acc: 0.887 - ETA: 0s - loss: 0.2443 - acc: 0.887 - ETA: 0s - loss: 0.2434 - acc: 0.888 - ETA: 0s - loss: 0.2424 - acc: 0.888 - ETA: 0s - loss: 0.2421 - acc: 0.888 - ETA: 0s - loss: 0.2438 - acc: 0.886 - ETA: 0s - loss: 0.2448 - acc: 0.886 - ETA: 0s - loss: 0.2443 - acc: 0.887 - 1s 50us/step - loss: 0.2447 - acc: 0.8871 - val_loss: 0.4533 - val_acc: 0.8454\n",
      "Epoch 53/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1820 - acc: 0.953 - ETA: 0s - loss: 0.2454 - acc: 0.886 - ETA: 0s - loss: 0.2345 - acc: 0.891 - ETA: 0s - loss: 0.2350 - acc: 0.893 - ETA: 0s - loss: 0.2345 - acc: 0.893 - ETA: 0s - loss: 0.2340 - acc: 0.893 - ETA: 0s - loss: 0.2354 - acc: 0.893 - ETA: 0s - loss: 0.2379 - acc: 0.893 - ETA: 0s - loss: 0.2401 - acc: 0.891 - ETA: 0s - loss: 0.2407 - acc: 0.890 - ETA: 0s - loss: 0.2435 - acc: 0.889 - ETA: 0s - loss: 0.2435 - acc: 0.889 - ETA: 0s - loss: 0.2415 - acc: 0.890 - ETA: 0s - loss: 0.2410 - acc: 0.890 - ETA: 0s - loss: 0.2416 - acc: 0.889 - ETA: 0s - loss: 0.2433 - acc: 0.888 - ETA: 0s - loss: 0.2445 - acc: 0.887 - ETA: 0s - loss: 0.2446 - acc: 0.887 - 1s 49us/step - loss: 0.2441 - acc: 0.8878 - val_loss: 0.4570 - val_acc: 0.8436\n",
      "Epoch 54/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1908 - acc: 0.906 - ETA: 0s - loss: 0.2484 - acc: 0.891 - ETA: 0s - loss: 0.2406 - acc: 0.891 - ETA: 0s - loss: 0.2433 - acc: 0.890 - ETA: 0s - loss: 0.2409 - acc: 0.891 - ETA: 0s - loss: 0.2367 - acc: 0.893 - ETA: 0s - loss: 0.2327 - acc: 0.895 - ETA: 0s - loss: 0.2338 - acc: 0.894 - ETA: 0s - loss: 0.2355 - acc: 0.893 - ETA: 0s - loss: 0.2361 - acc: 0.893 - ETA: 0s - loss: 0.2357 - acc: 0.892 - ETA: 0s - loss: 0.2373 - acc: 0.891 - ETA: 0s - loss: 0.2379 - acc: 0.890 - ETA: 0s - loss: 0.2382 - acc: 0.890 - ETA: 0s - loss: 0.2397 - acc: 0.889 - ETA: 0s - loss: 0.2413 - acc: 0.889 - ETA: 0s - loss: 0.2402 - acc: 0.890 - ETA: 0s - loss: 0.2422 - acc: 0.888 - 1s 49us/step - loss: 0.2423 - acc: 0.8887 - val_loss: 0.4401 - val_acc: 0.8451\n",
      "Epoch 55/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.3244 - acc: 0.828 - ETA: 0s - loss: 0.2392 - acc: 0.880 - ETA: 0s - loss: 0.2310 - acc: 0.888 - ETA: 0s - loss: 0.2319 - acc: 0.888 - ETA: 0s - loss: 0.2311 - acc: 0.890 - ETA: 0s - loss: 0.2313 - acc: 0.891 - ETA: 0s - loss: 0.2335 - acc: 0.891 - ETA: 0s - loss: 0.2355 - acc: 0.890 - ETA: 0s - loss: 0.2362 - acc: 0.890 - ETA: 0s - loss: 0.2358 - acc: 0.890 - ETA: 0s - loss: 0.2378 - acc: 0.890 - ETA: 0s - loss: 0.2373 - acc: 0.890 - ETA: 0s - loss: 0.2384 - acc: 0.890 - ETA: 0s - loss: 0.2383 - acc: 0.890 - ETA: 0s - loss: 0.2397 - acc: 0.889 - ETA: 0s - loss: 0.2419 - acc: 0.888 - ETA: 0s - loss: 0.2424 - acc: 0.889 - ETA: 0s - loss: 0.2425 - acc: 0.889 - 1s 50us/step - loss: 0.2424 - acc: 0.8895 - val_loss: 0.4805 - val_acc: 0.8329\n",
      "Epoch 56/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.1355 - acc: 0.968 - ETA: 0s - loss: 0.2291 - acc: 0.898 - ETA: 0s - loss: 0.2244 - acc: 0.894 - ETA: 0s - loss: 0.2326 - acc: 0.893 - ETA: 0s - loss: 0.2373 - acc: 0.891 - ETA: 0s - loss: 0.2379 - acc: 0.889 - ETA: 0s - loss: 0.2344 - acc: 0.892 - ETA: 0s - loss: 0.2346 - acc: 0.893 - ETA: 0s - loss: 0.2356 - acc: 0.893 - ETA: 0s - loss: 0.2359 - acc: 0.893 - ETA: 0s - loss: 0.2356 - acc: 0.893 - ETA: 0s - loss: 0.2367 - acc: 0.891 - ETA: 0s - loss: 0.2382 - acc: 0.890 - ETA: 0s - loss: 0.2417 - acc: 0.890 - ETA: 0s - loss: 0.2412 - acc: 0.890 - ETA: 0s - loss: 0.2426 - acc: 0.889 - ETA: 0s - loss: 0.2424 - acc: 0.889 - ETA: 0s - loss: 0.2428 - acc: 0.889 - 1s 49us/step - loss: 0.2431 - acc: 0.8890 - val_loss: 0.4561 - val_acc: 0.8426\n",
      "Epoch 57/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.2191 - acc: 0.859 - ETA: 0s - loss: 0.2220 - acc: 0.897 - ETA: 0s - loss: 0.2300 - acc: 0.896 - ETA: 0s - loss: 0.2291 - acc: 0.896 - ETA: 0s - loss: 0.2342 - acc: 0.895 - ETA: 0s - loss: 0.2355 - acc: 0.896 - ETA: 0s - loss: 0.2383 - acc: 0.893 - ETA: 0s - loss: 0.2365 - acc: 0.895 - ETA: 0s - loss: 0.2370 - acc: 0.895 - ETA: 0s - loss: 0.2376 - acc: 0.894 - ETA: 0s - loss: 0.2369 - acc: 0.895 - ETA: 0s - loss: 0.2383 - acc: 0.893 - ETA: 0s - loss: 0.2381 - acc: 0.892 - ETA: 0s - loss: 0.2398 - acc: 0.892 - ETA: 0s - loss: 0.2406 - acc: 0.891 - ETA: 0s - loss: 0.2415 - acc: 0.890 - ETA: 0s - loss: 0.2406 - acc: 0.890 - ETA: 0s - loss: 0.2395 - acc: 0.890 - 1s 50us/step - loss: 0.2407 - acc: 0.8905 - val_loss: 0.4448 - val_acc: 0.8437\n",
      "Epoch 58/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.3278 - acc: 0.843 - ETA: 0s - loss: 0.2130 - acc: 0.908 - ETA: 0s - loss: 0.2194 - acc: 0.897 - ETA: 0s - loss: 0.2241 - acc: 0.893 - ETA: 0s - loss: 0.2261 - acc: 0.893 - ETA: 0s - loss: 0.2315 - acc: 0.892 - ETA: 0s - loss: 0.2352 - acc: 0.891 - ETA: 0s - loss: 0.2358 - acc: 0.890 - ETA: 0s - loss: 0.2359 - acc: 0.890 - ETA: 0s - loss: 0.2349 - acc: 0.891 - ETA: 0s - loss: 0.2380 - acc: 0.890 - ETA: 0s - loss: 0.2397 - acc: 0.890 - ETA: 0s - loss: 0.2401 - acc: 0.890 - ETA: 0s - loss: 0.2388 - acc: 0.890 - ETA: 0s - loss: 0.2386 - acc: 0.891 - ETA: 0s - loss: 0.2385 - acc: 0.890 - ETA: 0s - loss: 0.2399 - acc: 0.889 - ETA: 0s - loss: 0.2413 - acc: 0.888 - 1s 49us/step - loss: 0.2418 - acc: 0.8882 - val_loss: 0.4451 - val_acc: 0.8453\n",
      "Epoch 59/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.2587 - acc: 0.890 - ETA: 0s - loss: 0.2184 - acc: 0.898 - ETA: 0s - loss: 0.2309 - acc: 0.893 - ETA: 0s - loss: 0.2303 - acc: 0.894 - ETA: 0s - loss: 0.2303 - acc: 0.896 - ETA: 0s - loss: 0.2297 - acc: 0.896 - ETA: 0s - loss: 0.2330 - acc: 0.892 - ETA: 0s - loss: 0.2339 - acc: 0.891 - ETA: 0s - loss: 0.2359 - acc: 0.890 - ETA: 0s - loss: 0.2349 - acc: 0.892 - ETA: 0s - loss: 0.2342 - acc: 0.892 - ETA: 0s - loss: 0.2337 - acc: 0.892 - ETA: 0s - loss: 0.2371 - acc: 0.890 - ETA: 0s - loss: 0.2375 - acc: 0.890 - ETA: 0s - loss: 0.2378 - acc: 0.890 - ETA: 0s - loss: 0.2382 - acc: 0.889 - ETA: 0s - loss: 0.2379 - acc: 0.889 - ETA: 0s - loss: 0.2388 - acc: 0.889 - 1s 50us/step - loss: 0.2387 - acc: 0.8901 - val_loss: 0.4723 - val_acc: 0.8463\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.2599 - acc: 0.921 - ETA: 0s - loss: 0.2362 - acc: 0.896 - ETA: 0s - loss: 0.2465 - acc: 0.886 - ETA: 0s - loss: 0.2341 - acc: 0.893 - ETA: 0s - loss: 0.2419 - acc: 0.890 - ETA: 0s - loss: 0.2405 - acc: 0.889 - ETA: 0s - loss: 0.2369 - acc: 0.892 - ETA: 0s - loss: 0.2423 - acc: 0.889 - ETA: 0s - loss: 0.2404 - acc: 0.890 - ETA: 0s - loss: 0.2407 - acc: 0.889 - ETA: 0s - loss: 0.2435 - acc: 0.887 - ETA: 0s - loss: 0.2430 - acc: 0.888 - ETA: 0s - loss: 0.2425 - acc: 0.889 - ETA: 0s - loss: 0.2414 - acc: 0.889 - ETA: 0s - loss: 0.2410 - acc: 0.889 - ETA: 0s - loss: 0.2414 - acc: 0.889 - ETA: 0s - loss: 0.2413 - acc: 0.889 - ETA: 0s - loss: 0.2400 - acc: 0.890 - 1s 49us/step - loss: 0.2398 - acc: 0.8900 - val_loss: 0.4870 - val_acc: 0.8439\n",
      "Epoch 61/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.2337 - acc: 0.906 - ETA: 0s - loss: 0.2360 - acc: 0.892 - ETA: 0s - loss: 0.2351 - acc: 0.894 - ETA: 0s - loss: 0.2337 - acc: 0.892 - ETA: 0s - loss: 0.2294 - acc: 0.895 - ETA: 0s - loss: 0.2257 - acc: 0.897 - ETA: 0s - loss: 0.2320 - acc: 0.895 - ETA: 0s - loss: 0.2336 - acc: 0.895 - ETA: 0s - loss: 0.2334 - acc: 0.895 - ETA: 0s - loss: 0.2344 - acc: 0.895 - ETA: 0s - loss: 0.2355 - acc: 0.894 - ETA: 0s - loss: 0.2345 - acc: 0.894 - ETA: 0s - loss: 0.2374 - acc: 0.892 - ETA: 0s - loss: 0.2398 - acc: 0.891 - ETA: 0s - loss: 0.2395 - acc: 0.890 - ETA: 0s - loss: 0.2385 - acc: 0.891 - ETA: 0s - loss: 0.2384 - acc: 0.891 - ETA: 0s - loss: 0.2396 - acc: 0.890 - 1s 49us/step - loss: 0.2395 - acc: 0.8906 - val_loss: 0.4888 - val_acc: 0.8319\n",
      "Epoch 62/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.2281 - acc: 0.890 - ETA: 0s - loss: 0.2228 - acc: 0.894 - ETA: 0s - loss: 0.2275 - acc: 0.894 - ETA: 0s - loss: 0.2328 - acc: 0.890 - ETA: 0s - loss: 0.2247 - acc: 0.897 - ETA: 0s - loss: 0.2301 - acc: 0.895 - ETA: 0s - loss: 0.2301 - acc: 0.894 - ETA: 0s - loss: 0.2345 - acc: 0.893 - ETA: 0s - loss: 0.2364 - acc: 0.890 - ETA: 0s - loss: 0.2381 - acc: 0.890 - ETA: 0s - loss: 0.2389 - acc: 0.889 - ETA: 0s - loss: 0.2394 - acc: 0.889 - ETA: 0s - loss: 0.2405 - acc: 0.888 - ETA: 0s - loss: 0.2399 - acc: 0.888 - ETA: 0s - loss: 0.2389 - acc: 0.888 - ETA: 0s - loss: 0.2380 - acc: 0.890 - ETA: 0s - loss: 0.2372 - acc: 0.890 - ETA: 0s - loss: 0.2384 - acc: 0.889 - 1s 50us/step - loss: 0.2389 - acc: 0.8898 - val_loss: 0.4449 - val_acc: 0.8450\n",
      "Epoch 63/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.2128 - acc: 0.875 - ETA: 0s - loss: 0.2283 - acc: 0.892 - ETA: 0s - loss: 0.2317 - acc: 0.889 - ETA: 0s - loss: 0.2239 - acc: 0.896 - ETA: 0s - loss: 0.2265 - acc: 0.895 - ETA: 0s - loss: 0.2326 - acc: 0.892 - ETA: 0s - loss: 0.2309 - acc: 0.893 - ETA: 0s - loss: 0.2314 - acc: 0.894 - ETA: 0s - loss: 0.2321 - acc: 0.893 - ETA: 0s - loss: 0.2378 - acc: 0.890 - ETA: 0s - loss: 0.2362 - acc: 0.891 - ETA: 0s - loss: 0.2384 - acc: 0.889 - ETA: 0s - loss: 0.2390 - acc: 0.890 - ETA: 0s - loss: 0.2382 - acc: 0.891 - ETA: 0s - loss: 0.2395 - acc: 0.890 - ETA: 0s - loss: 0.2386 - acc: 0.890 - ETA: 0s - loss: 0.2380 - acc: 0.891 - ETA: 0s - loss: 0.2389 - acc: 0.891 - 1s 50us/step - loss: 0.2391 - acc: 0.8909 - val_loss: 0.4650 - val_acc: 0.8470\n",
      "Epoch 64/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.2364 - acc: 0.890 - ETA: 0s - loss: 0.2457 - acc: 0.890 - ETA: 0s - loss: 0.2388 - acc: 0.893 - ETA: 0s - loss: 0.2451 - acc: 0.887 - ETA: 0s - loss: 0.2447 - acc: 0.890 - ETA: 0s - loss: 0.2418 - acc: 0.892 - ETA: 0s - loss: 0.2399 - acc: 0.892 - ETA: 0s - loss: 0.2355 - acc: 0.893 - ETA: 0s - loss: 0.2373 - acc: 0.893 - ETA: 0s - loss: 0.2404 - acc: 0.892 - ETA: 0s - loss: 0.2394 - acc: 0.891 - ETA: 0s - loss: 0.2390 - acc: 0.892 - ETA: 0s - loss: 0.2364 - acc: 0.893 - ETA: 0s - loss: 0.2359 - acc: 0.893 - ETA: 0s - loss: 0.2364 - acc: 0.892 - ETA: 0s - loss: 0.2370 - acc: 0.892 - ETA: 0s - loss: 0.2379 - acc: 0.891 - ETA: 0s - loss: 0.2374 - acc: 0.891 - 1s 50us/step - loss: 0.2386 - acc: 0.8909 - val_loss: 0.4916 - val_acc: 0.8400\n",
      "Epoch 65/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.2525 - acc: 0.875 - ETA: 0s - loss: 0.2134 - acc: 0.903 - ETA: 0s - loss: 0.2162 - acc: 0.898 - ETA: 0s - loss: 0.2220 - acc: 0.895 - ETA: 0s - loss: 0.2207 - acc: 0.895 - ETA: 0s - loss: 0.2246 - acc: 0.893 - ETA: 0s - loss: 0.2240 - acc: 0.894 - ETA: 0s - loss: 0.2247 - acc: 0.894 - ETA: 0s - loss: 0.2283 - acc: 0.893 - ETA: 0s - loss: 0.2299 - acc: 0.892 - ETA: 0s - loss: 0.2325 - acc: 0.892 - ETA: 0s - loss: 0.2329 - acc: 0.892 - ETA: 0s - loss: 0.2345 - acc: 0.892 - ETA: 0s - loss: 0.2347 - acc: 0.891 - ETA: 0s - loss: 0.2361 - acc: 0.890 - ETA: 0s - loss: 0.2358 - acc: 0.890 - ETA: 0s - loss: 0.2371 - acc: 0.890 - ETA: 0s - loss: 0.2381 - acc: 0.889 - 1s 49us/step - loss: 0.2374 - acc: 0.8903 - val_loss: 0.4889 - val_acc: 0.8432\n",
      "Epoch 66/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1420 - acc: 0.921 - ETA: 0s - loss: 0.2021 - acc: 0.918 - ETA: 0s - loss: 0.2210 - acc: 0.907 - ETA: 0s - loss: 0.2262 - acc: 0.899 - ETA: 0s - loss: 0.2279 - acc: 0.897 - ETA: 0s - loss: 0.2269 - acc: 0.896 - ETA: 0s - loss: 0.2290 - acc: 0.895 - ETA: 0s - loss: 0.2299 - acc: 0.896 - ETA: 0s - loss: 0.2334 - acc: 0.894 - ETA: 0s - loss: 0.2332 - acc: 0.894 - ETA: 0s - loss: 0.2335 - acc: 0.894 - ETA: 0s - loss: 0.2386 - acc: 0.892 - ETA: 0s - loss: 0.2380 - acc: 0.892 - ETA: 0s - loss: 0.2378 - acc: 0.893 - ETA: 0s - loss: 0.2356 - acc: 0.893 - ETA: 0s - loss: 0.2374 - acc: 0.893 - ETA: 0s - loss: 0.2375 - acc: 0.892 - ETA: 0s - loss: 0.2370 - acc: 0.892 - 1s 50us/step - loss: 0.2378 - acc: 0.8923 - val_loss: 0.4703 - val_acc: 0.8432\n",
      "Epoch 67/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.2926 - acc: 0.796 - ETA: 0s - loss: 0.2351 - acc: 0.886 - ETA: 0s - loss: 0.2374 - acc: 0.885 - ETA: 0s - loss: 0.2442 - acc: 0.884 - ETA: 0s - loss: 0.2403 - acc: 0.885 - ETA: 0s - loss: 0.2363 - acc: 0.888 - ETA: 0s - loss: 0.2330 - acc: 0.890 - ETA: 0s - loss: 0.2368 - acc: 0.890 - ETA: 0s - loss: 0.2352 - acc: 0.891 - ETA: 0s - loss: 0.2368 - acc: 0.890 - ETA: 0s - loss: 0.2384 - acc: 0.890 - ETA: 0s - loss: 0.2389 - acc: 0.890 - ETA: 0s - loss: 0.2378 - acc: 0.890 - ETA: 0s - loss: 0.2363 - acc: 0.891 - ETA: 0s - loss: 0.2358 - acc: 0.891 - ETA: 0s - loss: 0.2354 - acc: 0.891 - ETA: 0s - loss: 0.2363 - acc: 0.891 - ETA: 0s - loss: 0.2350 - acc: 0.892 - 1s 50us/step - loss: 0.2362 - acc: 0.8918 - val_loss: 0.4777 - val_acc: 0.8413\n",
      "Epoch 68/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.1714 - acc: 0.953 - ETA: 0s - loss: 0.2295 - acc: 0.892 - ETA: 0s - loss: 0.2320 - acc: 0.892 - ETA: 0s - loss: 0.2275 - acc: 0.895 - ETA: 0s - loss: 0.2313 - acc: 0.895 - ETA: 0s - loss: 0.2301 - acc: 0.892 - ETA: 0s - loss: 0.2288 - acc: 0.892 - ETA: 0s - loss: 0.2292 - acc: 0.892 - ETA: 0s - loss: 0.2320 - acc: 0.891 - ETA: 0s - loss: 0.2306 - acc: 0.891 - ETA: 0s - loss: 0.2313 - acc: 0.892 - ETA: 0s - loss: 0.2329 - acc: 0.892 - ETA: 0s - loss: 0.2342 - acc: 0.891 - ETA: 0s - loss: 0.2348 - acc: 0.890 - ETA: 0s - loss: 0.2328 - acc: 0.891 - ETA: 0s - loss: 0.2339 - acc: 0.891 - ETA: 0s - loss: 0.2349 - acc: 0.891 - ETA: 0s - loss: 0.2352 - acc: 0.891 - 1s 50us/step - loss: 0.2355 - acc: 0.8907 - val_loss: 0.5028 - val_acc: 0.8413\n",
      "Epoch 69/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.2138 - acc: 0.906 - ETA: 0s - loss: 0.2082 - acc: 0.903 - ETA: 0s - loss: 0.2219 - acc: 0.898 - ETA: 0s - loss: 0.2238 - acc: 0.895 - ETA: 0s - loss: 0.2245 - acc: 0.892 - ETA: 0s - loss: 0.2193 - acc: 0.895 - ETA: 0s - loss: 0.2190 - acc: 0.898 - ETA: 0s - loss: 0.2229 - acc: 0.895 - ETA: 0s - loss: 0.2255 - acc: 0.894 - ETA: 0s - loss: 0.2269 - acc: 0.894 - ETA: 0s - loss: 0.2303 - acc: 0.892 - ETA: 0s - loss: 0.2305 - acc: 0.891 - ETA: 0s - loss: 0.2300 - acc: 0.891 - ETA: 0s - loss: 0.2285 - acc: 0.892 - ETA: 0s - loss: 0.2308 - acc: 0.891 - ETA: 0s - loss: 0.2332 - acc: 0.890 - ETA: 0s - loss: 0.2338 - acc: 0.890 - ETA: 0s - loss: 0.2341 - acc: 0.890 - 1s 50us/step - loss: 0.2341 - acc: 0.8909 - val_loss: 0.4961 - val_acc: 0.8426\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.2354 - acc: 0.875 - ETA: 0s - loss: 0.2428 - acc: 0.892 - ETA: 0s - loss: 0.2326 - acc: 0.897 - ETA: 0s - loss: 0.2346 - acc: 0.894 - ETA: 0s - loss: 0.2328 - acc: 0.891 - ETA: 0s - loss: 0.2295 - acc: 0.894 - ETA: 0s - loss: 0.2295 - acc: 0.895 - ETA: 0s - loss: 0.2331 - acc: 0.893 - ETA: 0s - loss: 0.2335 - acc: 0.894 - ETA: 0s - loss: 0.2311 - acc: 0.895 - ETA: 0s - loss: 0.2328 - acc: 0.893 - ETA: 0s - loss: 0.2345 - acc: 0.892 - ETA: 0s - loss: 0.2344 - acc: 0.892 - ETA: 0s - loss: 0.2346 - acc: 0.892 - ETA: 0s - loss: 0.2344 - acc: 0.892 - ETA: 0s - loss: 0.2341 - acc: 0.892 - ETA: 0s - loss: 0.2333 - acc: 0.893 - ETA: 0s - loss: 0.2343 - acc: 0.892 - 1s 49us/step - loss: 0.2350 - acc: 0.8920 - val_loss: 0.4751 - val_acc: 0.8427\n",
      "Epoch 71/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.2845 - acc: 0.890 - ETA: 0s - loss: 0.2116 - acc: 0.898 - ETA: 0s - loss: 0.2237 - acc: 0.892 - ETA: 0s - loss: 0.2343 - acc: 0.892 - ETA: 0s - loss: 0.2403 - acc: 0.889 - ETA: 0s - loss: 0.2385 - acc: 0.889 - ETA: 0s - loss: 0.2393 - acc: 0.888 - ETA: 0s - loss: 0.2358 - acc: 0.890 - ETA: 0s - loss: 0.2338 - acc: 0.891 - ETA: 0s - loss: 0.2338 - acc: 0.891 - ETA: 0s - loss: 0.2324 - acc: 0.892 - ETA: 0s - loss: 0.2312 - acc: 0.893 - ETA: 0s - loss: 0.2315 - acc: 0.892 - ETA: 0s - loss: 0.2318 - acc: 0.892 - ETA: 0s - loss: 0.2333 - acc: 0.893 - ETA: 0s - loss: 0.2351 - acc: 0.892 - ETA: 0s - loss: 0.2343 - acc: 0.892 - ETA: 0s - loss: 0.2344 - acc: 0.892 - 1s 49us/step - loss: 0.2341 - acc: 0.8926 - val_loss: 0.5020 - val_acc: 0.8385\n",
      "Epoch 72/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1582 - acc: 0.921 - ETA: 0s - loss: 0.2122 - acc: 0.904 - ETA: 0s - loss: 0.2387 - acc: 0.889 - ETA: 0s - loss: 0.2314 - acc: 0.895 - ETA: 0s - loss: 0.2256 - acc: 0.898 - ETA: 0s - loss: 0.2267 - acc: 0.896 - ETA: 0s - loss: 0.2251 - acc: 0.897 - ETA: 0s - loss: 0.2266 - acc: 0.898 - ETA: 0s - loss: 0.2267 - acc: 0.897 - ETA: 0s - loss: 0.2273 - acc: 0.897 - ETA: 0s - loss: 0.2303 - acc: 0.896 - ETA: 0s - loss: 0.2306 - acc: 0.896 - ETA: 0s - loss: 0.2310 - acc: 0.895 - ETA: 0s - loss: 0.2329 - acc: 0.893 - ETA: 0s - loss: 0.2345 - acc: 0.893 - ETA: 0s - loss: 0.2344 - acc: 0.892 - ETA: 0s - loss: 0.2334 - acc: 0.893 - ETA: 0s - loss: 0.2333 - acc: 0.893 - 1s 50us/step - loss: 0.2337 - acc: 0.8931 - val_loss: 0.4924 - val_acc: 0.8434\n",
      "Epoch 73/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.1618 - acc: 0.906 - ETA: 0s - loss: 0.2146 - acc: 0.909 - ETA: 0s - loss: 0.2197 - acc: 0.904 - ETA: 0s - loss: 0.2248 - acc: 0.897 - ETA: 0s - loss: 0.2227 - acc: 0.900 - ETA: 0s - loss: 0.2235 - acc: 0.902 - ETA: 0s - loss: 0.2247 - acc: 0.900 - ETA: 0s - loss: 0.2248 - acc: 0.899 - ETA: 0s - loss: 0.2280 - acc: 0.897 - ETA: 0s - loss: 0.2309 - acc: 0.894 - ETA: 0s - loss: 0.2306 - acc: 0.895 - ETA: 0s - loss: 0.2287 - acc: 0.895 - ETA: 0s - loss: 0.2284 - acc: 0.895 - ETA: 0s - loss: 0.2275 - acc: 0.896 - ETA: 0s - loss: 0.2282 - acc: 0.895 - ETA: 0s - loss: 0.2280 - acc: 0.894 - ETA: 0s - loss: 0.2315 - acc: 0.892 - ETA: 0s - loss: 0.2324 - acc: 0.891 - 1s 50us/step - loss: 0.2327 - acc: 0.8918 - val_loss: 0.5009 - val_acc: 0.8432\n",
      "Epoch 74/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.2217 - acc: 0.875 - ETA: 0s - loss: 0.2170 - acc: 0.900 - ETA: 0s - loss: 0.2159 - acc: 0.905 - ETA: 0s - loss: 0.2312 - acc: 0.896 - ETA: 0s - loss: 0.2261 - acc: 0.898 - ETA: 0s - loss: 0.2251 - acc: 0.898 - ETA: 0s - loss: 0.2289 - acc: 0.894 - ETA: 0s - loss: 0.2294 - acc: 0.893 - ETA: 0s - loss: 0.2286 - acc: 0.894 - ETA: 0s - loss: 0.2291 - acc: 0.893 - ETA: 0s - loss: 0.2290 - acc: 0.894 - ETA: 0s - loss: 0.2310 - acc: 0.893 - ETA: 0s - loss: 0.2312 - acc: 0.894 - ETA: 0s - loss: 0.2315 - acc: 0.893 - ETA: 0s - loss: 0.2300 - acc: 0.894 - ETA: 0s - loss: 0.2308 - acc: 0.893 - ETA: 0s - loss: 0.2318 - acc: 0.893 - ETA: 0s - loss: 0.2326 - acc: 0.893 - 1s 50us/step - loss: 0.2324 - acc: 0.8933 - val_loss: 0.5221 - val_acc: 0.8416\n",
      "Epoch 75/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.2214 - acc: 0.906 - ETA: 0s - loss: 0.2517 - acc: 0.881 - ETA: 0s - loss: 0.2305 - acc: 0.893 - ETA: 0s - loss: 0.2298 - acc: 0.896 - ETA: 0s - loss: 0.2319 - acc: 0.895 - ETA: 0s - loss: 0.2279 - acc: 0.897 - ETA: 0s - loss: 0.2308 - acc: 0.896 - ETA: 0s - loss: 0.2314 - acc: 0.895 - ETA: 0s - loss: 0.2299 - acc: 0.895 - ETA: 0s - loss: 0.2302 - acc: 0.895 - ETA: 0s - loss: 0.2310 - acc: 0.895 - ETA: 0s - loss: 0.2303 - acc: 0.894 - ETA: 0s - loss: 0.2311 - acc: 0.894 - ETA: 0s - loss: 0.2317 - acc: 0.894 - ETA: 0s - loss: 0.2326 - acc: 0.894 - ETA: 0s - loss: 0.2326 - acc: 0.894 - ETA: 0s - loss: 0.2332 - acc: 0.893 - ETA: 0s - loss: 0.2343 - acc: 0.893 - 1s 49us/step - loss: 0.2345 - acc: 0.8927 - val_loss: 0.4995 - val_acc: 0.8430\n",
      "Epoch 76/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.2475 - acc: 0.843 - ETA: 0s - loss: 0.2172 - acc: 0.898 - ETA: 0s - loss: 0.2266 - acc: 0.894 - ETA: 0s - loss: 0.2227 - acc: 0.894 - ETA: 0s - loss: 0.2228 - acc: 0.896 - ETA: 0s - loss: 0.2253 - acc: 0.896 - ETA: 0s - loss: 0.2305 - acc: 0.895 - ETA: 0s - loss: 0.2277 - acc: 0.896 - ETA: 0s - loss: 0.2273 - acc: 0.896 - ETA: 0s - loss: 0.2280 - acc: 0.894 - ETA: 0s - loss: 0.2257 - acc: 0.895 - ETA: 0s - loss: 0.2281 - acc: 0.894 - ETA: 0s - loss: 0.2303 - acc: 0.892 - ETA: 0s - loss: 0.2318 - acc: 0.892 - ETA: 0s - loss: 0.2307 - acc: 0.892 - ETA: 0s - loss: 0.2306 - acc: 0.892 - ETA: 0s - loss: 0.2317 - acc: 0.892 - ETA: 0s - loss: 0.2325 - acc: 0.892 - ETA: 0s - loss: 0.2325 - acc: 0.892 - 1s 52us/step - loss: 0.2318 - acc: 0.8923 - val_loss: 0.5352 - val_acc: 0.8405\n",
      "Epoch 77/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.1512 - acc: 0.968 - ETA: 0s - loss: 0.2113 - acc: 0.908 - ETA: 0s - loss: 0.2130 - acc: 0.909 - ETA: 0s - loss: 0.2227 - acc: 0.904 - ETA: 0s - loss: 0.2241 - acc: 0.903 - ETA: 0s - loss: 0.2261 - acc: 0.900 - ETA: 0s - loss: 0.2329 - acc: 0.896 - ETA: 0s - loss: 0.2313 - acc: 0.896 - ETA: 0s - loss: 0.2288 - acc: 0.897 - ETA: 0s - loss: 0.2297 - acc: 0.897 - ETA: 0s - loss: 0.2285 - acc: 0.897 - ETA: 0s - loss: 0.2302 - acc: 0.896 - ETA: 0s - loss: 0.2294 - acc: 0.896 - ETA: 0s - loss: 0.2295 - acc: 0.895 - ETA: 0s - loss: 0.2293 - acc: 0.895 - ETA: 0s - loss: 0.2294 - acc: 0.895 - ETA: 0s - loss: 0.2299 - acc: 0.895 - ETA: 0s - loss: 0.2312 - acc: 0.894 - 1s 50us/step - loss: 0.2324 - acc: 0.8933 - val_loss: 0.5452 - val_acc: 0.8193\n",
      "Epoch 78/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.1928 - acc: 0.890 - ETA: 0s - loss: 0.2211 - acc: 0.898 - ETA: 0s - loss: 0.2220 - acc: 0.892 - ETA: 0s - loss: 0.2234 - acc: 0.891 - ETA: 0s - loss: 0.2275 - acc: 0.892 - ETA: 0s - loss: 0.2255 - acc: 0.893 - ETA: 0s - loss: 0.2221 - acc: 0.896 - ETA: 0s - loss: 0.2253 - acc: 0.894 - ETA: 0s - loss: 0.2255 - acc: 0.894 - ETA: 0s - loss: 0.2264 - acc: 0.894 - ETA: 0s - loss: 0.2262 - acc: 0.895 - ETA: 0s - loss: 0.2289 - acc: 0.894 - ETA: 0s - loss: 0.2286 - acc: 0.894 - ETA: 0s - loss: 0.2292 - acc: 0.893 - ETA: 0s - loss: 0.2298 - acc: 0.893 - ETA: 0s - loss: 0.2315 - acc: 0.892 - ETA: 0s - loss: 0.2308 - acc: 0.893 - ETA: 0s - loss: 0.2311 - acc: 0.893 - 1s 50us/step - loss: 0.2312 - acc: 0.8943 - val_loss: 0.5187 - val_acc: 0.8317\n",
      "Epoch 79/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1851 - acc: 0.937 - ETA: 0s - loss: 0.2179 - acc: 0.902 - ETA: 0s - loss: 0.2068 - acc: 0.905 - ETA: 0s - loss: 0.2125 - acc: 0.905 - ETA: 0s - loss: 0.2088 - acc: 0.905 - ETA: 0s - loss: 0.2145 - acc: 0.903 - ETA: 0s - loss: 0.2191 - acc: 0.899 - ETA: 0s - loss: 0.2218 - acc: 0.898 - ETA: 0s - loss: 0.2218 - acc: 0.898 - ETA: 0s - loss: 0.2242 - acc: 0.898 - ETA: 0s - loss: 0.2254 - acc: 0.898 - ETA: 0s - loss: 0.2268 - acc: 0.896 - ETA: 0s - loss: 0.2271 - acc: 0.896 - ETA: 0s - loss: 0.2290 - acc: 0.895 - ETA: 0s - loss: 0.2292 - acc: 0.895 - ETA: 0s - loss: 0.2294 - acc: 0.895 - ETA: 0s - loss: 0.2295 - acc: 0.895 - ETA: 0s - loss: 0.2305 - acc: 0.895 - ETA: 0s - loss: 0.2308 - acc: 0.894 - 1s 52us/step - loss: 0.2306 - acc: 0.8952 - val_loss: 0.5096 - val_acc: 0.8430\n",
      "Epoch 80/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.2129 - acc: 0.859 - ETA: 0s - loss: 0.2216 - acc: 0.899 - ETA: 0s - loss: 0.2211 - acc: 0.898 - ETA: 0s - loss: 0.2194 - acc: 0.901 - ETA: 0s - loss: 0.2219 - acc: 0.902 - ETA: 0s - loss: 0.2198 - acc: 0.901 - ETA: 0s - loss: 0.2217 - acc: 0.900 - ETA: 0s - loss: 0.2235 - acc: 0.898 - ETA: 0s - loss: 0.2250 - acc: 0.897 - ETA: 0s - loss: 0.2232 - acc: 0.898 - ETA: 0s - loss: 0.2256 - acc: 0.896 - ETA: 0s - loss: 0.2283 - acc: 0.895 - ETA: 0s - loss: 0.2289 - acc: 0.894 - ETA: 0s - loss: 0.2292 - acc: 0.894 - ETA: 0s - loss: 0.2305 - acc: 0.894 - ETA: 0s - loss: 0.2306 - acc: 0.894 - ETA: 0s - loss: 0.2318 - acc: 0.894 - ETA: 0s - loss: 0.2320 - acc: 0.893 - 1s 51us/step - loss: 0.2309 - acc: 0.8945 - val_loss: 0.5375 - val_acc: 0.8420\n",
      "Epoch 81/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1730 - acc: 0.906 - ETA: 0s - loss: 0.2313 - acc: 0.881 - ETA: 0s - loss: 0.2236 - acc: 0.896 - ETA: 0s - loss: 0.2145 - acc: 0.897 - ETA: 0s - loss: 0.2189 - acc: 0.898 - ETA: 0s - loss: 0.2174 - acc: 0.899 - ETA: 0s - loss: 0.2208 - acc: 0.899 - ETA: 0s - loss: 0.2240 - acc: 0.897 - ETA: 0s - loss: 0.2244 - acc: 0.897 - ETA: 0s - loss: 0.2277 - acc: 0.895 - ETA: 0s - loss: 0.2277 - acc: 0.895 - ETA: 0s - loss: 0.2292 - acc: 0.894 - ETA: 0s - loss: 0.2291 - acc: 0.895 - ETA: 0s - loss: 0.2286 - acc: 0.895 - ETA: 0s - loss: 0.2301 - acc: 0.894 - ETA: 0s - loss: 0.2284 - acc: 0.896 - ETA: 0s - loss: 0.2268 - acc: 0.896 - ETA: 0s - loss: 0.2275 - acc: 0.896 - ETA: 0s - loss: 0.2279 - acc: 0.895 - ETA: 0s - loss: 0.2283 - acc: 0.894 - 1s 56us/step - loss: 0.2295 - acc: 0.8939 - val_loss: 0.5233 - val_acc: 0.8403\n",
      "Epoch 82/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1335 - acc: 0.937 - ETA: 0s - loss: 0.2092 - acc: 0.901 - ETA: 0s - loss: 0.2319 - acc: 0.898 - ETA: 0s - loss: 0.2247 - acc: 0.897 - ETA: 0s - loss: 0.2238 - acc: 0.895 - ETA: 0s - loss: 0.2226 - acc: 0.898 - ETA: 0s - loss: 0.2228 - acc: 0.898 - ETA: 0s - loss: 0.2262 - acc: 0.897 - ETA: 0s - loss: 0.2252 - acc: 0.897 - ETA: 0s - loss: 0.2266 - acc: 0.897 - ETA: 0s - loss: 0.2281 - acc: 0.897 - ETA: 0s - loss: 0.2295 - acc: 0.896 - ETA: 0s - loss: 0.2288 - acc: 0.896 - ETA: 0s - loss: 0.2280 - acc: 0.896 - ETA: 0s - loss: 0.2301 - acc: 0.896 - ETA: 0s - loss: 0.2297 - acc: 0.896 - ETA: 0s - loss: 0.2303 - acc: 0.895 - ETA: 0s - loss: 0.2298 - acc: 0.895 - ETA: 0s - loss: 0.2313 - acc: 0.895 - 1s 53us/step - loss: 0.2314 - acc: 0.8953 - val_loss: 0.5261 - val_acc: 0.8351\n",
      "Epoch 83/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.1566 - acc: 0.921 - ETA: 0s - loss: 0.2411 - acc: 0.892 - ETA: 0s - loss: 0.2419 - acc: 0.888 - ETA: 0s - loss: 0.2373 - acc: 0.887 - ETA: 0s - loss: 0.2318 - acc: 0.890 - ETA: 0s - loss: 0.2283 - acc: 0.893 - ETA: 0s - loss: 0.2266 - acc: 0.895 - ETA: 0s - loss: 0.2247 - acc: 0.896 - ETA: 0s - loss: 0.2272 - acc: 0.896 - ETA: 0s - loss: 0.2282 - acc: 0.896 - ETA: 0s - loss: 0.2255 - acc: 0.898 - ETA: 0s - loss: 0.2264 - acc: 0.898 - ETA: 0s - loss: 0.2269 - acc: 0.897 - ETA: 0s - loss: 0.2261 - acc: 0.897 - ETA: 0s - loss: 0.2267 - acc: 0.897 - ETA: 0s - loss: 0.2268 - acc: 0.897 - ETA: 0s - loss: 0.2276 - acc: 0.897 - ETA: 0s - loss: 0.2281 - acc: 0.896 - ETA: 0s - loss: 0.2288 - acc: 0.895 - ETA: 0s - loss: 0.2291 - acc: 0.895 - 1s 54us/step - loss: 0.2289 - acc: 0.8958 - val_loss: 0.5239 - val_acc: 0.8396\n",
      "Epoch 84/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.2188 - acc: 0.921 - ETA: 0s - loss: 0.2115 - acc: 0.907 - ETA: 0s - loss: 0.2196 - acc: 0.898 - ETA: 0s - loss: 0.2251 - acc: 0.899 - ETA: 0s - loss: 0.2219 - acc: 0.899 - ETA: 0s - loss: 0.2245 - acc: 0.897 - ETA: 0s - loss: 0.2253 - acc: 0.895 - ETA: 0s - loss: 0.2221 - acc: 0.897 - ETA: 0s - loss: 0.2254 - acc: 0.896 - ETA: 0s - loss: 0.2254 - acc: 0.896 - ETA: 0s - loss: 0.2249 - acc: 0.896 - ETA: 0s - loss: 0.2278 - acc: 0.894 - ETA: 0s - loss: 0.2281 - acc: 0.895 - ETA: 0s - loss: 0.2292 - acc: 0.893 - ETA: 0s - loss: 0.2297 - acc: 0.893 - ETA: 0s - loss: 0.2289 - acc: 0.893 - ETA: 0s - loss: 0.2292 - acc: 0.892 - ETA: 0s - loss: 0.2295 - acc: 0.893 - ETA: 0s - loss: 0.2292 - acc: 0.893 - 1s 52us/step - loss: 0.2286 - acc: 0.8933 - val_loss: 0.5341 - val_acc: 0.8427\n",
      "Epoch 85/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.2785 - acc: 0.890 - ETA: 0s - loss: 0.2226 - acc: 0.905 - ETA: 0s - loss: 0.2257 - acc: 0.899 - ETA: 0s - loss: 0.2226 - acc: 0.899 - ETA: 0s - loss: 0.2210 - acc: 0.896 - ETA: 0s - loss: 0.2234 - acc: 0.894 - ETA: 0s - loss: 0.2224 - acc: 0.896 - ETA: 0s - loss: 0.2245 - acc: 0.895 - ETA: 0s - loss: 0.2217 - acc: 0.898 - ETA: 0s - loss: 0.2200 - acc: 0.900 - ETA: 0s - loss: 0.2218 - acc: 0.899 - ETA: 0s - loss: 0.2213 - acc: 0.899 - ETA: 0s - loss: 0.2245 - acc: 0.897 - ETA: 0s - loss: 0.2261 - acc: 0.896 - ETA: 0s - loss: 0.2243 - acc: 0.897 - ETA: 0s - loss: 0.2242 - acc: 0.897 - ETA: 0s - loss: 0.2247 - acc: 0.895 - ETA: 0s - loss: 0.2251 - acc: 0.895 - ETA: 0s - loss: 0.2260 - acc: 0.895 - ETA: 0s - loss: 0.2267 - acc: 0.894 - ETA: 0s - loss: 0.2269 - acc: 0.894 - ETA: 0s - loss: 0.2269 - acc: 0.894 - ETA: 0s - loss: 0.2281 - acc: 0.893 - 1s 64us/step - loss: 0.2280 - acc: 0.8941 - val_loss: 0.5075 - val_acc: 0.8417\n",
      "Epoch 86/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1744 - acc: 0.921 - ETA: 1s - loss: 0.2251 - acc: 0.899 - ETA: 0s - loss: 0.2172 - acc: 0.904 - ETA: 0s - loss: 0.2213 - acc: 0.899 - ETA: 0s - loss: 0.2257 - acc: 0.898 - ETA: 0s - loss: 0.2267 - acc: 0.894 - ETA: 0s - loss: 0.2308 - acc: 0.893 - ETA: 0s - loss: 0.2302 - acc: 0.893 - ETA: 0s - loss: 0.2279 - acc: 0.895 - ETA: 0s - loss: 0.2259 - acc: 0.895 - ETA: 0s - loss: 0.2241 - acc: 0.897 - ETA: 0s - loss: 0.2250 - acc: 0.897 - ETA: 0s - loss: 0.2261 - acc: 0.896 - ETA: 0s - loss: 0.2275 - acc: 0.896 - ETA: 0s - loss: 0.2288 - acc: 0.895 - ETA: 0s - loss: 0.2282 - acc: 0.896 - ETA: 0s - loss: 0.2284 - acc: 0.895 - ETA: 0s - loss: 0.2289 - acc: 0.895 - ETA: 0s - loss: 0.2300 - acc: 0.895 - 1s 52us/step - loss: 0.2294 - acc: 0.8953 - val_loss: 0.5419 - val_acc: 0.8391\n",
      "Epoch 87/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.2779 - acc: 0.843 - ETA: 0s - loss: 0.2397 - acc: 0.890 - ETA: 0s - loss: 0.2207 - acc: 0.898 - ETA: 0s - loss: 0.2289 - acc: 0.895 - ETA: 0s - loss: 0.2285 - acc: 0.897 - ETA: 0s - loss: 0.2323 - acc: 0.893 - ETA: 0s - loss: 0.2319 - acc: 0.893 - ETA: 0s - loss: 0.2285 - acc: 0.894 - ETA: 0s - loss: 0.2288 - acc: 0.893 - ETA: 0s - loss: 0.2284 - acc: 0.894 - ETA: 0s - loss: 0.2289 - acc: 0.895 - ETA: 0s - loss: 0.2276 - acc: 0.896 - ETA: 0s - loss: 0.2280 - acc: 0.895 - ETA: 0s - loss: 0.2280 - acc: 0.895 - ETA: 0s - loss: 0.2277 - acc: 0.895 - ETA: 0s - loss: 0.2291 - acc: 0.894 - ETA: 0s - loss: 0.2293 - acc: 0.894 - ETA: 0s - loss: 0.2286 - acc: 0.894 - 1s 50us/step - loss: 0.2284 - acc: 0.8945 - val_loss: 0.5291 - val_acc: 0.8383\n",
      "Epoch 88/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20513/20513 [==============================] - ETA: 1s - loss: 0.3160 - acc: 0.875 - ETA: 0s - loss: 0.2239 - acc: 0.894 - ETA: 0s - loss: 0.2341 - acc: 0.888 - ETA: 0s - loss: 0.2289 - acc: 0.891 - ETA: 0s - loss: 0.2245 - acc: 0.895 - ETA: 0s - loss: 0.2235 - acc: 0.895 - ETA: 0s - loss: 0.2257 - acc: 0.895 - ETA: 0s - loss: 0.2213 - acc: 0.897 - ETA: 0s - loss: 0.2209 - acc: 0.898 - ETA: 0s - loss: 0.2232 - acc: 0.897 - ETA: 0s - loss: 0.2263 - acc: 0.896 - ETA: 0s - loss: 0.2256 - acc: 0.897 - ETA: 0s - loss: 0.2268 - acc: 0.897 - ETA: 0s - loss: 0.2285 - acc: 0.897 - ETA: 0s - loss: 0.2292 - acc: 0.896 - ETA: 0s - loss: 0.2312 - acc: 0.895 - ETA: 0s - loss: 0.2311 - acc: 0.895 - ETA: 0s - loss: 0.2297 - acc: 0.895 - 1s 49us/step - loss: 0.2293 - acc: 0.8956 - val_loss: 0.5618 - val_acc: 0.8391\n",
      "Epoch 89/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.2182 - acc: 0.921 - ETA: 0s - loss: 0.2307 - acc: 0.896 - ETA: 0s - loss: 0.2175 - acc: 0.908 - ETA: 0s - loss: 0.2201 - acc: 0.906 - ETA: 0s - loss: 0.2192 - acc: 0.905 - ETA: 0s - loss: 0.2242 - acc: 0.899 - ETA: 0s - loss: 0.2232 - acc: 0.899 - ETA: 0s - loss: 0.2267 - acc: 0.897 - ETA: 0s - loss: 0.2297 - acc: 0.895 - ETA: 0s - loss: 0.2302 - acc: 0.894 - ETA: 0s - loss: 0.2300 - acc: 0.894 - ETA: 0s - loss: 0.2277 - acc: 0.895 - ETA: 0s - loss: 0.2272 - acc: 0.895 - ETA: 0s - loss: 0.2255 - acc: 0.896 - ETA: 0s - loss: 0.2252 - acc: 0.896 - ETA: 0s - loss: 0.2259 - acc: 0.897 - ETA: 0s - loss: 0.2275 - acc: 0.895 - ETA: 0s - loss: 0.2278 - acc: 0.895 - 1s 49us/step - loss: 0.2275 - acc: 0.8958 - val_loss: 0.5334 - val_acc: 0.8419\n",
      "Epoch 90/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.2532 - acc: 0.859 - ETA: 0s - loss: 0.2354 - acc: 0.888 - ETA: 0s - loss: 0.2356 - acc: 0.889 - ETA: 0s - loss: 0.2286 - acc: 0.892 - ETA: 0s - loss: 0.2304 - acc: 0.891 - ETA: 0s - loss: 0.2279 - acc: 0.894 - ETA: 0s - loss: 0.2297 - acc: 0.893 - ETA: 0s - loss: 0.2292 - acc: 0.893 - ETA: 0s - loss: 0.2259 - acc: 0.896 - ETA: 0s - loss: 0.2254 - acc: 0.896 - ETA: 0s - loss: 0.2264 - acc: 0.896 - ETA: 0s - loss: 0.2259 - acc: 0.897 - ETA: 0s - loss: 0.2255 - acc: 0.897 - ETA: 0s - loss: 0.2246 - acc: 0.898 - ETA: 0s - loss: 0.2258 - acc: 0.897 - ETA: 0s - loss: 0.2268 - acc: 0.897 - ETA: 0s - loss: 0.2266 - acc: 0.897 - ETA: 0s - loss: 0.2264 - acc: 0.897 - 1s 50us/step - loss: 0.2276 - acc: 0.8972 - val_loss: 0.5469 - val_acc: 0.8337\n",
      "Epoch 91/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.3933 - acc: 0.828 - ETA: 0s - loss: 0.2309 - acc: 0.889 - ETA: 0s - loss: 0.2149 - acc: 0.898 - ETA: 0s - loss: 0.2208 - acc: 0.898 - ETA: 0s - loss: 0.2214 - acc: 0.898 - ETA: 0s - loss: 0.2244 - acc: 0.898 - ETA: 0s - loss: 0.2252 - acc: 0.897 - ETA: 0s - loss: 0.2249 - acc: 0.896 - ETA: 0s - loss: 0.2245 - acc: 0.895 - ETA: 0s - loss: 0.2266 - acc: 0.895 - ETA: 0s - loss: 0.2254 - acc: 0.895 - ETA: 0s - loss: 0.2236 - acc: 0.895 - ETA: 0s - loss: 0.2259 - acc: 0.895 - ETA: 0s - loss: 0.2263 - acc: 0.896 - ETA: 0s - loss: 0.2258 - acc: 0.896 - ETA: 0s - loss: 0.2261 - acc: 0.895 - ETA: 0s - loss: 0.2282 - acc: 0.894 - ETA: 0s - loss: 0.2264 - acc: 0.895 - 1s 50us/step - loss: 0.2264 - acc: 0.8954 - val_loss: 0.5483 - val_acc: 0.8402\n",
      "Epoch 92/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.1990 - acc: 0.906 - ETA: 0s - loss: 0.2259 - acc: 0.895 - ETA: 0s - loss: 0.2205 - acc: 0.891 - ETA: 0s - loss: 0.2178 - acc: 0.890 - ETA: 0s - loss: 0.2149 - acc: 0.893 - ETA: 0s - loss: 0.2155 - acc: 0.896 - ETA: 0s - loss: 0.2148 - acc: 0.896 - ETA: 0s - loss: 0.2156 - acc: 0.896 - ETA: 0s - loss: 0.2195 - acc: 0.895 - ETA: 0s - loss: 0.2197 - acc: 0.896 - ETA: 0s - loss: 0.2210 - acc: 0.896 - ETA: 0s - loss: 0.2222 - acc: 0.895 - ETA: 0s - loss: 0.2235 - acc: 0.895 - ETA: 0s - loss: 0.2226 - acc: 0.896 - ETA: 0s - loss: 0.2225 - acc: 0.897 - ETA: 0s - loss: 0.2221 - acc: 0.897 - ETA: 0s - loss: 0.2232 - acc: 0.897 - ETA: 0s - loss: 0.2257 - acc: 0.895 - 1s 49us/step - loss: 0.2270 - acc: 0.8953 - val_loss: 0.5187 - val_acc: 0.8351\n",
      "Epoch 93/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.2726 - acc: 0.843 - ETA: 0s - loss: 0.2332 - acc: 0.885 - ETA: 0s - loss: 0.2205 - acc: 0.896 - ETA: 0s - loss: 0.2193 - acc: 0.896 - ETA: 0s - loss: 0.2226 - acc: 0.895 - ETA: 0s - loss: 0.2184 - acc: 0.898 - ETA: 0s - loss: 0.2229 - acc: 0.895 - ETA: 0s - loss: 0.2215 - acc: 0.897 - ETA: 0s - loss: 0.2237 - acc: 0.896 - ETA: 0s - loss: 0.2231 - acc: 0.897 - ETA: 0s - loss: 0.2246 - acc: 0.897 - ETA: 0s - loss: 0.2242 - acc: 0.896 - ETA: 0s - loss: 0.2240 - acc: 0.896 - ETA: 0s - loss: 0.2237 - acc: 0.896 - ETA: 0s - loss: 0.2231 - acc: 0.896 - ETA: 0s - loss: 0.2246 - acc: 0.896 - ETA: 0s - loss: 0.2258 - acc: 0.896 - ETA: 0s - loss: 0.2253 - acc: 0.895 - 1s 51us/step - loss: 0.2263 - acc: 0.8948 - val_loss: 0.5202 - val_acc: 0.8379\n",
      "Epoch 94/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.2650 - acc: 0.875 - ETA: 0s - loss: 0.2004 - acc: 0.902 - ETA: 0s - loss: 0.2029 - acc: 0.903 - ETA: 0s - loss: 0.2030 - acc: 0.905 - ETA: 0s - loss: 0.2191 - acc: 0.900 - ETA: 0s - loss: 0.2205 - acc: 0.901 - ETA: 0s - loss: 0.2214 - acc: 0.899 - ETA: 0s - loss: 0.2186 - acc: 0.900 - ETA: 0s - loss: 0.2226 - acc: 0.898 - ETA: 0s - loss: 0.2240 - acc: 0.897 - ETA: 0s - loss: 0.2266 - acc: 0.895 - ETA: 0s - loss: 0.2266 - acc: 0.895 - ETA: 0s - loss: 0.2242 - acc: 0.896 - ETA: 0s - loss: 0.2238 - acc: 0.897 - ETA: 0s - loss: 0.2240 - acc: 0.896 - ETA: 0s - loss: 0.2244 - acc: 0.896 - ETA: 0s - loss: 0.2237 - acc: 0.896 - ETA: 0s - loss: 0.2245 - acc: 0.896 - 1s 50us/step - loss: 0.2250 - acc: 0.8964 - val_loss: 0.5351 - val_acc: 0.8410\n",
      "Epoch 95/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1628 - acc: 0.937 - ETA: 0s - loss: 0.1898 - acc: 0.914 - ETA: 0s - loss: 0.2017 - acc: 0.910 - ETA: 0s - loss: 0.2084 - acc: 0.904 - ETA: 0s - loss: 0.2165 - acc: 0.899 - ETA: 0s - loss: 0.2163 - acc: 0.899 - ETA: 0s - loss: 0.2186 - acc: 0.897 - ETA: 0s - loss: 0.2195 - acc: 0.896 - ETA: 0s - loss: 0.2212 - acc: 0.895 - ETA: 0s - loss: 0.2227 - acc: 0.896 - ETA: 0s - loss: 0.2218 - acc: 0.896 - ETA: 0s - loss: 0.2206 - acc: 0.897 - ETA: 0s - loss: 0.2206 - acc: 0.897 - ETA: 0s - loss: 0.2187 - acc: 0.898 - ETA: 0s - loss: 0.2198 - acc: 0.897 - ETA: 0s - loss: 0.2206 - acc: 0.896 - ETA: 0s - loss: 0.2221 - acc: 0.896 - ETA: 0s - loss: 0.2243 - acc: 0.895 - 1s 52us/step - loss: 0.2263 - acc: 0.8949 - val_loss: 0.5243 - val_acc: 0.8404\n",
      "Epoch 96/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.2030 - acc: 0.906 - ETA: 0s - loss: 0.2043 - acc: 0.898 - ETA: 0s - loss: 0.2185 - acc: 0.902 - ETA: 0s - loss: 0.2173 - acc: 0.902 - ETA: 0s - loss: 0.2184 - acc: 0.900 - ETA: 0s - loss: 0.2192 - acc: 0.898 - ETA: 0s - loss: 0.2169 - acc: 0.898 - ETA: 0s - loss: 0.2218 - acc: 0.896 - ETA: 0s - loss: 0.2221 - acc: 0.896 - ETA: 0s - loss: 0.2224 - acc: 0.895 - ETA: 0s - loss: 0.2241 - acc: 0.895 - ETA: 0s - loss: 0.2224 - acc: 0.896 - ETA: 0s - loss: 0.2222 - acc: 0.896 - ETA: 0s - loss: 0.2239 - acc: 0.896 - ETA: 0s - loss: 0.2229 - acc: 0.896 - ETA: 0s - loss: 0.2236 - acc: 0.896 - ETA: 0s - loss: 0.2233 - acc: 0.896 - ETA: 0s - loss: 0.2250 - acc: 0.895 - ETA: 0s - loss: 0.2252 - acc: 0.895 - ETA: 0s - loss: 0.2265 - acc: 0.895 - ETA: 0s - loss: 0.2268 - acc: 0.896 - 1s 57us/step - loss: 0.2268 - acc: 0.8964 - val_loss: 0.5544 - val_acc: 0.8350\n",
      "Epoch 97/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.2118 - acc: 0.890 - ETA: 0s - loss: 0.2335 - acc: 0.903 - ETA: 0s - loss: 0.2242 - acc: 0.900 - ETA: 0s - loss: 0.2198 - acc: 0.899 - ETA: 0s - loss: 0.2204 - acc: 0.898 - ETA: 0s - loss: 0.2228 - acc: 0.896 - ETA: 0s - loss: 0.2231 - acc: 0.895 - ETA: 0s - loss: 0.2200 - acc: 0.897 - ETA: 0s - loss: 0.2197 - acc: 0.897 - ETA: 0s - loss: 0.2207 - acc: 0.895 - ETA: 0s - loss: 0.2200 - acc: 0.896 - ETA: 0s - loss: 0.2225 - acc: 0.895 - ETA: 0s - loss: 0.2240 - acc: 0.895 - ETA: 0s - loss: 0.2245 - acc: 0.895 - ETA: 0s - loss: 0.2239 - acc: 0.895 - ETA: 0s - loss: 0.2240 - acc: 0.895 - ETA: 0s - loss: 0.2248 - acc: 0.894 - ETA: 0s - loss: 0.2250 - acc: 0.894 - 1s 50us/step - loss: 0.2256 - acc: 0.8944 - val_loss: 0.5150 - val_acc: 0.8359\n",
      "Epoch 98/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.2562 - acc: 0.875 - ETA: 0s - loss: 0.1944 - acc: 0.907 - ETA: 0s - loss: 0.2170 - acc: 0.896 - ETA: 0s - loss: 0.2166 - acc: 0.894 - ETA: 0s - loss: 0.2166 - acc: 0.894 - ETA: 0s - loss: 0.2155 - acc: 0.895 - ETA: 0s - loss: 0.2116 - acc: 0.898 - ETA: 0s - loss: 0.2146 - acc: 0.899 - ETA: 0s - loss: 0.2158 - acc: 0.898 - ETA: 0s - loss: 0.2161 - acc: 0.899 - ETA: 0s - loss: 0.2185 - acc: 0.899 - ETA: 0s - loss: 0.2204 - acc: 0.899 - ETA: 0s - loss: 0.2218 - acc: 0.898 - ETA: 0s - loss: 0.2224 - acc: 0.898 - ETA: 0s - loss: 0.2224 - acc: 0.897 - ETA: 0s - loss: 0.2221 - acc: 0.898 - ETA: 0s - loss: 0.2227 - acc: 0.897 - ETA: 0s - loss: 0.2233 - acc: 0.897 - 1s 49us/step - loss: 0.2243 - acc: 0.8969 - val_loss: 0.5328 - val_acc: 0.8420\n",
      "Epoch 99/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1400 - acc: 0.921 - ETA: 0s - loss: 0.2089 - acc: 0.902 - ETA: 0s - loss: 0.2184 - acc: 0.897 - ETA: 0s - loss: 0.2170 - acc: 0.897 - ETA: 0s - loss: 0.2209 - acc: 0.896 - ETA: 0s - loss: 0.2236 - acc: 0.896 - ETA: 0s - loss: 0.2227 - acc: 0.897 - ETA: 0s - loss: 0.2214 - acc: 0.899 - ETA: 0s - loss: 0.2206 - acc: 0.899 - ETA: 0s - loss: 0.2245 - acc: 0.896 - ETA: 0s - loss: 0.2251 - acc: 0.897 - ETA: 0s - loss: 0.2261 - acc: 0.897 - ETA: 0s - loss: 0.2261 - acc: 0.896 - ETA: 0s - loss: 0.2278 - acc: 0.896 - ETA: 0s - loss: 0.2279 - acc: 0.895 - ETA: 0s - loss: 0.2275 - acc: 0.895 - ETA: 0s - loss: 0.2279 - acc: 0.895 - ETA: 0s - loss: 0.2269 - acc: 0.895 - 1s 50us/step - loss: 0.2261 - acc: 0.8960 - val_loss: 0.5503 - val_acc: 0.8395\n",
      "Epoch 100/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1015 - acc: 0.984 - ETA: 0s - loss: 0.2126 - acc: 0.905 - ETA: 0s - loss: 0.2263 - acc: 0.898 - ETA: 0s - loss: 0.2198 - acc: 0.902 - ETA: 0s - loss: 0.2153 - acc: 0.904 - ETA: 0s - loss: 0.2206 - acc: 0.903 - ETA: 0s - loss: 0.2201 - acc: 0.904 - ETA: 0s - loss: 0.2189 - acc: 0.903 - ETA: 0s - loss: 0.2190 - acc: 0.902 - ETA: 0s - loss: 0.2202 - acc: 0.901 - ETA: 0s - loss: 0.2223 - acc: 0.900 - ETA: 0s - loss: 0.2239 - acc: 0.900 - ETA: 0s - loss: 0.2252 - acc: 0.899 - ETA: 0s - loss: 0.2250 - acc: 0.899 - ETA: 0s - loss: 0.2231 - acc: 0.899 - ETA: 0s - loss: 0.2251 - acc: 0.898 - ETA: 0s - loss: 0.2238 - acc: 0.898 - ETA: 0s - loss: 0.2240 - acc: 0.898 - 1s 51us/step - loss: 0.2257 - acc: 0.8975 - val_loss: 0.5571 - val_acc: 0.8397\n",
      "Epoch 101/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.2788 - acc: 0.843 - ETA: 0s - loss: 0.2203 - acc: 0.886 - ETA: 0s - loss: 0.2167 - acc: 0.893 - ETA: 0s - loss: 0.2213 - acc: 0.895 - ETA: 0s - loss: 0.2182 - acc: 0.897 - ETA: 0s - loss: 0.2209 - acc: 0.896 - ETA: 0s - loss: 0.2251 - acc: 0.895 - ETA: 0s - loss: 0.2224 - acc: 0.896 - ETA: 0s - loss: 0.2213 - acc: 0.897 - ETA: 0s - loss: 0.2214 - acc: 0.896 - ETA: 0s - loss: 0.2241 - acc: 0.895 - ETA: 0s - loss: 0.2240 - acc: 0.895 - ETA: 0s - loss: 0.2229 - acc: 0.896 - ETA: 0s - loss: 0.2212 - acc: 0.897 - ETA: 0s - loss: 0.2212 - acc: 0.897 - ETA: 0s - loss: 0.2229 - acc: 0.896 - ETA: 0s - loss: 0.2228 - acc: 0.896 - ETA: 0s - loss: 0.2225 - acc: 0.896 - ETA: 0s - loss: 0.2246 - acc: 0.895 - 1s 52us/step - loss: 0.2249 - acc: 0.8959 - val_loss: 0.5596 - val_acc: 0.8420\n",
      "Epoch 102/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.2202 - acc: 0.906 - ETA: 0s - loss: 0.2152 - acc: 0.901 - ETA: 0s - loss: 0.2273 - acc: 0.898 - ETA: 0s - loss: 0.2194 - acc: 0.901 - ETA: 0s - loss: 0.2166 - acc: 0.902 - ETA: 0s - loss: 0.2176 - acc: 0.901 - ETA: 0s - loss: 0.2166 - acc: 0.902 - ETA: 0s - loss: 0.2168 - acc: 0.902 - ETA: 0s - loss: 0.2178 - acc: 0.900 - ETA: 0s - loss: 0.2176 - acc: 0.899 - ETA: 0s - loss: 0.2195 - acc: 0.899 - ETA: 0s - loss: 0.2194 - acc: 0.899 - ETA: 0s - loss: 0.2223 - acc: 0.897 - ETA: 0s - loss: 0.2239 - acc: 0.897 - ETA: 0s - loss: 0.2240 - acc: 0.897 - ETA: 0s - loss: 0.2240 - acc: 0.896 - ETA: 0s - loss: 0.2245 - acc: 0.896 - ETA: 0s - loss: 0.2235 - acc: 0.896 - 1s 50us/step - loss: 0.2237 - acc: 0.8967 - val_loss: 0.5686 - val_acc: 0.8351\n",
      "Epoch 103/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.2814 - acc: 0.843 - ETA: 0s - loss: 0.2020 - acc: 0.906 - ETA: 0s - loss: 0.1998 - acc: 0.905 - ETA: 0s - loss: 0.2052 - acc: 0.904 - ETA: 0s - loss: 0.2089 - acc: 0.902 - ETA: 0s - loss: 0.2090 - acc: 0.903 - ETA: 0s - loss: 0.2104 - acc: 0.902 - ETA: 0s - loss: 0.2118 - acc: 0.903 - ETA: 0s - loss: 0.2137 - acc: 0.901 - ETA: 0s - loss: 0.2136 - acc: 0.902 - ETA: 0s - loss: 0.2164 - acc: 0.899 - ETA: 0s - loss: 0.2182 - acc: 0.898 - ETA: 0s - loss: 0.2197 - acc: 0.898 - ETA: 0s - loss: 0.2230 - acc: 0.897 - ETA: 0s - loss: 0.2217 - acc: 0.898 - ETA: 0s - loss: 0.2219 - acc: 0.898 - ETA: 0s - loss: 0.2232 - acc: 0.898 - ETA: 0s - loss: 0.2241 - acc: 0.897 - ETA: 0s - loss: 0.2252 - acc: 0.896 - 1s 51us/step - loss: 0.2258 - acc: 0.8962 - val_loss: 0.5116 - val_acc: 0.8413\n",
      "Epoch 104/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.2064 - acc: 0.906 - ETA: 0s - loss: 0.2069 - acc: 0.914 - ETA: 0s - loss: 0.2101 - acc: 0.912 - ETA: 0s - loss: 0.2258 - acc: 0.904 - ETA: 0s - loss: 0.2238 - acc: 0.903 - ETA: 0s - loss: 0.2207 - acc: 0.902 - ETA: 0s - loss: 0.2209 - acc: 0.901 - ETA: 0s - loss: 0.2217 - acc: 0.900 - ETA: 0s - loss: 0.2243 - acc: 0.898 - ETA: 0s - loss: 0.2252 - acc: 0.897 - ETA: 0s - loss: 0.2240 - acc: 0.897 - ETA: 0s - loss: 0.2243 - acc: 0.897 - ETA: 0s - loss: 0.2226 - acc: 0.898 - ETA: 0s - loss: 0.2225 - acc: 0.898 - ETA: 0s - loss: 0.2220 - acc: 0.898 - ETA: 0s - loss: 0.2224 - acc: 0.898 - ETA: 0s - loss: 0.2228 - acc: 0.897 - ETA: 0s - loss: 0.2232 - acc: 0.897 - 1s 51us/step - loss: 0.2241 - acc: 0.8974 - val_loss: 0.5727 - val_acc: 0.8399\n",
      "Epoch 105/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.2595 - acc: 0.921 - ETA: 0s - loss: 0.2197 - acc: 0.894 - ETA: 0s - loss: 0.2259 - acc: 0.898 - ETA: 0s - loss: 0.2231 - acc: 0.899 - ETA: 0s - loss: 0.2262 - acc: 0.899 - ETA: 0s - loss: 0.2218 - acc: 0.901 - ETA: 0s - loss: 0.2219 - acc: 0.901 - ETA: 0s - loss: 0.2241 - acc: 0.898 - ETA: 0s - loss: 0.2250 - acc: 0.898 - ETA: 0s - loss: 0.2238 - acc: 0.898 - ETA: 0s - loss: 0.2234 - acc: 0.898 - ETA: 0s - loss: 0.2222 - acc: 0.898 - ETA: 0s - loss: 0.2204 - acc: 0.900 - ETA: 0s - loss: 0.2201 - acc: 0.900 - ETA: 0s - loss: 0.2205 - acc: 0.900 - ETA: 0s - loss: 0.2214 - acc: 0.900 - ETA: 0s - loss: 0.2225 - acc: 0.899 - ETA: 0s - loss: 0.2233 - acc: 0.899 - 1s 50us/step - loss: 0.2231 - acc: 0.8991 - val_loss: 0.5379 - val_acc: 0.8424\n",
      "Epoch 106/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20513/20513 [==============================] - ETA: 1s - loss: 0.2295 - acc: 0.890 - ETA: 0s - loss: 0.2105 - acc: 0.912 - ETA: 0s - loss: 0.2127 - acc: 0.904 - ETA: 0s - loss: 0.2186 - acc: 0.899 - ETA: 0s - loss: 0.2160 - acc: 0.900 - ETA: 0s - loss: 0.2179 - acc: 0.900 - ETA: 0s - loss: 0.2186 - acc: 0.901 - ETA: 0s - loss: 0.2230 - acc: 0.899 - ETA: 0s - loss: 0.2215 - acc: 0.900 - ETA: 0s - loss: 0.2193 - acc: 0.900 - ETA: 0s - loss: 0.2229 - acc: 0.898 - ETA: 0s - loss: 0.2218 - acc: 0.898 - ETA: 0s - loss: 0.2215 - acc: 0.898 - ETA: 0s - loss: 0.2211 - acc: 0.898 - ETA: 0s - loss: 0.2226 - acc: 0.897 - ETA: 0s - loss: 0.2244 - acc: 0.897 - ETA: 0s - loss: 0.2266 - acc: 0.896 - ETA: 0s - loss: 0.2242 - acc: 0.897 - 1s 50us/step - loss: 0.2237 - acc: 0.8985 - val_loss: 0.5826 - val_acc: 0.8422\n",
      "Epoch 107/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.2216 - acc: 0.921 - ETA: 0s - loss: 0.2189 - acc: 0.896 - ETA: 0s - loss: 0.2215 - acc: 0.895 - ETA: 0s - loss: 0.2218 - acc: 0.895 - ETA: 0s - loss: 0.2220 - acc: 0.893 - ETA: 0s - loss: 0.2272 - acc: 0.892 - ETA: 0s - loss: 0.2244 - acc: 0.894 - ETA: 0s - loss: 0.2218 - acc: 0.896 - ETA: 0s - loss: 0.2211 - acc: 0.896 - ETA: 0s - loss: 0.2203 - acc: 0.897 - ETA: 0s - loss: 0.2209 - acc: 0.897 - ETA: 0s - loss: 0.2190 - acc: 0.899 - ETA: 0s - loss: 0.2210 - acc: 0.899 - ETA: 0s - loss: 0.2214 - acc: 0.898 - ETA: 0s - loss: 0.2225 - acc: 0.898 - ETA: 0s - loss: 0.2211 - acc: 0.899 - ETA: 0s - loss: 0.2217 - acc: 0.898 - ETA: 0s - loss: 0.2225 - acc: 0.898 - 1s 50us/step - loss: 0.2220 - acc: 0.8988 - val_loss: 0.5809 - val_acc: 0.8412\n",
      "Epoch 108/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.3220 - acc: 0.812 - ETA: 0s - loss: 0.2455 - acc: 0.877 - ETA: 0s - loss: 0.2413 - acc: 0.880 - ETA: 0s - loss: 0.2431 - acc: 0.883 - ETA: 0s - loss: 0.2370 - acc: 0.887 - ETA: 0s - loss: 0.2310 - acc: 0.891 - ETA: 0s - loss: 0.2276 - acc: 0.892 - ETA: 0s - loss: 0.2231 - acc: 0.895 - ETA: 0s - loss: 0.2215 - acc: 0.895 - ETA: 0s - loss: 0.2196 - acc: 0.897 - ETA: 0s - loss: 0.2211 - acc: 0.895 - ETA: 0s - loss: 0.2217 - acc: 0.896 - ETA: 0s - loss: 0.2224 - acc: 0.896 - ETA: 0s - loss: 0.2230 - acc: 0.896 - ETA: 0s - loss: 0.2265 - acc: 0.897 - ETA: 0s - loss: 0.2278 - acc: 0.896 - ETA: 0s - loss: 0.2270 - acc: 0.896 - ETA: 0s - loss: 0.2278 - acc: 0.896 - 1s 50us/step - loss: 0.2270 - acc: 0.8970 - val_loss: 0.5643 - val_acc: 0.8418\n",
      "Epoch 109/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.3099 - acc: 0.875 - ETA: 0s - loss: 0.2002 - acc: 0.907 - ETA: 0s - loss: 0.2012 - acc: 0.904 - ETA: 0s - loss: 0.2028 - acc: 0.902 - ETA: 0s - loss: 0.1979 - acc: 0.904 - ETA: 0s - loss: 0.2035 - acc: 0.902 - ETA: 0s - loss: 0.2103 - acc: 0.900 - ETA: 0s - loss: 0.2126 - acc: 0.898 - ETA: 0s - loss: 0.2139 - acc: 0.899 - ETA: 0s - loss: 0.2144 - acc: 0.900 - ETA: 0s - loss: 0.2155 - acc: 0.901 - ETA: 0s - loss: 0.2172 - acc: 0.899 - ETA: 0s - loss: 0.2163 - acc: 0.900 - ETA: 0s - loss: 0.2159 - acc: 0.901 - ETA: 0s - loss: 0.2160 - acc: 0.901 - ETA: 0s - loss: 0.2175 - acc: 0.900 - ETA: 0s - loss: 0.2180 - acc: 0.900 - ETA: 0s - loss: 0.2210 - acc: 0.898 - 1s 51us/step - loss: 0.2224 - acc: 0.8983 - val_loss: 0.5855 - val_acc: 0.8355\n",
      "Epoch 110/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.2290 - acc: 0.875 - ETA: 0s - loss: 0.2093 - acc: 0.899 - ETA: 0s - loss: 0.2141 - acc: 0.898 - ETA: 0s - loss: 0.2145 - acc: 0.896 - ETA: 0s - loss: 0.2196 - acc: 0.894 - ETA: 0s - loss: 0.2203 - acc: 0.895 - ETA: 0s - loss: 0.2222 - acc: 0.895 - ETA: 0s - loss: 0.2248 - acc: 0.893 - ETA: 0s - loss: 0.2246 - acc: 0.893 - ETA: 0s - loss: 0.2230 - acc: 0.894 - ETA: 0s - loss: 0.2214 - acc: 0.896 - ETA: 0s - loss: 0.2221 - acc: 0.896 - ETA: 0s - loss: 0.2211 - acc: 0.896 - ETA: 0s - loss: 0.2231 - acc: 0.896 - ETA: 0s - loss: 0.2249 - acc: 0.895 - ETA: 0s - loss: 0.2254 - acc: 0.896 - ETA: 0s - loss: 0.2255 - acc: 0.895 - ETA: 0s - loss: 0.2248 - acc: 0.896 - ETA: 0s - loss: 0.2239 - acc: 0.896 - ETA: 0s - loss: 0.2226 - acc: 0.897 - 1s 55us/step - loss: 0.2228 - acc: 0.8979 - val_loss: 0.5714 - val_acc: 0.8350\n",
      "Epoch 111/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.2657 - acc: 0.843 - ETA: 0s - loss: 0.1974 - acc: 0.908 - ETA: 0s - loss: 0.2121 - acc: 0.906 - ETA: 0s - loss: 0.2176 - acc: 0.901 - ETA: 0s - loss: 0.2181 - acc: 0.900 - ETA: 0s - loss: 0.2153 - acc: 0.901 - ETA: 0s - loss: 0.2209 - acc: 0.898 - ETA: 0s - loss: 0.2193 - acc: 0.898 - ETA: 0s - loss: 0.2174 - acc: 0.899 - ETA: 0s - loss: 0.2190 - acc: 0.898 - ETA: 0s - loss: 0.2182 - acc: 0.899 - ETA: 0s - loss: 0.2209 - acc: 0.897 - ETA: 0s - loss: 0.2221 - acc: 0.897 - ETA: 0s - loss: 0.2221 - acc: 0.897 - ETA: 0s - loss: 0.2210 - acc: 0.897 - ETA: 0s - loss: 0.2207 - acc: 0.897 - ETA: 0s - loss: 0.2223 - acc: 0.897 - ETA: 0s - loss: 0.2229 - acc: 0.897 - ETA: 0s - loss: 0.2230 - acc: 0.897 - 1s 51us/step - loss: 0.2232 - acc: 0.8976 - val_loss: 0.5732 - val_acc: 0.8369\n",
      "Epoch 112/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.2388 - acc: 0.890 - ETA: 0s - loss: 0.2073 - acc: 0.910 - ETA: 0s - loss: 0.2218 - acc: 0.902 - ETA: 0s - loss: 0.2157 - acc: 0.900 - ETA: 0s - loss: 0.2151 - acc: 0.899 - ETA: 0s - loss: 0.2167 - acc: 0.898 - ETA: 0s - loss: 0.2192 - acc: 0.897 - ETA: 0s - loss: 0.2195 - acc: 0.896 - ETA: 0s - loss: 0.2194 - acc: 0.896 - ETA: 0s - loss: 0.2206 - acc: 0.896 - ETA: 0s - loss: 0.2193 - acc: 0.897 - ETA: 0s - loss: 0.2200 - acc: 0.897 - ETA: 0s - loss: 0.2222 - acc: 0.897 - ETA: 0s - loss: 0.2231 - acc: 0.897 - ETA: 0s - loss: 0.2244 - acc: 0.896 - ETA: 0s - loss: 0.2241 - acc: 0.896 - ETA: 0s - loss: 0.2226 - acc: 0.897 - ETA: 0s - loss: 0.2226 - acc: 0.898 - ETA: 0s - loss: 0.2204 - acc: 0.899 - 1s 52us/step - loss: 0.2202 - acc: 0.8995 - val_loss: 0.5821 - val_acc: 0.8384\n",
      "Epoch 113/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1598 - acc: 0.937 - ETA: 0s - loss: 0.2255 - acc: 0.897 - ETA: 0s - loss: 0.2220 - acc: 0.898 - ETA: 0s - loss: 0.2169 - acc: 0.900 - ETA: 0s - loss: 0.2212 - acc: 0.899 - ETA: 0s - loss: 0.2234 - acc: 0.899 - ETA: 0s - loss: 0.2226 - acc: 0.899 - ETA: 0s - loss: 0.2223 - acc: 0.899 - ETA: 0s - loss: 0.2198 - acc: 0.899 - ETA: 0s - loss: 0.2195 - acc: 0.899 - ETA: 0s - loss: 0.2198 - acc: 0.899 - ETA: 0s - loss: 0.2204 - acc: 0.899 - ETA: 0s - loss: 0.2199 - acc: 0.899 - ETA: 0s - loss: 0.2207 - acc: 0.898 - ETA: 0s - loss: 0.2190 - acc: 0.899 - ETA: 0s - loss: 0.2197 - acc: 0.899 - ETA: 0s - loss: 0.2212 - acc: 0.898 - ETA: 0s - loss: 0.2216 - acc: 0.898 - ETA: 0s - loss: 0.2216 - acc: 0.897 - ETA: 0s - loss: 0.2215 - acc: 0.897 - 1s 57us/step - loss: 0.2226 - acc: 0.8980 - val_loss: 0.5794 - val_acc: 0.8341\n",
      "Epoch 114/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1875 - acc: 0.875 - ETA: 0s - loss: 0.2425 - acc: 0.882 - ETA: 0s - loss: 0.2199 - acc: 0.898 - ETA: 1s - loss: 0.2138 - acc: 0.901 - ETA: 0s - loss: 0.2135 - acc: 0.904 - ETA: 0s - loss: 0.2172 - acc: 0.905 - ETA: 0s - loss: 0.2147 - acc: 0.906 - ETA: 0s - loss: 0.2148 - acc: 0.904 - ETA: 0s - loss: 0.2158 - acc: 0.903 - ETA: 0s - loss: 0.2203 - acc: 0.902 - ETA: 0s - loss: 0.2188 - acc: 0.901 - ETA: 0s - loss: 0.2183 - acc: 0.900 - ETA: 0s - loss: 0.2178 - acc: 0.901 - ETA: 0s - loss: 0.2206 - acc: 0.900 - ETA: 0s - loss: 0.2190 - acc: 0.900 - ETA: 0s - loss: 0.2191 - acc: 0.900 - ETA: 0s - loss: 0.2199 - acc: 0.899 - ETA: 0s - loss: 0.2199 - acc: 0.899 - ETA: 0s - loss: 0.2203 - acc: 0.899 - 1s 53us/step - loss: 0.2212 - acc: 0.8989 - val_loss: 0.5883 - val_acc: 0.8334\n",
      "Epoch 115/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.2155 - acc: 0.937 - ETA: 0s - loss: 0.2276 - acc: 0.911 - ETA: 0s - loss: 0.2220 - acc: 0.905 - ETA: 0s - loss: 0.2191 - acc: 0.905 - ETA: 0s - loss: 0.2155 - acc: 0.903 - ETA: 0s - loss: 0.2191 - acc: 0.902 - ETA: 0s - loss: 0.2190 - acc: 0.900 - ETA: 0s - loss: 0.2199 - acc: 0.898 - ETA: 0s - loss: 0.2160 - acc: 0.900 - ETA: 0s - loss: 0.2151 - acc: 0.900 - ETA: 0s - loss: 0.2147 - acc: 0.900 - ETA: 0s - loss: 0.2151 - acc: 0.899 - ETA: 0s - loss: 0.2147 - acc: 0.900 - ETA: 0s - loss: 0.2151 - acc: 0.901 - ETA: 0s - loss: 0.2153 - acc: 0.900 - ETA: 0s - loss: 0.2183 - acc: 0.899 - ETA: 0s - loss: 0.2202 - acc: 0.898 - ETA: 0s - loss: 0.2220 - acc: 0.897 - 1s 49us/step - loss: 0.2228 - acc: 0.8976 - val_loss: 0.5598 - val_acc: 0.8388\n",
      "Epoch 116/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.2176 - acc: 0.906 - ETA: 0s - loss: 0.2228 - acc: 0.901 - ETA: 0s - loss: 0.2236 - acc: 0.898 - ETA: 0s - loss: 0.2169 - acc: 0.900 - ETA: 0s - loss: 0.2187 - acc: 0.897 - ETA: 0s - loss: 0.2239 - acc: 0.896 - ETA: 0s - loss: 0.2189 - acc: 0.898 - ETA: 0s - loss: 0.2230 - acc: 0.898 - ETA: 0s - loss: 0.2222 - acc: 0.899 - ETA: 0s - loss: 0.2238 - acc: 0.899 - ETA: 0s - loss: 0.2245 - acc: 0.897 - ETA: 0s - loss: 0.2244 - acc: 0.897 - ETA: 0s - loss: 0.2231 - acc: 0.897 - ETA: 0s - loss: 0.2233 - acc: 0.896 - ETA: 0s - loss: 0.2220 - acc: 0.898 - ETA: 0s - loss: 0.2218 - acc: 0.898 - ETA: 0s - loss: 0.2203 - acc: 0.899 - ETA: 0s - loss: 0.2214 - acc: 0.898 - 1s 51us/step - loss: 0.2212 - acc: 0.8991 - val_loss: 0.5620 - val_acc: 0.8413\n",
      "Epoch 117/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.2165 - acc: 0.906 - ETA: 0s - loss: 0.2028 - acc: 0.900 - ETA: 0s - loss: 0.2133 - acc: 0.902 - ETA: 0s - loss: 0.2140 - acc: 0.901 - ETA: 0s - loss: 0.2140 - acc: 0.901 - ETA: 0s - loss: 0.2106 - acc: 0.901 - ETA: 0s - loss: 0.2136 - acc: 0.901 - ETA: 0s - loss: 0.2116 - acc: 0.901 - ETA: 0s - loss: 0.2145 - acc: 0.899 - ETA: 0s - loss: 0.2139 - acc: 0.900 - ETA: 0s - loss: 0.2151 - acc: 0.898 - ETA: 0s - loss: 0.2163 - acc: 0.898 - ETA: 0s - loss: 0.2182 - acc: 0.897 - ETA: 0s - loss: 0.2175 - acc: 0.898 - ETA: 0s - loss: 0.2206 - acc: 0.897 - ETA: 0s - loss: 0.2210 - acc: 0.897 - ETA: 0s - loss: 0.2220 - acc: 0.896 - ETA: 0s - loss: 0.2220 - acc: 0.896 - 1s 49us/step - loss: 0.2217 - acc: 0.8970 - val_loss: 0.5697 - val_acc: 0.8355\n",
      "Epoch 118/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.2437 - acc: 0.890 - ETA: 0s - loss: 0.2361 - acc: 0.889 - ETA: 0s - loss: 0.2274 - acc: 0.893 - ETA: 0s - loss: 0.2232 - acc: 0.892 - ETA: 0s - loss: 0.2197 - acc: 0.894 - ETA: 0s - loss: 0.2239 - acc: 0.895 - ETA: 0s - loss: 0.2229 - acc: 0.894 - ETA: 0s - loss: 0.2233 - acc: 0.894 - ETA: 0s - loss: 0.2213 - acc: 0.895 - ETA: 0s - loss: 0.2200 - acc: 0.896 - ETA: 0s - loss: 0.2178 - acc: 0.898 - ETA: 0s - loss: 0.2174 - acc: 0.899 - ETA: 0s - loss: 0.2212 - acc: 0.896 - ETA: 0s - loss: 0.2205 - acc: 0.897 - ETA: 0s - loss: 0.2194 - acc: 0.898 - ETA: 0s - loss: 0.2205 - acc: 0.897 - ETA: 0s - loss: 0.2202 - acc: 0.897 - ETA: 0s - loss: 0.2212 - acc: 0.897 - 1s 50us/step - loss: 0.2202 - acc: 0.8977 - val_loss: 0.5831 - val_acc: 0.8375\n",
      "Epoch 119/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.3365 - acc: 0.796 - ETA: 0s - loss: 0.2155 - acc: 0.893 - ETA: 0s - loss: 0.2131 - acc: 0.897 - ETA: 0s - loss: 0.2245 - acc: 0.893 - ETA: 0s - loss: 0.2167 - acc: 0.899 - ETA: 0s - loss: 0.2193 - acc: 0.897 - ETA: 0s - loss: 0.2174 - acc: 0.899 - ETA: 0s - loss: 0.2169 - acc: 0.900 - ETA: 0s - loss: 0.2166 - acc: 0.898 - ETA: 0s - loss: 0.2176 - acc: 0.898 - ETA: 0s - loss: 0.2162 - acc: 0.899 - ETA: 0s - loss: 0.2190 - acc: 0.897 - ETA: 0s - loss: 0.2191 - acc: 0.897 - ETA: 0s - loss: 0.2173 - acc: 0.898 - ETA: 0s - loss: 0.2166 - acc: 0.899 - ETA: 0s - loss: 0.2171 - acc: 0.899 - ETA: 0s - loss: 0.2179 - acc: 0.899 - ETA: 0s - loss: 0.2198 - acc: 0.898 - ETA: 0s - loss: 0.2192 - acc: 0.898 - 1s 51us/step - loss: 0.2193 - acc: 0.8989 - val_loss: 0.5744 - val_acc: 0.8402\n",
      "Epoch 120/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.2928 - acc: 0.843 - ETA: 0s - loss: 0.1888 - acc: 0.914 - ETA: 0s - loss: 0.2050 - acc: 0.910 - ETA: 0s - loss: 0.2150 - acc: 0.905 - ETA: 0s - loss: 0.2119 - acc: 0.905 - ETA: 0s - loss: 0.2146 - acc: 0.906 - ETA: 0s - loss: 0.2116 - acc: 0.906 - ETA: 0s - loss: 0.2114 - acc: 0.906 - ETA: 0s - loss: 0.2092 - acc: 0.907 - ETA: 0s - loss: 0.2098 - acc: 0.906 - ETA: 0s - loss: 0.2105 - acc: 0.905 - ETA: 0s - loss: 0.2113 - acc: 0.905 - ETA: 0s - loss: 0.2142 - acc: 0.903 - ETA: 0s - loss: 0.2154 - acc: 0.903 - ETA: 0s - loss: 0.2170 - acc: 0.902 - ETA: 0s - loss: 0.2169 - acc: 0.902 - ETA: 0s - loss: 0.2197 - acc: 0.901 - ETA: 0s - loss: 0.2195 - acc: 0.901 - 1s 50us/step - loss: 0.2208 - acc: 0.9010 - val_loss: 0.5635 - val_acc: 0.8380\n",
      "Epoch 121/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.1517 - acc: 0.921 - ETA: 0s - loss: 0.2078 - acc: 0.907 - ETA: 0s - loss: 0.2172 - acc: 0.900 - ETA: 0s - loss: 0.2186 - acc: 0.901 - ETA: 0s - loss: 0.2254 - acc: 0.897 - ETA: 0s - loss: 0.2268 - acc: 0.897 - ETA: 0s - loss: 0.2251 - acc: 0.900 - ETA: 0s - loss: 0.2233 - acc: 0.900 - ETA: 0s - loss: 0.2238 - acc: 0.901 - ETA: 0s - loss: 0.2234 - acc: 0.900 - ETA: 0s - loss: 0.2218 - acc: 0.900 - ETA: 0s - loss: 0.2231 - acc: 0.900 - ETA: 0s - loss: 0.2224 - acc: 0.900 - ETA: 0s - loss: 0.2219 - acc: 0.900 - ETA: 0s - loss: 0.2211 - acc: 0.900 - ETA: 0s - loss: 0.2214 - acc: 0.899 - ETA: 0s - loss: 0.2215 - acc: 0.899 - ETA: 0s - loss: 0.2219 - acc: 0.899 - 1s 50us/step - loss: 0.2215 - acc: 0.8994 - val_loss: 0.6050 - val_acc: 0.8212\n",
      "Epoch 122/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.2026 - acc: 0.921 - ETA: 1s - loss: 0.1979 - acc: 0.913 - ETA: 0s - loss: 0.2003 - acc: 0.904 - ETA: 0s - loss: 0.2012 - acc: 0.902 - ETA: 0s - loss: 0.2063 - acc: 0.901 - ETA: 0s - loss: 0.2092 - acc: 0.900 - ETA: 0s - loss: 0.2088 - acc: 0.901 - ETA: 0s - loss: 0.2090 - acc: 0.900 - ETA: 0s - loss: 0.2144 - acc: 0.899 - ETA: 0s - loss: 0.2148 - acc: 0.898 - ETA: 0s - loss: 0.2181 - acc: 0.897 - ETA: 0s - loss: 0.2177 - acc: 0.897 - ETA: 0s - loss: 0.2182 - acc: 0.898 - ETA: 0s - loss: 0.2205 - acc: 0.897 - ETA: 0s - loss: 0.2195 - acc: 0.897 - ETA: 0s - loss: 0.2216 - acc: 0.897 - ETA: 0s - loss: 0.2214 - acc: 0.897 - ETA: 0s - loss: 0.2210 - acc: 0.898 - ETA: 0s - loss: 0.2218 - acc: 0.897 - ETA: 0s - loss: 0.2220 - acc: 0.897 - ETA: 0s - loss: 0.2219 - acc: 0.897 - ETA: 0s - loss: 0.2217 - acc: 0.897 - 1s 62us/step - loss: 0.2219 - acc: 0.8973 - val_loss: 0.5656 - val_acc: 0.8313\n",
      "Epoch 123/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1901 - acc: 0.937 - ETA: 0s - loss: 0.1905 - acc: 0.907 - ETA: 0s - loss: 0.2105 - acc: 0.900 - ETA: 0s - loss: 0.2162 - acc: 0.897 - ETA: 0s - loss: 0.2165 - acc: 0.897 - ETA: 0s - loss: 0.2163 - acc: 0.900 - ETA: 0s - loss: 0.2159 - acc: 0.901 - ETA: 0s - loss: 0.2171 - acc: 0.899 - ETA: 0s - loss: 0.2188 - acc: 0.898 - ETA: 0s - loss: 0.2211 - acc: 0.897 - ETA: 0s - loss: 0.2188 - acc: 0.898 - ETA: 0s - loss: 0.2170 - acc: 0.900 - ETA: 0s - loss: 0.2179 - acc: 0.900 - ETA: 0s - loss: 0.2194 - acc: 0.900 - ETA: 0s - loss: 0.2188 - acc: 0.900 - ETA: 0s - loss: 0.2173 - acc: 0.901 - ETA: 0s - loss: 0.2176 - acc: 0.901 - ETA: 0s - loss: 0.2191 - acc: 0.900 - ETA: 0s - loss: 0.2182 - acc: 0.900 - ETA: 0s - loss: 0.2191 - acc: 0.900 - ETA: 0s - loss: 0.2190 - acc: 0.900 - 1s 60us/step - loss: 0.2201 - acc: 0.9002 - val_loss: 0.6120 - val_acc: 0.8281\n",
      "Epoch 124/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20513/20513 [==============================] - ETA: 1s - loss: 0.2577 - acc: 0.875 - ETA: 1s - loss: 0.1899 - acc: 0.915 - ETA: 0s - loss: 0.2011 - acc: 0.911 - ETA: 0s - loss: 0.2026 - acc: 0.910 - ETA: 0s - loss: 0.2142 - acc: 0.904 - ETA: 0s - loss: 0.2105 - acc: 0.904 - ETA: 0s - loss: 0.2132 - acc: 0.903 - ETA: 0s - loss: 0.2149 - acc: 0.904 - ETA: 0s - loss: 0.2166 - acc: 0.904 - ETA: 0s - loss: 0.2160 - acc: 0.902 - ETA: 0s - loss: 0.2149 - acc: 0.904 - ETA: 0s - loss: 0.2148 - acc: 0.903 - ETA: 0s - loss: 0.2164 - acc: 0.902 - ETA: 0s - loss: 0.2189 - acc: 0.900 - ETA: 0s - loss: 0.2198 - acc: 0.900 - ETA: 0s - loss: 0.2207 - acc: 0.899 - ETA: 0s - loss: 0.2212 - acc: 0.898 - ETA: 0s - loss: 0.2210 - acc: 0.898 - ETA: 0s - loss: 0.2203 - acc: 0.899 - 1s 51us/step - loss: 0.2210 - acc: 0.8987 - val_loss: 0.5904 - val_acc: 0.8359\n",
      "Epoch 125/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1044 - acc: 0.984 - ETA: 0s - loss: 0.1767 - acc: 0.920 - ETA: 0s - loss: 0.2005 - acc: 0.909 - ETA: 0s - loss: 0.2058 - acc: 0.906 - ETA: 0s - loss: 0.2033 - acc: 0.906 - ETA: 0s - loss: 0.2029 - acc: 0.907 - ETA: 0s - loss: 0.2099 - acc: 0.904 - ETA: 0s - loss: 0.2148 - acc: 0.900 - ETA: 0s - loss: 0.2144 - acc: 0.900 - ETA: 0s - loss: 0.2114 - acc: 0.901 - ETA: 0s - loss: 0.2151 - acc: 0.899 - ETA: 0s - loss: 0.2134 - acc: 0.900 - ETA: 0s - loss: 0.2171 - acc: 0.899 - ETA: 0s - loss: 0.2172 - acc: 0.898 - ETA: 0s - loss: 0.2161 - acc: 0.899 - ETA: 0s - loss: 0.2180 - acc: 0.898 - ETA: 0s - loss: 0.2183 - acc: 0.899 - ETA: 0s - loss: 0.2196 - acc: 0.898 - ETA: 0s - loss: 0.2198 - acc: 0.898 - ETA: 0s - loss: 0.2199 - acc: 0.898 - 1s 55us/step - loss: 0.2191 - acc: 0.8989 - val_loss: 0.6077 - val_acc: 0.8379\n",
      "Epoch 126/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.1966 - acc: 0.890 - ETA: 0s - loss: 0.2176 - acc: 0.895 - ETA: 0s - loss: 0.2183 - acc: 0.895 - ETA: 0s - loss: 0.2144 - acc: 0.897 - ETA: 0s - loss: 0.2108 - acc: 0.899 - ETA: 0s - loss: 0.2133 - acc: 0.899 - ETA: 0s - loss: 0.2099 - acc: 0.900 - ETA: 0s - loss: 0.2100 - acc: 0.900 - ETA: 0s - loss: 0.2128 - acc: 0.898 - ETA: 0s - loss: 0.2152 - acc: 0.898 - ETA: 0s - loss: 0.2154 - acc: 0.899 - ETA: 0s - loss: 0.2154 - acc: 0.899 - ETA: 0s - loss: 0.2161 - acc: 0.899 - ETA: 0s - loss: 0.2186 - acc: 0.899 - ETA: 0s - loss: 0.2184 - acc: 0.898 - ETA: 0s - loss: 0.2198 - acc: 0.897 - ETA: 0s - loss: 0.2207 - acc: 0.896 - ETA: 0s - loss: 0.2210 - acc: 0.897 - ETA: 0s - loss: 0.2201 - acc: 0.898 - 1s 52us/step - loss: 0.2197 - acc: 0.8987 - val_loss: 0.6543 - val_acc: 0.8305\n",
      "Epoch 127/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1033 - acc: 0.984 - ETA: 0s - loss: 0.1835 - acc: 0.922 - ETA: 0s - loss: 0.1997 - acc: 0.913 - ETA: 0s - loss: 0.2035 - acc: 0.910 - ETA: 0s - loss: 0.2169 - acc: 0.903 - ETA: 0s - loss: 0.2176 - acc: 0.901 - ETA: 0s - loss: 0.2158 - acc: 0.903 - ETA: 0s - loss: 0.2162 - acc: 0.902 - ETA: 0s - loss: 0.2161 - acc: 0.901 - ETA: 0s - loss: 0.2143 - acc: 0.902 - ETA: 0s - loss: 0.2144 - acc: 0.902 - ETA: 0s - loss: 0.2153 - acc: 0.901 - ETA: 0s - loss: 0.2127 - acc: 0.902 - ETA: 0s - loss: 0.2128 - acc: 0.902 - ETA: 0s - loss: 0.2152 - acc: 0.901 - ETA: 0s - loss: 0.2164 - acc: 0.900 - ETA: 0s - loss: 0.2160 - acc: 0.900 - ETA: 0s - loss: 0.2177 - acc: 0.899 - ETA: 0s - loss: 0.2182 - acc: 0.899 - 1s 52us/step - loss: 0.2182 - acc: 0.8994 - val_loss: 0.5942 - val_acc: 0.8359\n",
      "Epoch 128/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.1986 - acc: 0.906 - ETA: 0s - loss: 0.1838 - acc: 0.924 - ETA: 0s - loss: 0.2167 - acc: 0.903 - ETA: 0s - loss: 0.2195 - acc: 0.900 - ETA: 0s - loss: 0.2179 - acc: 0.898 - ETA: 0s - loss: 0.2157 - acc: 0.899 - ETA: 0s - loss: 0.2136 - acc: 0.901 - ETA: 0s - loss: 0.2169 - acc: 0.899 - ETA: 0s - loss: 0.2158 - acc: 0.899 - ETA: 0s - loss: 0.2162 - acc: 0.898 - ETA: 0s - loss: 0.2149 - acc: 0.900 - ETA: 0s - loss: 0.2161 - acc: 0.900 - ETA: 0s - loss: 0.2176 - acc: 0.900 - ETA: 0s - loss: 0.2180 - acc: 0.900 - ETA: 0s - loss: 0.2206 - acc: 0.899 - ETA: 0s - loss: 0.2214 - acc: 0.899 - ETA: 0s - loss: 0.2234 - acc: 0.898 - ETA: 0s - loss: 0.2222 - acc: 0.898 - 1s 49us/step - loss: 0.2222 - acc: 0.8986 - val_loss: 0.5709 - val_acc: 0.8362\n",
      "Epoch 129/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.2547 - acc: 0.890 - ETA: 0s - loss: 0.2186 - acc: 0.912 - ETA: 0s - loss: 0.2274 - acc: 0.897 - ETA: 0s - loss: 0.2179 - acc: 0.904 - ETA: 0s - loss: 0.2164 - acc: 0.902 - ETA: 0s - loss: 0.2110 - acc: 0.906 - ETA: 0s - loss: 0.2153 - acc: 0.904 - ETA: 0s - loss: 0.2152 - acc: 0.904 - ETA: 0s - loss: 0.2163 - acc: 0.903 - ETA: 0s - loss: 0.2133 - acc: 0.903 - ETA: 0s - loss: 0.2148 - acc: 0.903 - ETA: 0s - loss: 0.2146 - acc: 0.903 - ETA: 0s - loss: 0.2171 - acc: 0.901 - ETA: 0s - loss: 0.2178 - acc: 0.901 - ETA: 0s - loss: 0.2180 - acc: 0.901 - ETA: 0s - loss: 0.2192 - acc: 0.900 - ETA: 0s - loss: 0.2194 - acc: 0.900 - ETA: 0s - loss: 0.2183 - acc: 0.900 - 1s 50us/step - loss: 0.2199 - acc: 0.9000 - val_loss: 0.5661 - val_acc: 0.8347\n",
      "Epoch 130/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.2956 - acc: 0.890 - ETA: 0s - loss: 0.2078 - acc: 0.906 - ETA: 0s - loss: 0.2159 - acc: 0.902 - ETA: 0s - loss: 0.2175 - acc: 0.900 - ETA: 0s - loss: 0.2133 - acc: 0.903 - ETA: 0s - loss: 0.2147 - acc: 0.903 - ETA: 0s - loss: 0.2167 - acc: 0.903 - ETA: 0s - loss: 0.2147 - acc: 0.902 - ETA: 0s - loss: 0.2167 - acc: 0.902 - ETA: 0s - loss: 0.2167 - acc: 0.901 - ETA: 0s - loss: 0.2179 - acc: 0.900 - ETA: 0s - loss: 0.2172 - acc: 0.901 - ETA: 0s - loss: 0.2151 - acc: 0.901 - ETA: 0s - loss: 0.2145 - acc: 0.901 - ETA: 0s - loss: 0.2152 - acc: 0.900 - ETA: 0s - loss: 0.2175 - acc: 0.900 - ETA: 0s - loss: 0.2187 - acc: 0.899 - ETA: 0s - loss: 0.2189 - acc: 0.899 - 1s 51us/step - loss: 0.2186 - acc: 0.8991 - val_loss: 0.5948 - val_acc: 0.8330\n",
      "Epoch 131/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.2402 - acc: 0.890 - ETA: 0s - loss: 0.2122 - acc: 0.904 - ETA: 0s - loss: 0.2178 - acc: 0.900 - ETA: 0s - loss: 0.2125 - acc: 0.900 - ETA: 0s - loss: 0.2126 - acc: 0.900 - ETA: 0s - loss: 0.2142 - acc: 0.898 - ETA: 0s - loss: 0.2150 - acc: 0.899 - ETA: 0s - loss: 0.2158 - acc: 0.900 - ETA: 0s - loss: 0.2163 - acc: 0.900 - ETA: 0s - loss: 0.2168 - acc: 0.899 - ETA: 0s - loss: 0.2157 - acc: 0.901 - ETA: 0s - loss: 0.2149 - acc: 0.901 - ETA: 0s - loss: 0.2167 - acc: 0.901 - ETA: 0s - loss: 0.2169 - acc: 0.901 - ETA: 0s - loss: 0.2163 - acc: 0.901 - ETA: 0s - loss: 0.2171 - acc: 0.900 - ETA: 0s - loss: 0.2185 - acc: 0.899 - ETA: 0s - loss: 0.2173 - acc: 0.900 - 1s 50us/step - loss: 0.2189 - acc: 0.9005 - val_loss: 0.5857 - val_acc: 0.8348\n",
      "Epoch 132/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1319 - acc: 0.937 - ETA: 0s - loss: 0.1991 - acc: 0.891 - ETA: 0s - loss: 0.2210 - acc: 0.891 - ETA: 0s - loss: 0.2226 - acc: 0.892 - ETA: 0s - loss: 0.2213 - acc: 0.894 - ETA: 0s - loss: 0.2153 - acc: 0.899 - ETA: 0s - loss: 0.2155 - acc: 0.899 - ETA: 0s - loss: 0.2181 - acc: 0.898 - ETA: 0s - loss: 0.2168 - acc: 0.899 - ETA: 0s - loss: 0.2171 - acc: 0.899 - ETA: 0s - loss: 0.2144 - acc: 0.901 - ETA: 0s - loss: 0.2147 - acc: 0.901 - ETA: 0s - loss: 0.2155 - acc: 0.901 - ETA: 0s - loss: 0.2159 - acc: 0.900 - ETA: 0s - loss: 0.2157 - acc: 0.900 - ETA: 0s - loss: 0.2165 - acc: 0.899 - ETA: 0s - loss: 0.2158 - acc: 0.900 - ETA: 0s - loss: 0.2165 - acc: 0.900 - ETA: 0s - loss: 0.2168 - acc: 0.900 - 1s 51us/step - loss: 0.2168 - acc: 0.9002 - val_loss: 0.6131 - val_acc: 0.8351\n",
      "Epoch 133/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.2155 - acc: 0.890 - ETA: 0s - loss: 0.2347 - acc: 0.895 - ETA: 0s - loss: 0.2278 - acc: 0.899 - ETA: 0s - loss: 0.2183 - acc: 0.903 - ETA: 0s - loss: 0.2160 - acc: 0.902 - ETA: 0s - loss: 0.2171 - acc: 0.902 - ETA: 0s - loss: 0.2173 - acc: 0.900 - ETA: 0s - loss: 0.2154 - acc: 0.901 - ETA: 0s - loss: 0.2191 - acc: 0.900 - ETA: 0s - loss: 0.2213 - acc: 0.898 - ETA: 0s - loss: 0.2210 - acc: 0.897 - ETA: 0s - loss: 0.2216 - acc: 0.897 - ETA: 0s - loss: 0.2189 - acc: 0.898 - ETA: 0s - loss: 0.2199 - acc: 0.899 - ETA: 0s - loss: 0.2187 - acc: 0.899 - ETA: 0s - loss: 0.2182 - acc: 0.899 - ETA: 0s - loss: 0.2186 - acc: 0.899 - ETA: 0s - loss: 0.2176 - acc: 0.900 - 1s 49us/step - loss: 0.2167 - acc: 0.9013 - val_loss: 0.6217 - val_acc: 0.8381\n",
      "Epoch 134/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.2012 - acc: 0.890 - ETA: 0s - loss: 0.2243 - acc: 0.903 - ETA: 0s - loss: 0.2266 - acc: 0.896 - ETA: 0s - loss: 0.2292 - acc: 0.897 - ETA: 0s - loss: 0.2208 - acc: 0.900 - ETA: 0s - loss: 0.2176 - acc: 0.903 - ETA: 0s - loss: 0.2178 - acc: 0.903 - ETA: 0s - loss: 0.2132 - acc: 0.906 - ETA: 0s - loss: 0.2105 - acc: 0.906 - ETA: 0s - loss: 0.2129 - acc: 0.906 - ETA: 0s - loss: 0.2136 - acc: 0.904 - ETA: 0s - loss: 0.2143 - acc: 0.904 - ETA: 0s - loss: 0.2183 - acc: 0.902 - ETA: 0s - loss: 0.2202 - acc: 0.901 - ETA: 0s - loss: 0.2196 - acc: 0.901 - ETA: 0s - loss: 0.2189 - acc: 0.901 - ETA: 0s - loss: 0.2184 - acc: 0.901 - ETA: 0s - loss: 0.2188 - acc: 0.901 - 1s 49us/step - loss: 0.2190 - acc: 0.9008 - val_loss: 0.6108 - val_acc: 0.8322\n",
      "Epoch 135/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.2228 - acc: 0.890 - ETA: 0s - loss: 0.1887 - acc: 0.911 - ETA: 0s - loss: 0.1912 - acc: 0.913 - ETA: 0s - loss: 0.2024 - acc: 0.908 - ETA: 0s - loss: 0.2057 - acc: 0.906 - ETA: 0s - loss: 0.2074 - acc: 0.907 - ETA: 0s - loss: 0.2100 - acc: 0.904 - ETA: 0s - loss: 0.2121 - acc: 0.901 - ETA: 0s - loss: 0.2124 - acc: 0.900 - ETA: 0s - loss: 0.2153 - acc: 0.898 - ETA: 0s - loss: 0.2163 - acc: 0.898 - ETA: 0s - loss: 0.2140 - acc: 0.899 - ETA: 0s - loss: 0.2126 - acc: 0.900 - ETA: 0s - loss: 0.2142 - acc: 0.899 - ETA: 0s - loss: 0.2151 - acc: 0.898 - ETA: 0s - loss: 0.2145 - acc: 0.899 - ETA: 0s - loss: 0.2133 - acc: 0.899 - ETA: 0s - loss: 0.2165 - acc: 0.898 - 1s 51us/step - loss: 0.2181 - acc: 0.8986 - val_loss: 0.5987 - val_acc: 0.8359\n",
      "Epoch 136/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1767 - acc: 0.937 - ETA: 0s - loss: 0.1766 - acc: 0.928 - ETA: 0s - loss: 0.2083 - acc: 0.916 - ETA: 0s - loss: 0.2087 - acc: 0.913 - ETA: 0s - loss: 0.2070 - acc: 0.912 - ETA: 0s - loss: 0.2086 - acc: 0.909 - ETA: 0s - loss: 0.2071 - acc: 0.909 - ETA: 0s - loss: 0.2076 - acc: 0.907 - ETA: 0s - loss: 0.2084 - acc: 0.904 - ETA: 0s - loss: 0.2113 - acc: 0.904 - ETA: 0s - loss: 0.2120 - acc: 0.904 - ETA: 0s - loss: 0.2144 - acc: 0.902 - ETA: 0s - loss: 0.2138 - acc: 0.902 - ETA: 0s - loss: 0.2142 - acc: 0.902 - ETA: 0s - loss: 0.2143 - acc: 0.902 - ETA: 0s - loss: 0.2155 - acc: 0.901 - ETA: 0s - loss: 0.2172 - acc: 0.900 - ETA: 0s - loss: 0.2173 - acc: 0.900 - 1s 50us/step - loss: 0.2181 - acc: 0.9004 - val_loss: 0.5699 - val_acc: 0.8393\n",
      "Epoch 137/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1617 - acc: 0.953 - ETA: 0s - loss: 0.1923 - acc: 0.909 - ETA: 0s - loss: 0.2156 - acc: 0.903 - ETA: 0s - loss: 0.2206 - acc: 0.904 - ETA: 0s - loss: 0.2193 - acc: 0.904 - ETA: 0s - loss: 0.2166 - acc: 0.904 - ETA: 0s - loss: 0.2157 - acc: 0.905 - ETA: 0s - loss: 0.2166 - acc: 0.903 - ETA: 0s - loss: 0.2184 - acc: 0.901 - ETA: 0s - loss: 0.2194 - acc: 0.899 - ETA: 0s - loss: 0.2181 - acc: 0.900 - ETA: 0s - loss: 0.2175 - acc: 0.900 - ETA: 0s - loss: 0.2162 - acc: 0.901 - ETA: 0s - loss: 0.2146 - acc: 0.901 - ETA: 0s - loss: 0.2147 - acc: 0.900 - ETA: 0s - loss: 0.2156 - acc: 0.899 - ETA: 0s - loss: 0.2159 - acc: 0.900 - ETA: 0s - loss: 0.2164 - acc: 0.899 - 1s 51us/step - loss: 0.2173 - acc: 0.8991 - val_loss: 0.5859 - val_acc: 0.8368\n",
      "Epoch 138/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.1988 - acc: 0.890 - ETA: 0s - loss: 0.2037 - acc: 0.899 - ETA: 0s - loss: 0.2096 - acc: 0.902 - ETA: 0s - loss: 0.2119 - acc: 0.900 - ETA: 0s - loss: 0.2069 - acc: 0.900 - ETA: 0s - loss: 0.2127 - acc: 0.896 - ETA: 0s - loss: 0.2112 - acc: 0.897 - ETA: 0s - loss: 0.2117 - acc: 0.898 - ETA: 0s - loss: 0.2125 - acc: 0.898 - ETA: 0s - loss: 0.2133 - acc: 0.899 - ETA: 0s - loss: 0.2136 - acc: 0.899 - ETA: 0s - loss: 0.2129 - acc: 0.899 - ETA: 0s - loss: 0.2146 - acc: 0.898 - ETA: 0s - loss: 0.2153 - acc: 0.898 - ETA: 0s - loss: 0.2154 - acc: 0.899 - ETA: 0s - loss: 0.2164 - acc: 0.898 - ETA: 0s - loss: 0.2162 - acc: 0.899 - ETA: 0s - loss: 0.2160 - acc: 0.900 - 1s 51us/step - loss: 0.2170 - acc: 0.9002 - val_loss: 0.5981 - val_acc: 0.8312\n",
      "Epoch 139/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.2005 - acc: 0.921 - ETA: 0s - loss: 0.2212 - acc: 0.899 - ETA: 0s - loss: 0.2101 - acc: 0.905 - ETA: 0s - loss: 0.2072 - acc: 0.903 - ETA: 0s - loss: 0.2056 - acc: 0.903 - ETA: 0s - loss: 0.2097 - acc: 0.903 - ETA: 0s - loss: 0.2124 - acc: 0.905 - ETA: 0s - loss: 0.2121 - acc: 0.905 - ETA: 0s - loss: 0.2117 - acc: 0.904 - ETA: 0s - loss: 0.2103 - acc: 0.904 - ETA: 0s - loss: 0.2122 - acc: 0.904 - ETA: 0s - loss: 0.2124 - acc: 0.903 - ETA: 0s - loss: 0.2120 - acc: 0.904 - ETA: 0s - loss: 0.2125 - acc: 0.904 - ETA: 0s - loss: 0.2127 - acc: 0.903 - ETA: 0s - loss: 0.2155 - acc: 0.902 - ETA: 0s - loss: 0.2163 - acc: 0.901 - ETA: 0s - loss: 0.2171 - acc: 0.901 - 1s 50us/step - loss: 0.2174 - acc: 0.9014 - val_loss: 0.5976 - val_acc: 0.8254\n",
      "Epoch 140/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.2835 - acc: 0.875 - ETA: 0s - loss: 0.2059 - acc: 0.908 - ETA: 0s - loss: 0.2072 - acc: 0.903 - ETA: 0s - loss: 0.2162 - acc: 0.901 - ETA: 0s - loss: 0.2184 - acc: 0.901 - ETA: 0s - loss: 0.2168 - acc: 0.901 - ETA: 0s - loss: 0.2168 - acc: 0.900 - ETA: 0s - loss: 0.2207 - acc: 0.898 - ETA: 0s - loss: 0.2185 - acc: 0.900 - ETA: 0s - loss: 0.2172 - acc: 0.901 - ETA: 0s - loss: 0.2159 - acc: 0.900 - ETA: 0s - loss: 0.2151 - acc: 0.900 - ETA: 0s - loss: 0.2143 - acc: 0.901 - ETA: 0s - loss: 0.2133 - acc: 0.902 - ETA: 0s - loss: 0.2146 - acc: 0.901 - ETA: 0s - loss: 0.2141 - acc: 0.902 - ETA: 0s - loss: 0.2165 - acc: 0.901 - ETA: 0s - loss: 0.2162 - acc: 0.901 - 1s 50us/step - loss: 0.2166 - acc: 0.9011 - val_loss: 0.5832 - val_acc: 0.8375\n",
      "Epoch 141/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.2706 - acc: 0.890 - ETA: 0s - loss: 0.2064 - acc: 0.901 - ETA: 0s - loss: 0.2031 - acc: 0.905 - ETA: 0s - loss: 0.2132 - acc: 0.898 - ETA: 0s - loss: 0.2081 - acc: 0.902 - ETA: 0s - loss: 0.2054 - acc: 0.905 - ETA: 0s - loss: 0.2101 - acc: 0.902 - ETA: 0s - loss: 0.2112 - acc: 0.902 - ETA: 0s - loss: 0.2126 - acc: 0.900 - ETA: 0s - loss: 0.2112 - acc: 0.902 - ETA: 0s - loss: 0.2135 - acc: 0.899 - ETA: 0s - loss: 0.2153 - acc: 0.898 - ETA: 0s - loss: 0.2154 - acc: 0.898 - ETA: 0s - loss: 0.2155 - acc: 0.899 - ETA: 0s - loss: 0.2154 - acc: 0.899 - ETA: 0s - loss: 0.2147 - acc: 0.899 - ETA: 0s - loss: 0.2140 - acc: 0.900 - ETA: 0s - loss: 0.2172 - acc: 0.899 - ETA: 0s - loss: 0.2173 - acc: 0.899 - 1s 52us/step - loss: 0.2171 - acc: 0.8996 - val_loss: 0.6615 - val_acc: 0.8250\n",
      "Epoch 142/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1654 - acc: 0.921 - ETA: 0s - loss: 0.2045 - acc: 0.907 - ETA: 0s - loss: 0.1965 - acc: 0.910 - ETA: 0s - loss: 0.2069 - acc: 0.905 - ETA: 0s - loss: 0.2097 - acc: 0.903 - ETA: 0s - loss: 0.2139 - acc: 0.900 - ETA: 0s - loss: 0.2177 - acc: 0.898 - ETA: 0s - loss: 0.2184 - acc: 0.898 - ETA: 0s - loss: 0.2172 - acc: 0.900 - ETA: 0s - loss: 0.2163 - acc: 0.901 - ETA: 0s - loss: 0.2165 - acc: 0.901 - ETA: 0s - loss: 0.2153 - acc: 0.901 - ETA: 0s - loss: 0.2142 - acc: 0.902 - ETA: 0s - loss: 0.2168 - acc: 0.901 - ETA: 0s - loss: 0.2176 - acc: 0.901 - ETA: 0s - loss: 0.2183 - acc: 0.901 - ETA: 0s - loss: 0.2185 - acc: 0.901 - ETA: 0s - loss: 0.2177 - acc: 0.901 - ETA: 0s - loss: 0.2177 - acc: 0.901 - ETA: 0s - loss: 0.2185 - acc: 0.901 - 1s 56us/step - loss: 0.2188 - acc: 0.9002 - val_loss: 0.6152 - val_acc: 0.8372\n",
      "Epoch 143/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.2654 - acc: 0.859 - ETA: 0s - loss: 0.2287 - acc: 0.898 - ETA: 0s - loss: 0.2144 - acc: 0.901 - ETA: 0s - loss: 0.2195 - acc: 0.899 - ETA: 0s - loss: 0.2224 - acc: 0.899 - ETA: 0s - loss: 0.2226 - acc: 0.899 - ETA: 0s - loss: 0.2187 - acc: 0.901 - ETA: 0s - loss: 0.2174 - acc: 0.901 - ETA: 0s - loss: 0.2183 - acc: 0.902 - ETA: 0s - loss: 0.2159 - acc: 0.903 - ETA: 0s - loss: 0.2157 - acc: 0.903 - ETA: 0s - loss: 0.2148 - acc: 0.903 - ETA: 0s - loss: 0.2146 - acc: 0.904 - ETA: 0s - loss: 0.2150 - acc: 0.902 - ETA: 0s - loss: 0.2147 - acc: 0.903 - ETA: 0s - loss: 0.2146 - acc: 0.903 - ETA: 0s - loss: 0.2138 - acc: 0.903 - ETA: 0s - loss: 0.2160 - acc: 0.902 - ETA: 0s - loss: 0.2151 - acc: 0.902 - 1s 53us/step - loss: 0.2154 - acc: 0.9021 - val_loss: 0.6050 - val_acc: 0.8310\n",
      "Epoch 144/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.1319 - acc: 0.968 - ETA: 0s - loss: 0.2155 - acc: 0.896 - ETA: 0s - loss: 0.2151 - acc: 0.891 - ETA: 0s - loss: 0.2189 - acc: 0.893 - ETA: 0s - loss: 0.2167 - acc: 0.893 - ETA: 0s - loss: 0.2092 - acc: 0.899 - ETA: 0s - loss: 0.2155 - acc: 0.897 - ETA: 0s - loss: 0.2164 - acc: 0.896 - ETA: 0s - loss: 0.2146 - acc: 0.899 - ETA: 0s - loss: 0.2142 - acc: 0.899 - ETA: 0s - loss: 0.2173 - acc: 0.899 - ETA: 0s - loss: 0.2174 - acc: 0.899 - ETA: 0s - loss: 0.2179 - acc: 0.899 - ETA: 0s - loss: 0.2182 - acc: 0.899 - ETA: 0s - loss: 0.2160 - acc: 0.900 - ETA: 0s - loss: 0.2150 - acc: 0.901 - ETA: 0s - loss: 0.2140 - acc: 0.901 - ETA: 0s - loss: 0.2149 - acc: 0.901 - ETA: 0s - loss: 0.2162 - acc: 0.901 - 1s 52us/step - loss: 0.2163 - acc: 0.9011 - val_loss: 0.6004 - val_acc: 0.8325\n",
      "Epoch 145/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.1588 - acc: 0.906 - ETA: 0s - loss: 0.2126 - acc: 0.903 - ETA: 0s - loss: 0.2172 - acc: 0.901 - ETA: 0s - loss: 0.2124 - acc: 0.903 - ETA: 0s - loss: 0.2140 - acc: 0.903 - ETA: 0s - loss: 0.2152 - acc: 0.902 - ETA: 0s - loss: 0.2150 - acc: 0.903 - ETA: 0s - loss: 0.2125 - acc: 0.904 - ETA: 0s - loss: 0.2102 - acc: 0.904 - ETA: 0s - loss: 0.2114 - acc: 0.904 - ETA: 0s - loss: 0.2118 - acc: 0.904 - ETA: 0s - loss: 0.2111 - acc: 0.905 - ETA: 0s - loss: 0.2116 - acc: 0.904 - ETA: 0s - loss: 0.2129 - acc: 0.904 - ETA: 0s - loss: 0.2135 - acc: 0.903 - ETA: 0s - loss: 0.2145 - acc: 0.902 - ETA: 0s - loss: 0.2158 - acc: 0.902 - ETA: 0s - loss: 0.2165 - acc: 0.901 - 1s 50us/step - loss: 0.2164 - acc: 0.9013 - val_loss: 0.5979 - val_acc: 0.8362\n",
      "Epoch 146/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.1367 - acc: 0.953 - ETA: 0s - loss: 0.2166 - acc: 0.904 - ETA: 0s - loss: 0.2334 - acc: 0.900 - ETA: 0s - loss: 0.2222 - acc: 0.903 - ETA: 0s - loss: 0.2175 - acc: 0.903 - ETA: 0s - loss: 0.2124 - acc: 0.905 - ETA: 0s - loss: 0.2125 - acc: 0.904 - ETA: 0s - loss: 0.2137 - acc: 0.902 - ETA: 0s - loss: 0.2126 - acc: 0.901 - ETA: 0s - loss: 0.2129 - acc: 0.903 - ETA: 0s - loss: 0.2118 - acc: 0.903 - ETA: 0s - loss: 0.2115 - acc: 0.902 - ETA: 0s - loss: 0.2135 - acc: 0.901 - ETA: 0s - loss: 0.2140 - acc: 0.901 - ETA: 0s - loss: 0.2137 - acc: 0.901 - ETA: 0s - loss: 0.2148 - acc: 0.900 - ETA: 0s - loss: 0.2145 - acc: 0.900 - ETA: 0s - loss: 0.2153 - acc: 0.900 - 1s 51us/step - loss: 0.2149 - acc: 0.9008 - val_loss: 0.5839 - val_acc: 0.8360\n",
      "Epoch 147/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1527 - acc: 0.953 - ETA: 0s - loss: 0.2042 - acc: 0.912 - ETA: 0s - loss: 0.2073 - acc: 0.908 - ETA: 0s - loss: 0.2078 - acc: 0.908 - ETA: 0s - loss: 0.2129 - acc: 0.904 - ETA: 0s - loss: 0.2115 - acc: 0.905 - ETA: 0s - loss: 0.2123 - acc: 0.904 - ETA: 0s - loss: 0.2110 - acc: 0.906 - ETA: 0s - loss: 0.2107 - acc: 0.906 - ETA: 0s - loss: 0.2129 - acc: 0.905 - ETA: 0s - loss: 0.2135 - acc: 0.904 - ETA: 0s - loss: 0.2129 - acc: 0.905 - ETA: 0s - loss: 0.2144 - acc: 0.903 - ETA: 0s - loss: 0.2153 - acc: 0.903 - ETA: 0s - loss: 0.2151 - acc: 0.903 - ETA: 0s - loss: 0.2157 - acc: 0.902 - ETA: 0s - loss: 0.2156 - acc: 0.902 - ETA: 0s - loss: 0.2167 - acc: 0.902 - 1s 50us/step - loss: 0.2166 - acc: 0.9021 - val_loss: 0.5875 - val_acc: 0.8303\n",
      "Epoch 148/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.1891 - acc: 0.875 - ETA: 0s - loss: 0.2155 - acc: 0.896 - ETA: 0s - loss: 0.2061 - acc: 0.904 - ETA: 0s - loss: 0.2013 - acc: 0.905 - ETA: 0s - loss: 0.2061 - acc: 0.902 - ETA: 0s - loss: 0.2081 - acc: 0.901 - ETA: 0s - loss: 0.2048 - acc: 0.904 - ETA: 0s - loss: 0.2074 - acc: 0.903 - ETA: 0s - loss: 0.2105 - acc: 0.902 - ETA: 0s - loss: 0.2112 - acc: 0.901 - ETA: 0s - loss: 0.2104 - acc: 0.902 - ETA: 0s - loss: 0.2121 - acc: 0.900 - ETA: 0s - loss: 0.2109 - acc: 0.901 - ETA: 0s - loss: 0.2105 - acc: 0.902 - ETA: 0s - loss: 0.2111 - acc: 0.902 - ETA: 0s - loss: 0.2130 - acc: 0.902 - ETA: 0s - loss: 0.2130 - acc: 0.902 - ETA: 0s - loss: 0.2147 - acc: 0.901 - 1s 51us/step - loss: 0.2153 - acc: 0.9015 - val_loss: 0.6025 - val_acc: 0.8319\n",
      "Epoch 149/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.1877 - acc: 0.906 - ETA: 0s - loss: 0.1925 - acc: 0.903 - ETA: 0s - loss: 0.1916 - acc: 0.915 - ETA: 0s - loss: 0.1988 - acc: 0.911 - ETA: 0s - loss: 0.2167 - acc: 0.902 - ETA: 0s - loss: 0.2141 - acc: 0.903 - ETA: 0s - loss: 0.2151 - acc: 0.903 - ETA: 0s - loss: 0.2186 - acc: 0.902 - ETA: 0s - loss: 0.2191 - acc: 0.902 - ETA: 0s - loss: 0.2204 - acc: 0.901 - ETA: 0s - loss: 0.2199 - acc: 0.901 - ETA: 0s - loss: 0.2199 - acc: 0.901 - ETA: 0s - loss: 0.2203 - acc: 0.900 - ETA: 0s - loss: 0.2171 - acc: 0.902 - ETA: 0s - loss: 0.2151 - acc: 0.903 - ETA: 0s - loss: 0.2155 - acc: 0.902 - ETA: 0s - loss: 0.2149 - acc: 0.902 - ETA: 0s - loss: 0.2153 - acc: 0.902 - 1s 51us/step - loss: 0.2146 - acc: 0.9029 - val_loss: 0.6294 - val_acc: 0.8362\n",
      "Epoch 150/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1736 - acc: 0.906 - ETA: 0s - loss: 0.2093 - acc: 0.903 - ETA: 0s - loss: 0.1988 - acc: 0.909 - ETA: 0s - loss: 0.2286 - acc: 0.900 - ETA: 0s - loss: 0.2266 - acc: 0.900 - ETA: 0s - loss: 0.2178 - acc: 0.903 - ETA: 0s - loss: 0.2210 - acc: 0.902 - ETA: 0s - loss: 0.2197 - acc: 0.903 - ETA: 0s - loss: 0.2207 - acc: 0.902 - ETA: 0s - loss: 0.2207 - acc: 0.902 - ETA: 0s - loss: 0.2188 - acc: 0.902 - ETA: 0s - loss: 0.2202 - acc: 0.902 - ETA: 0s - loss: 0.2179 - acc: 0.902 - ETA: 0s - loss: 0.2176 - acc: 0.901 - ETA: 0s - loss: 0.2179 - acc: 0.901 - ETA: 0s - loss: 0.2176 - acc: 0.901 - ETA: 0s - loss: 0.2177 - acc: 0.900 - ETA: 0s - loss: 0.2162 - acc: 0.900 - 1s 51us/step - loss: 0.2169 - acc: 0.9008 - val_loss: 0.6143 - val_acc: 0.8343\n",
      "Epoch 151/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.1387 - acc: 0.968 - ETA: 0s - loss: 0.2128 - acc: 0.903 - ETA: 0s - loss: 0.2166 - acc: 0.898 - ETA: 0s - loss: 0.2147 - acc: 0.902 - ETA: 0s - loss: 0.2177 - acc: 0.902 - ETA: 0s - loss: 0.2127 - acc: 0.906 - ETA: 0s - loss: 0.2104 - acc: 0.906 - ETA: 0s - loss: 0.2124 - acc: 0.904 - ETA: 0s - loss: 0.2151 - acc: 0.902 - ETA: 0s - loss: 0.2137 - acc: 0.904 - ETA: 0s - loss: 0.2118 - acc: 0.904 - ETA: 0s - loss: 0.2119 - acc: 0.904 - ETA: 0s - loss: 0.2136 - acc: 0.903 - ETA: 0s - loss: 0.2129 - acc: 0.903 - ETA: 0s - loss: 0.2145 - acc: 0.901 - ETA: 0s - loss: 0.2156 - acc: 0.901 - ETA: 0s - loss: 0.2143 - acc: 0.902 - ETA: 0s - loss: 0.2138 - acc: 0.902 - ETA: 0s - loss: 0.2139 - acc: 0.902 - 1s 54us/step - loss: 0.2156 - acc: 0.9023 - val_loss: 0.6709 - val_acc: 0.8212\n",
      "Epoch 152/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.2176 - acc: 0.906 - ETA: 0s - loss: 0.2144 - acc: 0.898 - ETA: 0s - loss: 0.2170 - acc: 0.900 - ETA: 0s - loss: 0.2050 - acc: 0.906 - ETA: 0s - loss: 0.2086 - acc: 0.905 - ETA: 0s - loss: 0.2084 - acc: 0.907 - ETA: 0s - loss: 0.2093 - acc: 0.907 - ETA: 0s - loss: 0.2092 - acc: 0.907 - ETA: 0s - loss: 0.2076 - acc: 0.906 - ETA: 0s - loss: 0.2095 - acc: 0.906 - ETA: 0s - loss: 0.2095 - acc: 0.906 - ETA: 0s - loss: 0.2117 - acc: 0.904 - ETA: 0s - loss: 0.2128 - acc: 0.903 - ETA: 0s - loss: 0.2137 - acc: 0.902 - ETA: 0s - loss: 0.2123 - acc: 0.902 - ETA: 0s - loss: 0.2151 - acc: 0.900 - ETA: 0s - loss: 0.2152 - acc: 0.900 - ETA: 0s - loss: 0.2147 - acc: 0.900 - 1s 50us/step - loss: 0.2146 - acc: 0.9011 - val_loss: 0.6361 - val_acc: 0.8262\n",
      "Epoch 153/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.2759 - acc: 0.875 - ETA: 0s - loss: 0.1975 - acc: 0.900 - ETA: 0s - loss: 0.2061 - acc: 0.900 - ETA: 0s - loss: 0.1967 - acc: 0.904 - ETA: 0s - loss: 0.2040 - acc: 0.903 - ETA: 0s - loss: 0.2053 - acc: 0.902 - ETA: 0s - loss: 0.2054 - acc: 0.903 - ETA: 0s - loss: 0.2076 - acc: 0.902 - ETA: 0s - loss: 0.2090 - acc: 0.900 - ETA: 0s - loss: 0.2102 - acc: 0.900 - ETA: 0s - loss: 0.2110 - acc: 0.901 - ETA: 0s - loss: 0.2112 - acc: 0.901 - ETA: 0s - loss: 0.2105 - acc: 0.902 - ETA: 0s - loss: 0.2094 - acc: 0.903 - ETA: 0s - loss: 0.2089 - acc: 0.903 - ETA: 0s - loss: 0.2111 - acc: 0.903 - ETA: 0s - loss: 0.2116 - acc: 0.904 - ETA: 0s - loss: 0.2126 - acc: 0.903 - ETA: 0s - loss: 0.2122 - acc: 0.903 - 1s 52us/step - loss: 0.2131 - acc: 0.9033 - val_loss: 0.6019 - val_acc: 0.8325\n",
      "Epoch 154/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1044 - acc: 0.968 - ETA: 0s - loss: 0.1985 - acc: 0.906 - ETA: 0s - loss: 0.2026 - acc: 0.904 - ETA: 0s - loss: 0.2091 - acc: 0.900 - ETA: 0s - loss: 0.2057 - acc: 0.901 - ETA: 0s - loss: 0.2088 - acc: 0.903 - ETA: 0s - loss: 0.2034 - acc: 0.906 - ETA: 0s - loss: 0.2074 - acc: 0.905 - ETA: 0s - loss: 0.2110 - acc: 0.904 - ETA: 0s - loss: 0.2108 - acc: 0.903 - ETA: 0s - loss: 0.2128 - acc: 0.904 - ETA: 0s - loss: 0.2125 - acc: 0.903 - ETA: 0s - loss: 0.2141 - acc: 0.901 - ETA: 0s - loss: 0.2143 - acc: 0.901 - ETA: 0s - loss: 0.2144 - acc: 0.901 - ETA: 0s - loss: 0.2137 - acc: 0.901 - ETA: 0s - loss: 0.2131 - acc: 0.902 - ETA: 0s - loss: 0.2143 - acc: 0.901 - 1s 50us/step - loss: 0.2150 - acc: 0.9014 - val_loss: 0.5993 - val_acc: 0.8376\n",
      "Epoch 155/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.2084 - acc: 0.906 - ETA: 0s - loss: 0.2071 - acc: 0.893 - ETA: 0s - loss: 0.2086 - acc: 0.900 - ETA: 0s - loss: 0.2040 - acc: 0.900 - ETA: 0s - loss: 0.2077 - acc: 0.899 - ETA: 0s - loss: 0.2073 - acc: 0.900 - ETA: 0s - loss: 0.2073 - acc: 0.902 - ETA: 0s - loss: 0.2077 - acc: 0.903 - ETA: 0s - loss: 0.2077 - acc: 0.903 - ETA: 0s - loss: 0.2076 - acc: 0.903 - ETA: 0s - loss: 0.2074 - acc: 0.903 - ETA: 0s - loss: 0.2074 - acc: 0.903 - ETA: 0s - loss: 0.2059 - acc: 0.904 - ETA: 0s - loss: 0.2085 - acc: 0.903 - ETA: 0s - loss: 0.2119 - acc: 0.901 - ETA: 0s - loss: 0.2119 - acc: 0.901 - ETA: 0s - loss: 0.2121 - acc: 0.901 - ETA: 0s - loss: 0.2122 - acc: 0.901 - 1s 51us/step - loss: 0.2131 - acc: 0.9012 - val_loss: 0.6260 - val_acc: 0.8370\n",
      "Epoch 156/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.1845 - acc: 0.875 - ETA: 0s - loss: 0.2093 - acc: 0.900 - ETA: 0s - loss: 0.2091 - acc: 0.906 - ETA: 0s - loss: 0.2097 - acc: 0.905 - ETA: 0s - loss: 0.2078 - acc: 0.906 - ETA: 0s - loss: 0.2144 - acc: 0.904 - ETA: 0s - loss: 0.2114 - acc: 0.904 - ETA: 0s - loss: 0.2147 - acc: 0.902 - ETA: 0s - loss: 0.2155 - acc: 0.902 - ETA: 0s - loss: 0.2152 - acc: 0.902 - ETA: 0s - loss: 0.2186 - acc: 0.901 - ETA: 0s - loss: 0.2157 - acc: 0.903 - ETA: 0s - loss: 0.2162 - acc: 0.903 - ETA: 0s - loss: 0.2175 - acc: 0.903 - ETA: 0s - loss: 0.2164 - acc: 0.903 - ETA: 0s - loss: 0.2150 - acc: 0.903 - ETA: 0s - loss: 0.2145 - acc: 0.903 - ETA: 0s - loss: 0.2138 - acc: 0.903 - 1s 50us/step - loss: 0.2135 - acc: 0.9034 - val_loss: 0.6483 - val_acc: 0.8341\n",
      "Epoch 157/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1295 - acc: 0.937 - ETA: 0s - loss: 0.2150 - acc: 0.892 - ETA: 0s - loss: 0.2227 - acc: 0.887 - ETA: 0s - loss: 0.2144 - acc: 0.897 - ETA: 0s - loss: 0.2150 - acc: 0.901 - ETA: 0s - loss: 0.2114 - acc: 0.902 - ETA: 0s - loss: 0.2111 - acc: 0.902 - ETA: 0s - loss: 0.2106 - acc: 0.903 - ETA: 0s - loss: 0.2105 - acc: 0.903 - ETA: 0s - loss: 0.2111 - acc: 0.902 - ETA: 0s - loss: 0.2129 - acc: 0.901 - ETA: 0s - loss: 0.2114 - acc: 0.902 - ETA: 0s - loss: 0.2096 - acc: 0.903 - ETA: 0s - loss: 0.2100 - acc: 0.903 - ETA: 0s - loss: 0.2092 - acc: 0.903 - ETA: 0s - loss: 0.2128 - acc: 0.902 - ETA: 0s - loss: 0.2124 - acc: 0.903 - ETA: 0s - loss: 0.2128 - acc: 0.902 - 1s 50us/step - loss: 0.2145 - acc: 0.9029 - val_loss: 0.6083 - val_acc: 0.8384\n",
      "Epoch 158/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.2687 - acc: 0.875 - ETA: 0s - loss: 0.2245 - acc: 0.902 - ETA: 0s - loss: 0.2253 - acc: 0.892 - ETA: 0s - loss: 0.2159 - acc: 0.895 - ETA: 0s - loss: 0.2116 - acc: 0.898 - ETA: 0s - loss: 0.2206 - acc: 0.897 - ETA: 0s - loss: 0.2172 - acc: 0.898 - ETA: 0s - loss: 0.2153 - acc: 0.899 - ETA: 0s - loss: 0.2115 - acc: 0.902 - ETA: 0s - loss: 0.2098 - acc: 0.904 - ETA: 0s - loss: 0.2112 - acc: 0.902 - ETA: 0s - loss: 0.2108 - acc: 0.903 - ETA: 0s - loss: 0.2115 - acc: 0.902 - ETA: 0s - loss: 0.2108 - acc: 0.902 - ETA: 0s - loss: 0.2107 - acc: 0.902 - ETA: 0s - loss: 0.2118 - acc: 0.903 - ETA: 0s - loss: 0.2134 - acc: 0.902 - ETA: 0s - loss: 0.2138 - acc: 0.902 - 1s 50us/step - loss: 0.2137 - acc: 0.9026 - val_loss: 0.6455 - val_acc: 0.8226\n",
      "Epoch 159/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.2219 - acc: 0.906 - ETA: 0s - loss: 0.2114 - acc: 0.904 - ETA: 0s - loss: 0.2112 - acc: 0.905 - ETA: 0s - loss: 0.2113 - acc: 0.904 - ETA: 0s - loss: 0.2047 - acc: 0.909 - ETA: 0s - loss: 0.2045 - acc: 0.907 - ETA: 0s - loss: 0.2123 - acc: 0.904 - ETA: 0s - loss: 0.2113 - acc: 0.902 - ETA: 0s - loss: 0.2143 - acc: 0.901 - ETA: 0s - loss: 0.2139 - acc: 0.903 - ETA: 0s - loss: 0.2138 - acc: 0.903 - ETA: 0s - loss: 0.2141 - acc: 0.902 - ETA: 0s - loss: 0.2157 - acc: 0.901 - ETA: 0s - loss: 0.2139 - acc: 0.902 - ETA: 0s - loss: 0.2128 - acc: 0.903 - ETA: 0s - loss: 0.2112 - acc: 0.904 - ETA: 0s - loss: 0.2106 - acc: 0.905 - ETA: 0s - loss: 0.2121 - acc: 0.904 - 1s 50us/step - loss: 0.2126 - acc: 0.9042 - val_loss: 0.6204 - val_acc: 0.8325\n",
      "Epoch 160/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20513/20513 [==============================] - ETA: 0s - loss: 0.1321 - acc: 0.937 - ETA: 0s - loss: 0.1693 - acc: 0.927 - ETA: 0s - loss: 0.1882 - acc: 0.914 - ETA: 0s - loss: 0.2000 - acc: 0.909 - ETA: 0s - loss: 0.2018 - acc: 0.908 - ETA: 0s - loss: 0.2075 - acc: 0.907 - ETA: 0s - loss: 0.2091 - acc: 0.906 - ETA: 0s - loss: 0.2103 - acc: 0.905 - ETA: 0s - loss: 0.2100 - acc: 0.906 - ETA: 0s - loss: 0.2105 - acc: 0.905 - ETA: 0s - loss: 0.2121 - acc: 0.905 - ETA: 0s - loss: 0.2127 - acc: 0.904 - ETA: 0s - loss: 0.2123 - acc: 0.904 - ETA: 0s - loss: 0.2132 - acc: 0.904 - ETA: 0s - loss: 0.2135 - acc: 0.904 - ETA: 0s - loss: 0.2125 - acc: 0.904 - ETA: 0s - loss: 0.2131 - acc: 0.904 - ETA: 0s - loss: 0.2130 - acc: 0.905 - 1s 50us/step - loss: 0.2128 - acc: 0.9047 - val_loss: 0.6146 - val_acc: 0.8345\n",
      "Epoch 161/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.2484 - acc: 0.859 - ETA: 0s - loss: 0.1962 - acc: 0.902 - ETA: 0s - loss: 0.2045 - acc: 0.905 - ETA: 0s - loss: 0.2066 - acc: 0.902 - ETA: 0s - loss: 0.2080 - acc: 0.903 - ETA: 0s - loss: 0.2076 - acc: 0.904 - ETA: 0s - loss: 0.2104 - acc: 0.904 - ETA: 0s - loss: 0.2114 - acc: 0.903 - ETA: 0s - loss: 0.2120 - acc: 0.903 - ETA: 0s - loss: 0.2108 - acc: 0.903 - ETA: 0s - loss: 0.2077 - acc: 0.904 - ETA: 0s - loss: 0.2078 - acc: 0.904 - ETA: 0s - loss: 0.2078 - acc: 0.904 - ETA: 0s - loss: 0.2097 - acc: 0.902 - ETA: 0s - loss: 0.2106 - acc: 0.902 - ETA: 0s - loss: 0.2107 - acc: 0.902 - ETA: 0s - loss: 0.2107 - acc: 0.902 - ETA: 0s - loss: 0.2128 - acc: 0.902 - 1s 51us/step - loss: 0.2122 - acc: 0.9030 - val_loss: 0.6449 - val_acc: 0.8330\n",
      "Epoch 162/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.2814 - acc: 0.828 - ETA: 0s - loss: 0.2117 - acc: 0.908 - ETA: 0s - loss: 0.2119 - acc: 0.911 - ETA: 0s - loss: 0.2066 - acc: 0.911 - ETA: 0s - loss: 0.2070 - acc: 0.908 - ETA: 0s - loss: 0.2081 - acc: 0.907 - ETA: 0s - loss: 0.2148 - acc: 0.906 - ETA: 0s - loss: 0.2141 - acc: 0.905 - ETA: 0s - loss: 0.2172 - acc: 0.902 - ETA: 0s - loss: 0.2139 - acc: 0.903 - ETA: 0s - loss: 0.2143 - acc: 0.904 - ETA: 0s - loss: 0.2138 - acc: 0.904 - ETA: 0s - loss: 0.2145 - acc: 0.902 - ETA: 0s - loss: 0.2119 - acc: 0.903 - ETA: 0s - loss: 0.2129 - acc: 0.902 - ETA: 0s - loss: 0.2138 - acc: 0.901 - ETA: 0s - loss: 0.2132 - acc: 0.902 - ETA: 0s - loss: 0.2135 - acc: 0.902 - 1s 51us/step - loss: 0.2129 - acc: 0.9025 - val_loss: 0.6564 - val_acc: 0.8341\n",
      "Epoch 163/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1992 - acc: 0.890 - ETA: 0s - loss: 0.1965 - acc: 0.908 - ETA: 0s - loss: 0.1912 - acc: 0.910 - ETA: 0s - loss: 0.1975 - acc: 0.908 - ETA: 0s - loss: 0.1939 - acc: 0.908 - ETA: 0s - loss: 0.1969 - acc: 0.908 - ETA: 0s - loss: 0.2006 - acc: 0.907 - ETA: 0s - loss: 0.2036 - acc: 0.905 - ETA: 0s - loss: 0.2032 - acc: 0.906 - ETA: 0s - loss: 0.2035 - acc: 0.906 - ETA: 0s - loss: 0.2051 - acc: 0.906 - ETA: 0s - loss: 0.2050 - acc: 0.906 - ETA: 0s - loss: 0.2071 - acc: 0.904 - ETA: 0s - loss: 0.2075 - acc: 0.904 - ETA: 0s - loss: 0.2086 - acc: 0.903 - ETA: 0s - loss: 0.2086 - acc: 0.903 - ETA: 0s - loss: 0.2074 - acc: 0.904 - ETA: 0s - loss: 0.2094 - acc: 0.904 - 1s 50us/step - loss: 0.2116 - acc: 0.9037 - val_loss: 0.6218 - val_acc: 0.8323\n",
      "Epoch 164/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.1985 - acc: 0.921 - ETA: 0s - loss: 0.1840 - acc: 0.912 - ETA: 0s - loss: 0.1952 - acc: 0.908 - ETA: 0s - loss: 0.2066 - acc: 0.905 - ETA: 0s - loss: 0.2114 - acc: 0.903 - ETA: 0s - loss: 0.2071 - acc: 0.903 - ETA: 0s - loss: 0.2076 - acc: 0.903 - ETA: 0s - loss: 0.2083 - acc: 0.903 - ETA: 0s - loss: 0.2101 - acc: 0.902 - ETA: 0s - loss: 0.2123 - acc: 0.902 - ETA: 0s - loss: 0.2098 - acc: 0.904 - ETA: 0s - loss: 0.2101 - acc: 0.904 - ETA: 0s - loss: 0.2097 - acc: 0.904 - ETA: 0s - loss: 0.2113 - acc: 0.903 - ETA: 0s - loss: 0.2105 - acc: 0.903 - ETA: 0s - loss: 0.2118 - acc: 0.903 - ETA: 0s - loss: 0.2138 - acc: 0.902 - ETA: 0s - loss: 0.2133 - acc: 0.902 - 1s 50us/step - loss: 0.2140 - acc: 0.9022 - val_loss: 0.6473 - val_acc: 0.8240\n",
      "Epoch 165/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.2596 - acc: 0.890 - ETA: 0s - loss: 0.2095 - acc: 0.907 - ETA: 0s - loss: 0.1993 - acc: 0.910 - ETA: 0s - loss: 0.2081 - acc: 0.907 - ETA: 0s - loss: 0.2097 - acc: 0.906 - ETA: 0s - loss: 0.2080 - acc: 0.905 - ETA: 0s - loss: 0.2086 - acc: 0.905 - ETA: 0s - loss: 0.2092 - acc: 0.904 - ETA: 0s - loss: 0.2119 - acc: 0.903 - ETA: 0s - loss: 0.2125 - acc: 0.902 - ETA: 0s - loss: 0.2098 - acc: 0.904 - ETA: 0s - loss: 0.2112 - acc: 0.904 - ETA: 0s - loss: 0.2106 - acc: 0.905 - ETA: 0s - loss: 0.2096 - acc: 0.905 - ETA: 0s - loss: 0.2088 - acc: 0.905 - ETA: 0s - loss: 0.2095 - acc: 0.904 - ETA: 0s - loss: 0.2109 - acc: 0.903 - ETA: 0s - loss: 0.2125 - acc: 0.903 - 1s 49us/step - loss: 0.2125 - acc: 0.9031 - val_loss: 0.6155 - val_acc: 0.8319\n",
      "Epoch 166/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1890 - acc: 0.921 - ETA: 0s - loss: 0.1988 - acc: 0.905 - ETA: 0s - loss: 0.2019 - acc: 0.902 - ETA: 0s - loss: 0.2090 - acc: 0.901 - ETA: 0s - loss: 0.2048 - acc: 0.903 - ETA: 0s - loss: 0.2067 - acc: 0.902 - ETA: 0s - loss: 0.2053 - acc: 0.903 - ETA: 0s - loss: 0.2092 - acc: 0.901 - ETA: 0s - loss: 0.2120 - acc: 0.901 - ETA: 0s - loss: 0.2127 - acc: 0.901 - ETA: 0s - loss: 0.2108 - acc: 0.901 - ETA: 0s - loss: 0.2123 - acc: 0.900 - ETA: 0s - loss: 0.2140 - acc: 0.899 - ETA: 0s - loss: 0.2125 - acc: 0.900 - ETA: 0s - loss: 0.2140 - acc: 0.900 - ETA: 0s - loss: 0.2134 - acc: 0.901 - ETA: 0s - loss: 0.2126 - acc: 0.901 - ETA: 0s - loss: 0.2127 - acc: 0.901 - 1s 49us/step - loss: 0.2125 - acc: 0.9021 - val_loss: 0.6449 - val_acc: 0.8333\n",
      "Epoch 167/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.2246 - acc: 0.890 - ETA: 0s - loss: 0.2435 - acc: 0.882 - ETA: 0s - loss: 0.2191 - acc: 0.897 - ETA: 0s - loss: 0.2172 - acc: 0.898 - ETA: 0s - loss: 0.2117 - acc: 0.900 - ETA: 0s - loss: 0.2131 - acc: 0.900 - ETA: 0s - loss: 0.2102 - acc: 0.902 - ETA: 0s - loss: 0.2105 - acc: 0.903 - ETA: 0s - loss: 0.2108 - acc: 0.904 - ETA: 0s - loss: 0.2106 - acc: 0.903 - ETA: 0s - loss: 0.2086 - acc: 0.905 - ETA: 0s - loss: 0.2070 - acc: 0.906 - ETA: 0s - loss: 0.2079 - acc: 0.905 - ETA: 0s - loss: 0.2098 - acc: 0.905 - ETA: 0s - loss: 0.2104 - acc: 0.905 - ETA: 0s - loss: 0.2105 - acc: 0.904 - ETA: 0s - loss: 0.2105 - acc: 0.904 - ETA: 0s - loss: 0.2104 - acc: 0.904 - 1s 51us/step - loss: 0.2112 - acc: 0.9039 - val_loss: 0.6294 - val_acc: 0.8289\n",
      "Epoch 168/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.1960 - acc: 0.875 - ETA: 0s - loss: 0.2019 - acc: 0.904 - ETA: 0s - loss: 0.2028 - acc: 0.902 - ETA: 0s - loss: 0.2027 - acc: 0.904 - ETA: 0s - loss: 0.2081 - acc: 0.902 - ETA: 0s - loss: 0.2092 - acc: 0.900 - ETA: 0s - loss: 0.2089 - acc: 0.901 - ETA: 0s - loss: 0.2097 - acc: 0.901 - ETA: 0s - loss: 0.2088 - acc: 0.901 - ETA: 0s - loss: 0.2096 - acc: 0.902 - ETA: 0s - loss: 0.2099 - acc: 0.902 - ETA: 0s - loss: 0.2078 - acc: 0.903 - ETA: 0s - loss: 0.2085 - acc: 0.903 - ETA: 0s - loss: 0.2086 - acc: 0.904 - ETA: 0s - loss: 0.2100 - acc: 0.903 - ETA: 0s - loss: 0.2095 - acc: 0.903 - ETA: 0s - loss: 0.2093 - acc: 0.903 - ETA: 0s - loss: 0.2108 - acc: 0.902 - 1s 50us/step - loss: 0.2106 - acc: 0.9029 - val_loss: 0.6421 - val_acc: 0.8319\n",
      "Epoch 169/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.1904 - acc: 0.937 - ETA: 0s - loss: 0.2112 - acc: 0.900 - ETA: 0s - loss: 0.2009 - acc: 0.905 - ETA: 0s - loss: 0.2015 - acc: 0.902 - ETA: 0s - loss: 0.2049 - acc: 0.901 - ETA: 0s - loss: 0.2012 - acc: 0.902 - ETA: 0s - loss: 0.2009 - acc: 0.905 - ETA: 0s - loss: 0.2026 - acc: 0.905 - ETA: 0s - loss: 0.2032 - acc: 0.905 - ETA: 0s - loss: 0.2037 - acc: 0.906 - ETA: 0s - loss: 0.2053 - acc: 0.905 - ETA: 0s - loss: 0.2051 - acc: 0.905 - ETA: 0s - loss: 0.2053 - acc: 0.905 - ETA: 0s - loss: 0.2045 - acc: 0.906 - ETA: 0s - loss: 0.2063 - acc: 0.905 - ETA: 0s - loss: 0.2078 - acc: 0.904 - ETA: 0s - loss: 0.2093 - acc: 0.904 - ETA: 0s - loss: 0.2089 - acc: 0.903 - 1s 51us/step - loss: 0.2099 - acc: 0.9035 - val_loss: 0.6461 - val_acc: 0.8309\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 170/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1467 - acc: 0.953 - ETA: 0s - loss: 0.2131 - acc: 0.902 - ETA: 0s - loss: 0.1953 - acc: 0.912 - ETA: 0s - loss: 0.1958 - acc: 0.911 - ETA: 0s - loss: 0.1960 - acc: 0.911 - ETA: 0s - loss: 0.2002 - acc: 0.909 - ETA: 0s - loss: 0.2042 - acc: 0.906 - ETA: 0s - loss: 0.2044 - acc: 0.904 - ETA: 0s - loss: 0.2033 - acc: 0.905 - ETA: 0s - loss: 0.2072 - acc: 0.903 - ETA: 0s - loss: 0.2073 - acc: 0.903 - ETA: 0s - loss: 0.2078 - acc: 0.903 - ETA: 0s - loss: 0.2067 - acc: 0.903 - ETA: 0s - loss: 0.2103 - acc: 0.902 - ETA: 0s - loss: 0.2085 - acc: 0.903 - ETA: 0s - loss: 0.2082 - acc: 0.904 - ETA: 0s - loss: 0.2087 - acc: 0.903 - ETA: 0s - loss: 0.2093 - acc: 0.903 - 1s 50us/step - loss: 0.2110 - acc: 0.9025 - val_loss: 0.6705 - val_acc: 0.8209\n",
      "Epoch 171/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.2371 - acc: 0.890 - ETA: 0s - loss: 0.1963 - acc: 0.911 - ETA: 0s - loss: 0.2008 - acc: 0.910 - ETA: 0s - loss: 0.2062 - acc: 0.906 - ETA: 0s - loss: 0.2034 - acc: 0.905 - ETA: 0s - loss: 0.2053 - acc: 0.903 - ETA: 0s - loss: 0.2018 - acc: 0.906 - ETA: 0s - loss: 0.2044 - acc: 0.905 - ETA: 0s - loss: 0.2075 - acc: 0.904 - ETA: 0s - loss: 0.2069 - acc: 0.904 - ETA: 0s - loss: 0.2119 - acc: 0.902 - ETA: 0s - loss: 0.2126 - acc: 0.902 - ETA: 0s - loss: 0.2119 - acc: 0.902 - ETA: 0s - loss: 0.2096 - acc: 0.903 - ETA: 0s - loss: 0.2111 - acc: 0.902 - ETA: 0s - loss: 0.2093 - acc: 0.903 - ETA: 0s - loss: 0.2082 - acc: 0.904 - ETA: 0s - loss: 0.2090 - acc: 0.904 - 1s 50us/step - loss: 0.2100 - acc: 0.9039 - val_loss: 0.6265 - val_acc: 0.8305\n",
      "Epoch 172/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1687 - acc: 0.937 - ETA: 0s - loss: 0.2159 - acc: 0.917 - ETA: 0s - loss: 0.2205 - acc: 0.908 - ETA: 0s - loss: 0.2170 - acc: 0.906 - ETA: 0s - loss: 0.2108 - acc: 0.905 - ETA: 0s - loss: 0.2106 - acc: 0.905 - ETA: 0s - loss: 0.2104 - acc: 0.905 - ETA: 0s - loss: 0.2115 - acc: 0.903 - ETA: 0s - loss: 0.2129 - acc: 0.902 - ETA: 0s - loss: 0.2119 - acc: 0.903 - ETA: 0s - loss: 0.2133 - acc: 0.903 - ETA: 0s - loss: 0.2130 - acc: 0.903 - ETA: 0s - loss: 0.2125 - acc: 0.903 - ETA: 0s - loss: 0.2109 - acc: 0.904 - ETA: 0s - loss: 0.2112 - acc: 0.904 - ETA: 0s - loss: 0.2112 - acc: 0.904 - ETA: 0s - loss: 0.2118 - acc: 0.903 - ETA: 0s - loss: 0.2109 - acc: 0.904 - 1s 50us/step - loss: 0.2114 - acc: 0.9041 - val_loss: 0.6676 - val_acc: 0.8317\n",
      "Epoch 173/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.2298 - acc: 0.843 - ETA: 0s - loss: 0.1980 - acc: 0.914 - ETA: 0s - loss: 0.1869 - acc: 0.917 - ETA: 0s - loss: 0.1916 - acc: 0.915 - ETA: 0s - loss: 0.1943 - acc: 0.914 - ETA: 0s - loss: 0.1950 - acc: 0.914 - ETA: 0s - loss: 0.1974 - acc: 0.912 - ETA: 0s - loss: 0.2028 - acc: 0.909 - ETA: 0s - loss: 0.2037 - acc: 0.908 - ETA: 0s - loss: 0.2022 - acc: 0.908 - ETA: 0s - loss: 0.2055 - acc: 0.906 - ETA: 0s - loss: 0.2047 - acc: 0.906 - ETA: 0s - loss: 0.2048 - acc: 0.906 - ETA: 0s - loss: 0.2062 - acc: 0.906 - ETA: 0s - loss: 0.2066 - acc: 0.905 - ETA: 0s - loss: 0.2094 - acc: 0.903 - ETA: 0s - loss: 0.2109 - acc: 0.903 - ETA: 0s - loss: 0.2104 - acc: 0.904 - 1s 49us/step - loss: 0.2107 - acc: 0.9043 - val_loss: 0.6625 - val_acc: 0.8313\n",
      "Epoch 174/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.2455 - acc: 0.875 - ETA: 0s - loss: 0.2161 - acc: 0.909 - ETA: 0s - loss: 0.2064 - acc: 0.911 - ETA: 0s - loss: 0.2051 - acc: 0.907 - ETA: 0s - loss: 0.2084 - acc: 0.904 - ETA: 0s - loss: 0.2108 - acc: 0.902 - ETA: 0s - loss: 0.2101 - acc: 0.904 - ETA: 0s - loss: 0.2127 - acc: 0.904 - ETA: 0s - loss: 0.2120 - acc: 0.904 - ETA: 0s - loss: 0.2102 - acc: 0.905 - ETA: 0s - loss: 0.2097 - acc: 0.906 - ETA: 0s - loss: 0.2098 - acc: 0.905 - ETA: 0s - loss: 0.2110 - acc: 0.904 - ETA: 0s - loss: 0.2118 - acc: 0.903 - ETA: 0s - loss: 0.2123 - acc: 0.903 - ETA: 0s - loss: 0.2115 - acc: 0.903 - ETA: 0s - loss: 0.2110 - acc: 0.903 - ETA: 0s - loss: 0.2100 - acc: 0.903 - 1s 50us/step - loss: 0.2095 - acc: 0.9044 - val_loss: 0.6382 - val_acc: 0.8379\n",
      "Epoch 175/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.2087 - acc: 0.890 - ETA: 0s - loss: 0.1996 - acc: 0.905 - ETA: 0s - loss: 0.2123 - acc: 0.898 - ETA: 0s - loss: 0.2097 - acc: 0.901 - ETA: 0s - loss: 0.2075 - acc: 0.903 - ETA: 0s - loss: 0.2080 - acc: 0.904 - ETA: 0s - loss: 0.2051 - acc: 0.905 - ETA: 0s - loss: 0.2055 - acc: 0.905 - ETA: 0s - loss: 0.2056 - acc: 0.904 - ETA: 0s - loss: 0.2071 - acc: 0.904 - ETA: 0s - loss: 0.2108 - acc: 0.903 - ETA: 0s - loss: 0.2090 - acc: 0.904 - ETA: 0s - loss: 0.2098 - acc: 0.903 - ETA: 0s - loss: 0.2101 - acc: 0.902 - ETA: 0s - loss: 0.2100 - acc: 0.903 - ETA: 0s - loss: 0.2092 - acc: 0.903 - ETA: 0s - loss: 0.2110 - acc: 0.903 - ETA: 0s - loss: 0.2114 - acc: 0.902 - ETA: 0s - loss: 0.2100 - acc: 0.903 - 1s 54us/step - loss: 0.2100 - acc: 0.9034 - val_loss: 0.6720 - val_acc: 0.8318\n",
      "Epoch 176/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1578 - acc: 0.921 - ETA: 1s - loss: 0.1785 - acc: 0.918 - ETA: 1s - loss: 0.1797 - acc: 0.917 - ETA: 0s - loss: 0.1931 - acc: 0.910 - ETA: 0s - loss: 0.1942 - acc: 0.909 - ETA: 0s - loss: 0.1986 - acc: 0.907 - ETA: 0s - loss: 0.1997 - acc: 0.905 - ETA: 0s - loss: 0.2006 - acc: 0.905 - ETA: 0s - loss: 0.2012 - acc: 0.905 - ETA: 0s - loss: 0.2022 - acc: 0.905 - ETA: 0s - loss: 0.2044 - acc: 0.905 - ETA: 0s - loss: 0.2042 - acc: 0.905 - ETA: 0s - loss: 0.2037 - acc: 0.906 - ETA: 0s - loss: 0.2065 - acc: 0.905 - ETA: 0s - loss: 0.2084 - acc: 0.903 - ETA: 0s - loss: 0.2106 - acc: 0.902 - ETA: 0s - loss: 0.2106 - acc: 0.902 - ETA: 0s - loss: 0.2105 - acc: 0.901 - ETA: 0s - loss: 0.2109 - acc: 0.901 - 1s 53us/step - loss: 0.2104 - acc: 0.9015 - val_loss: 0.6628 - val_acc: 0.8255\n",
      "Epoch 177/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1799 - acc: 0.921 - ETA: 1s - loss: 0.2254 - acc: 0.900 - ETA: 1s - loss: 0.2063 - acc: 0.910 - ETA: 1s - loss: 0.1976 - acc: 0.912 - ETA: 1s - loss: 0.1957 - acc: 0.911 - ETA: 0s - loss: 0.1942 - acc: 0.911 - ETA: 0s - loss: 0.1947 - acc: 0.911 - ETA: 0s - loss: 0.1982 - acc: 0.910 - ETA: 0s - loss: 0.1994 - acc: 0.910 - ETA: 0s - loss: 0.2027 - acc: 0.907 - ETA: 0s - loss: 0.2039 - acc: 0.906 - ETA: 0s - loss: 0.2067 - acc: 0.905 - ETA: 0s - loss: 0.2067 - acc: 0.905 - ETA: 0s - loss: 0.2065 - acc: 0.905 - ETA: 0s - loss: 0.2063 - acc: 0.906 - ETA: 0s - loss: 0.2066 - acc: 0.905 - ETA: 0s - loss: 0.2092 - acc: 0.904 - ETA: 0s - loss: 0.2106 - acc: 0.904 - ETA: 0s - loss: 0.2100 - acc: 0.904 - ETA: 0s - loss: 0.2108 - acc: 0.903 - 1s 54us/step - loss: 0.2108 - acc: 0.9038 - val_loss: 0.6297 - val_acc: 0.8318\n",
      "Epoch 178/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.1432 - acc: 0.953 - ETA: 0s - loss: 0.1943 - acc: 0.907 - ETA: 0s - loss: 0.1862 - acc: 0.912 - ETA: 0s - loss: 0.1850 - acc: 0.914 - ETA: 0s - loss: 0.1927 - acc: 0.910 - ETA: 0s - loss: 0.1984 - acc: 0.907 - ETA: 0s - loss: 0.2046 - acc: 0.906 - ETA: 0s - loss: 0.2061 - acc: 0.903 - ETA: 0s - loss: 0.2068 - acc: 0.905 - ETA: 0s - loss: 0.2077 - acc: 0.904 - ETA: 0s - loss: 0.2066 - acc: 0.905 - ETA: 0s - loss: 0.2055 - acc: 0.905 - ETA: 0s - loss: 0.2070 - acc: 0.905 - ETA: 0s - loss: 0.2089 - acc: 0.904 - ETA: 0s - loss: 0.2105 - acc: 0.903 - ETA: 0s - loss: 0.2099 - acc: 0.903 - ETA: 0s - loss: 0.2105 - acc: 0.903 - ETA: 0s - loss: 0.2110 - acc: 0.903 - 1s 50us/step - loss: 0.2112 - acc: 0.9034 - val_loss: 0.6770 - val_acc: 0.8246\n",
      "Epoch 179/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1651 - acc: 0.906 - ETA: 0s - loss: 0.1927 - acc: 0.903 - ETA: 0s - loss: 0.2199 - acc: 0.900 - ETA: 0s - loss: 0.2109 - acc: 0.905 - ETA: 0s - loss: 0.2097 - acc: 0.905 - ETA: 0s - loss: 0.2063 - acc: 0.906 - ETA: 0s - loss: 0.2044 - acc: 0.907 - ETA: 0s - loss: 0.2037 - acc: 0.906 - ETA: 0s - loss: 0.2070 - acc: 0.905 - ETA: 0s - loss: 0.2077 - acc: 0.905 - ETA: 0s - loss: 0.2050 - acc: 0.906 - ETA: 0s - loss: 0.2060 - acc: 0.906 - ETA: 0s - loss: 0.2059 - acc: 0.906 - ETA: 0s - loss: 0.2077 - acc: 0.905 - ETA: 0s - loss: 0.2083 - acc: 0.905 - ETA: 0s - loss: 0.2087 - acc: 0.905 - ETA: 0s - loss: 0.2095 - acc: 0.904 - ETA: 0s - loss: 0.2107 - acc: 0.903 - ETA: 0s - loss: 0.2105 - acc: 0.904 - 1s 52us/step - loss: 0.2106 - acc: 0.9040 - val_loss: 0.6677 - val_acc: 0.8303\n",
      "Epoch 180/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.2021 - acc: 0.906 - ETA: 0s - loss: 0.1796 - acc: 0.921 - ETA: 0s - loss: 0.1896 - acc: 0.916 - ETA: 0s - loss: 0.1920 - acc: 0.913 - ETA: 0s - loss: 0.1916 - acc: 0.913 - ETA: 0s - loss: 0.1997 - acc: 0.909 - ETA: 0s - loss: 0.1985 - acc: 0.910 - ETA: 0s - loss: 0.2013 - acc: 0.908 - ETA: 0s - loss: 0.2030 - acc: 0.907 - ETA: 0s - loss: 0.2045 - acc: 0.906 - ETA: 0s - loss: 0.2032 - acc: 0.905 - ETA: 0s - loss: 0.2029 - acc: 0.906 - ETA: 0s - loss: 0.2045 - acc: 0.905 - ETA: 0s - loss: 0.2055 - acc: 0.905 - ETA: 0s - loss: 0.2063 - acc: 0.904 - ETA: 0s - loss: 0.2065 - acc: 0.904 - ETA: 0s - loss: 0.2074 - acc: 0.903 - ETA: 0s - loss: 0.2083 - acc: 0.903 - 1s 50us/step - loss: 0.2082 - acc: 0.9034 - val_loss: 0.6369 - val_acc: 0.8353\n",
      "Epoch 181/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.0991 - acc: 0.984 - ETA: 0s - loss: 0.1819 - acc: 0.920 - ETA: 0s - loss: 0.1979 - acc: 0.911 - ETA: 0s - loss: 0.1950 - acc: 0.909 - ETA: 0s - loss: 0.1943 - acc: 0.909 - ETA: 0s - loss: 0.1978 - acc: 0.907 - ETA: 0s - loss: 0.1976 - acc: 0.908 - ETA: 0s - loss: 0.1999 - acc: 0.906 - ETA: 0s - loss: 0.1996 - acc: 0.907 - ETA: 0s - loss: 0.2013 - acc: 0.906 - ETA: 0s - loss: 0.2022 - acc: 0.905 - ETA: 0s - loss: 0.2025 - acc: 0.904 - ETA: 0s - loss: 0.2047 - acc: 0.904 - ETA: 0s - loss: 0.2051 - acc: 0.903 - ETA: 0s - loss: 0.2062 - acc: 0.903 - ETA: 0s - loss: 0.2079 - acc: 0.902 - ETA: 0s - loss: 0.2064 - acc: 0.903 - ETA: 0s - loss: 0.2064 - acc: 0.903 - 1s 51us/step - loss: 0.2090 - acc: 0.9022 - val_loss: 0.6430 - val_acc: 0.8300\n",
      "Epoch 182/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1639 - acc: 0.906 - ETA: 0s - loss: 0.2018 - acc: 0.904 - ETA: 0s - loss: 0.2090 - acc: 0.902 - ETA: 0s - loss: 0.1992 - acc: 0.906 - ETA: 0s - loss: 0.1987 - acc: 0.908 - ETA: 0s - loss: 0.2013 - acc: 0.906 - ETA: 0s - loss: 0.2006 - acc: 0.908 - ETA: 0s - loss: 0.2002 - acc: 0.908 - ETA: 0s - loss: 0.2012 - acc: 0.909 - ETA: 0s - loss: 0.2039 - acc: 0.908 - ETA: 0s - loss: 0.2047 - acc: 0.907 - ETA: 0s - loss: 0.2057 - acc: 0.906 - ETA: 0s - loss: 0.2060 - acc: 0.906 - ETA: 0s - loss: 0.2058 - acc: 0.905 - ETA: 0s - loss: 0.2060 - acc: 0.905 - ETA: 0s - loss: 0.2073 - acc: 0.905 - ETA: 0s - loss: 0.2074 - acc: 0.905 - ETA: 0s - loss: 0.2087 - acc: 0.904 - 1s 50us/step - loss: 0.2081 - acc: 0.9049 - val_loss: 0.6651 - val_acc: 0.8366\n",
      "Epoch 183/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1762 - acc: 0.937 - ETA: 0s - loss: 0.1980 - acc: 0.901 - ETA: 0s - loss: 0.1892 - acc: 0.911 - ETA: 0s - loss: 0.1921 - acc: 0.908 - ETA: 0s - loss: 0.1944 - acc: 0.910 - ETA: 0s - loss: 0.1973 - acc: 0.911 - ETA: 0s - loss: 0.1963 - acc: 0.911 - ETA: 0s - loss: 0.1966 - acc: 0.912 - ETA: 0s - loss: 0.2009 - acc: 0.908 - ETA: 0s - loss: 0.2027 - acc: 0.906 - ETA: 0s - loss: 0.2067 - acc: 0.904 - ETA: 0s - loss: 0.2077 - acc: 0.902 - ETA: 0s - loss: 0.2067 - acc: 0.904 - ETA: 0s - loss: 0.2107 - acc: 0.902 - ETA: 0s - loss: 0.2100 - acc: 0.903 - ETA: 0s - loss: 0.2109 - acc: 0.902 - ETA: 0s - loss: 0.2112 - acc: 0.902 - ETA: 0s - loss: 0.2099 - acc: 0.902 - 1s 49us/step - loss: 0.2102 - acc: 0.9028 - val_loss: 0.6380 - val_acc: 0.8294\n",
      "Epoch 184/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.2294 - acc: 0.875 - ETA: 0s - loss: 0.1891 - acc: 0.907 - ETA: 0s - loss: 0.1920 - acc: 0.910 - ETA: 0s - loss: 0.1974 - acc: 0.912 - ETA: 0s - loss: 0.2019 - acc: 0.910 - ETA: 0s - loss: 0.1983 - acc: 0.911 - ETA: 0s - loss: 0.2018 - acc: 0.909 - ETA: 0s - loss: 0.2018 - acc: 0.908 - ETA: 0s - loss: 0.2038 - acc: 0.906 - ETA: 0s - loss: 0.2070 - acc: 0.906 - ETA: 0s - loss: 0.2067 - acc: 0.906 - ETA: 0s - loss: 0.2081 - acc: 0.906 - ETA: 0s - loss: 0.2098 - acc: 0.904 - ETA: 0s - loss: 0.2097 - acc: 0.905 - ETA: 0s - loss: 0.2092 - acc: 0.905 - ETA: 0s - loss: 0.2088 - acc: 0.905 - ETA: 0s - loss: 0.2084 - acc: 0.905 - ETA: 0s - loss: 0.2082 - acc: 0.905 - 1s 50us/step - loss: 0.2090 - acc: 0.9046 - val_loss: 0.7494 - val_acc: 0.8226\n",
      "Epoch 185/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1440 - acc: 0.937 - ETA: 0s - loss: 0.2088 - acc: 0.896 - ETA: 0s - loss: 0.2019 - acc: 0.906 - ETA: 0s - loss: 0.2048 - acc: 0.906 - ETA: 0s - loss: 0.2067 - acc: 0.906 - ETA: 0s - loss: 0.2142 - acc: 0.903 - ETA: 0s - loss: 0.2158 - acc: 0.901 - ETA: 0s - loss: 0.2158 - acc: 0.900 - ETA: 0s - loss: 0.2126 - acc: 0.903 - ETA: 0s - loss: 0.2129 - acc: 0.903 - ETA: 0s - loss: 0.2099 - acc: 0.904 - ETA: 0s - loss: 0.2101 - acc: 0.904 - ETA: 0s - loss: 0.2080 - acc: 0.905 - ETA: 0s - loss: 0.2095 - acc: 0.904 - ETA: 0s - loss: 0.2085 - acc: 0.905 - ETA: 0s - loss: 0.2080 - acc: 0.904 - ETA: 0s - loss: 0.2089 - acc: 0.904 - ETA: 0s - loss: 0.2083 - acc: 0.905 - 1s 50us/step - loss: 0.2087 - acc: 0.9044 - val_loss: 0.6654 - val_acc: 0.8304\n",
      "Epoch 186/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.1474 - acc: 0.906 - ETA: 0s - loss: 0.1926 - acc: 0.904 - ETA: 0s - loss: 0.1908 - acc: 0.909 - ETA: 0s - loss: 0.2021 - acc: 0.901 - ETA: 0s - loss: 0.1944 - acc: 0.906 - ETA: 0s - loss: 0.1922 - acc: 0.908 - ETA: 0s - loss: 0.1924 - acc: 0.907 - ETA: 0s - loss: 0.1952 - acc: 0.907 - ETA: 0s - loss: 0.1987 - acc: 0.906 - ETA: 0s - loss: 0.2017 - acc: 0.904 - ETA: 0s - loss: 0.2001 - acc: 0.905 - ETA: 0s - loss: 0.2002 - acc: 0.906 - ETA: 0s - loss: 0.2020 - acc: 0.906 - ETA: 0s - loss: 0.2029 - acc: 0.906 - ETA: 0s - loss: 0.2040 - acc: 0.905 - ETA: 0s - loss: 0.2049 - acc: 0.905 - ETA: 0s - loss: 0.2057 - acc: 0.905 - ETA: 0s - loss: 0.2056 - acc: 0.904 - ETA: 0s - loss: 0.2077 - acc: 0.903 - 1s 53us/step - loss: 0.2080 - acc: 0.9031 - val_loss: 0.6611 - val_acc: 0.8296\n",
      "Epoch 187/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.2461 - acc: 0.859 - ETA: 1s - loss: 0.1893 - acc: 0.907 - ETA: 1s - loss: 0.1845 - acc: 0.913 - ETA: 0s - loss: 0.1872 - acc: 0.914 - ETA: 0s - loss: 0.1976 - acc: 0.911 - ETA: 0s - loss: 0.1945 - acc: 0.911 - ETA: 0s - loss: 0.1942 - acc: 0.911 - ETA: 0s - loss: 0.1961 - acc: 0.911 - ETA: 0s - loss: 0.1968 - acc: 0.910 - ETA: 0s - loss: 0.1974 - acc: 0.910 - ETA: 0s - loss: 0.2015 - acc: 0.908 - ETA: 0s - loss: 0.2053 - acc: 0.907 - ETA: 0s - loss: 0.2046 - acc: 0.907 - ETA: 0s - loss: 0.2057 - acc: 0.906 - ETA: 0s - loss: 0.2062 - acc: 0.904 - ETA: 0s - loss: 0.2047 - acc: 0.905 - ETA: 0s - loss: 0.2049 - acc: 0.904 - ETA: 0s - loss: 0.2068 - acc: 0.903 - ETA: 0s - loss: 0.2079 - acc: 0.903 - ETA: 0s - loss: 0.2093 - acc: 0.903 - 1s 55us/step - loss: 0.2088 - acc: 0.9034 - val_loss: 0.6838 - val_acc: 0.8336\n",
      "Epoch 188/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20513/20513 [==============================] - ETA: 0s - loss: 0.0685 - acc: 0.984 - ETA: 0s - loss: 0.2030 - acc: 0.904 - ETA: 0s - loss: 0.1971 - acc: 0.905 - ETA: 0s - loss: 0.2034 - acc: 0.906 - ETA: 0s - loss: 0.2053 - acc: 0.902 - ETA: 0s - loss: 0.2091 - acc: 0.903 - ETA: 0s - loss: 0.2079 - acc: 0.904 - ETA: 0s - loss: 0.2078 - acc: 0.906 - ETA: 0s - loss: 0.2093 - acc: 0.904 - ETA: 0s - loss: 0.2100 - acc: 0.905 - ETA: 0s - loss: 0.2099 - acc: 0.904 - ETA: 0s - loss: 0.2081 - acc: 0.905 - ETA: 0s - loss: 0.2081 - acc: 0.905 - ETA: 0s - loss: 0.2090 - acc: 0.905 - ETA: 0s - loss: 0.2080 - acc: 0.905 - ETA: 0s - loss: 0.2073 - acc: 0.905 - ETA: 0s - loss: 0.2075 - acc: 0.904 - ETA: 0s - loss: 0.2089 - acc: 0.904 - ETA: 0s - loss: 0.2095 - acc: 0.903 - 1s 52us/step - loss: 0.2094 - acc: 0.9037 - val_loss: 0.7053 - val_acc: 0.8270\n",
      "Epoch 189/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.2141 - acc: 0.875 - ETA: 0s - loss: 0.2013 - acc: 0.898 - ETA: 0s - loss: 0.1942 - acc: 0.905 - ETA: 0s - loss: 0.1912 - acc: 0.910 - ETA: 0s - loss: 0.1962 - acc: 0.906 - ETA: 0s - loss: 0.1999 - acc: 0.906 - ETA: 0s - loss: 0.2009 - acc: 0.905 - ETA: 0s - loss: 0.2017 - acc: 0.905 - ETA: 0s - loss: 0.2031 - acc: 0.905 - ETA: 0s - loss: 0.2029 - acc: 0.905 - ETA: 0s - loss: 0.2058 - acc: 0.903 - ETA: 0s - loss: 0.2085 - acc: 0.903 - ETA: 0s - loss: 0.2087 - acc: 0.903 - ETA: 0s - loss: 0.2081 - acc: 0.904 - ETA: 0s - loss: 0.2075 - acc: 0.903 - ETA: 0s - loss: 0.2072 - acc: 0.904 - ETA: 0s - loss: 0.2084 - acc: 0.903 - ETA: 0s - loss: 0.2076 - acc: 0.904 - 1s 50us/step - loss: 0.2084 - acc: 0.9043 - val_loss: 0.6585 - val_acc: 0.8309\n",
      "Epoch 190/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1215 - acc: 0.937 - ETA: 0s - loss: 0.1621 - acc: 0.927 - ETA: 0s - loss: 0.1857 - acc: 0.916 - ETA: 0s - loss: 0.1964 - acc: 0.909 - ETA: 0s - loss: 0.1999 - acc: 0.908 - ETA: 0s - loss: 0.1990 - acc: 0.909 - ETA: 0s - loss: 0.1996 - acc: 0.910 - ETA: 0s - loss: 0.2022 - acc: 0.909 - ETA: 0s - loss: 0.2012 - acc: 0.910 - ETA: 0s - loss: 0.2056 - acc: 0.907 - ETA: 0s - loss: 0.2048 - acc: 0.907 - ETA: 0s - loss: 0.2055 - acc: 0.907 - ETA: 0s - loss: 0.2065 - acc: 0.906 - ETA: 0s - loss: 0.2075 - acc: 0.906 - ETA: 0s - loss: 0.2077 - acc: 0.905 - ETA: 0s - loss: 0.2075 - acc: 0.905 - ETA: 0s - loss: 0.2073 - acc: 0.904 - ETA: 0s - loss: 0.2083 - acc: 0.904 - ETA: 0s - loss: 0.2086 - acc: 0.903 - 1s 52us/step - loss: 0.2083 - acc: 0.9041 - val_loss: 0.6735 - val_acc: 0.8290\n",
      "Epoch 191/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.2924 - acc: 0.828 - ETA: 0s - loss: 0.2039 - acc: 0.896 - ETA: 0s - loss: 0.2000 - acc: 0.904 - ETA: 0s - loss: 0.1954 - acc: 0.908 - ETA: 0s - loss: 0.1964 - acc: 0.909 - ETA: 0s - loss: 0.1965 - acc: 0.908 - ETA: 0s - loss: 0.1973 - acc: 0.908 - ETA: 0s - loss: 0.1998 - acc: 0.907 - ETA: 0s - loss: 0.2002 - acc: 0.906 - ETA: 0s - loss: 0.1991 - acc: 0.908 - ETA: 0s - loss: 0.2014 - acc: 0.908 - ETA: 0s - loss: 0.2049 - acc: 0.906 - ETA: 0s - loss: 0.2070 - acc: 0.905 - ETA: 0s - loss: 0.2059 - acc: 0.906 - ETA: 0s - loss: 0.2052 - acc: 0.906 - ETA: 0s - loss: 0.2061 - acc: 0.906 - ETA: 0s - loss: 0.2063 - acc: 0.906 - ETA: 0s - loss: 0.2068 - acc: 0.906 - ETA: 0s - loss: 0.2072 - acc: 0.905 - 1s 54us/step - loss: 0.2080 - acc: 0.9055 - val_loss: 0.6741 - val_acc: 0.8229\n",
      "Epoch 192/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.1826 - acc: 0.968 - ETA: 0s - loss: 0.1975 - acc: 0.917 - ETA: 0s - loss: 0.1985 - acc: 0.915 - ETA: 0s - loss: 0.1964 - acc: 0.914 - ETA: 0s - loss: 0.1955 - acc: 0.909 - ETA: 0s - loss: 0.1998 - acc: 0.907 - ETA: 0s - loss: 0.2012 - acc: 0.906 - ETA: 0s - loss: 0.2007 - acc: 0.906 - ETA: 0s - loss: 0.1997 - acc: 0.908 - ETA: 0s - loss: 0.2025 - acc: 0.907 - ETA: 0s - loss: 0.2035 - acc: 0.906 - ETA: 0s - loss: 0.2034 - acc: 0.906 - ETA: 0s - loss: 0.2029 - acc: 0.905 - ETA: 0s - loss: 0.2048 - acc: 0.905 - ETA: 0s - loss: 0.2052 - acc: 0.905 - ETA: 0s - loss: 0.2059 - acc: 0.904 - ETA: 0s - loss: 0.2071 - acc: 0.904 - ETA: 0s - loss: 0.2069 - acc: 0.905 - ETA: 0s - loss: 0.2074 - acc: 0.905 - ETA: 0s - loss: 0.2088 - acc: 0.904 - ETA: 0s - loss: 0.2087 - acc: 0.905 - 1s 58us/step - loss: 0.2087 - acc: 0.9054 - val_loss: 0.6661 - val_acc: 0.8313\n",
      "Epoch 193/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1508 - acc: 0.921 - ETA: 0s - loss: 0.1854 - acc: 0.917 - ETA: 0s - loss: 0.2012 - acc: 0.911 - ETA: 0s - loss: 0.2063 - acc: 0.910 - ETA: 0s - loss: 0.2092 - acc: 0.909 - ETA: 0s - loss: 0.2078 - acc: 0.909 - ETA: 0s - loss: 0.2099 - acc: 0.907 - ETA: 0s - loss: 0.2086 - acc: 0.905 - ETA: 0s - loss: 0.2071 - acc: 0.906 - ETA: 0s - loss: 0.2081 - acc: 0.905 - ETA: 0s - loss: 0.2079 - acc: 0.904 - ETA: 0s - loss: 0.2077 - acc: 0.904 - ETA: 0s - loss: 0.2079 - acc: 0.904 - ETA: 0s - loss: 0.2116 - acc: 0.902 - ETA: 0s - loss: 0.2105 - acc: 0.902 - ETA: 0s - loss: 0.2100 - acc: 0.902 - ETA: 0s - loss: 0.2093 - acc: 0.902 - ETA: 0s - loss: 0.2085 - acc: 0.903 - ETA: 0s - loss: 0.2071 - acc: 0.904 - 1s 54us/step - loss: 0.2072 - acc: 0.9044 - val_loss: 0.7119 - val_acc: 0.8174\n",
      "Epoch 194/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.1751 - acc: 0.921 - ETA: 0s - loss: 0.2100 - acc: 0.900 - ETA: 0s - loss: 0.1974 - acc: 0.905 - ETA: 0s - loss: 0.2007 - acc: 0.907 - ETA: 0s - loss: 0.2049 - acc: 0.904 - ETA: 0s - loss: 0.2051 - acc: 0.905 - ETA: 0s - loss: 0.2069 - acc: 0.905 - ETA: 0s - loss: 0.2039 - acc: 0.907 - ETA: 0s - loss: 0.2058 - acc: 0.908 - ETA: 0s - loss: 0.2088 - acc: 0.907 - ETA: 0s - loss: 0.2090 - acc: 0.907 - ETA: 0s - loss: 0.2085 - acc: 0.906 - ETA: 0s - loss: 0.2096 - acc: 0.904 - ETA: 0s - loss: 0.2092 - acc: 0.904 - ETA: 0s - loss: 0.2091 - acc: 0.904 - ETA: 0s - loss: 0.2090 - acc: 0.904 - ETA: 0s - loss: 0.2087 - acc: 0.904 - ETA: 0s - loss: 0.2083 - acc: 0.904 - 1s 49us/step - loss: 0.2085 - acc: 0.9042 - val_loss: 0.6794 - val_acc: 0.8327\n",
      "Epoch 195/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.2556 - acc: 0.875 - ETA: 0s - loss: 0.1795 - acc: 0.909 - ETA: 0s - loss: 0.2011 - acc: 0.906 - ETA: 0s - loss: 0.2000 - acc: 0.903 - ETA: 0s - loss: 0.2025 - acc: 0.900 - ETA: 0s - loss: 0.1962 - acc: 0.904 - ETA: 0s - loss: 0.2016 - acc: 0.904 - ETA: 0s - loss: 0.2043 - acc: 0.903 - ETA: 0s - loss: 0.2052 - acc: 0.905 - ETA: 0s - loss: 0.2038 - acc: 0.905 - ETA: 0s - loss: 0.2038 - acc: 0.906 - ETA: 0s - loss: 0.2050 - acc: 0.905 - ETA: 0s - loss: 0.2040 - acc: 0.905 - ETA: 0s - loss: 0.2057 - acc: 0.905 - ETA: 0s - loss: 0.2057 - acc: 0.904 - ETA: 0s - loss: 0.2046 - acc: 0.904 - ETA: 0s - loss: 0.2056 - acc: 0.904 - ETA: 0s - loss: 0.2066 - acc: 0.904 - ETA: 0s - loss: 0.2078 - acc: 0.903 - 1s 52us/step - loss: 0.2073 - acc: 0.9038 - val_loss: 0.6524 - val_acc: 0.8336\n",
      "Epoch 196/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.3197 - acc: 0.828 - ETA: 0s - loss: 0.1906 - acc: 0.913 - ETA: 0s - loss: 0.1896 - acc: 0.912 - ETA: 0s - loss: 0.2025 - acc: 0.904 - ETA: 0s - loss: 0.2037 - acc: 0.904 - ETA: 0s - loss: 0.2028 - acc: 0.905 - ETA: 0s - loss: 0.2026 - acc: 0.903 - ETA: 0s - loss: 0.2012 - acc: 0.903 - ETA: 0s - loss: 0.2007 - acc: 0.903 - ETA: 0s - loss: 0.2025 - acc: 0.903 - ETA: 0s - loss: 0.2027 - acc: 0.903 - ETA: 0s - loss: 0.2031 - acc: 0.903 - ETA: 0s - loss: 0.2045 - acc: 0.903 - ETA: 0s - loss: 0.2038 - acc: 0.903 - ETA: 0s - loss: 0.2027 - acc: 0.903 - ETA: 0s - loss: 0.2043 - acc: 0.904 - ETA: 0s - loss: 0.2047 - acc: 0.904 - ETA: 0s - loss: 0.2062 - acc: 0.902 - 1s 50us/step - loss: 0.2074 - acc: 0.9027 - val_loss: 0.6646 - val_acc: 0.8339\n",
      "Epoch 197/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1911 - acc: 0.906 - ETA: 0s - loss: 0.1854 - acc: 0.920 - ETA: 0s - loss: 0.1841 - acc: 0.913 - ETA: 0s - loss: 0.1969 - acc: 0.905 - ETA: 0s - loss: 0.1948 - acc: 0.906 - ETA: 0s - loss: 0.1945 - acc: 0.907 - ETA: 0s - loss: 0.1962 - acc: 0.907 - ETA: 0s - loss: 0.1959 - acc: 0.907 - ETA: 0s - loss: 0.1983 - acc: 0.908 - ETA: 0s - loss: 0.1985 - acc: 0.908 - ETA: 0s - loss: 0.2010 - acc: 0.908 - ETA: 0s - loss: 0.2018 - acc: 0.908 - ETA: 0s - loss: 0.2027 - acc: 0.908 - ETA: 0s - loss: 0.2042 - acc: 0.907 - ETA: 0s - loss: 0.2054 - acc: 0.906 - ETA: 0s - loss: 0.2054 - acc: 0.906 - ETA: 0s - loss: 0.2070 - acc: 0.906 - ETA: 0s - loss: 0.2091 - acc: 0.904 - ETA: 0s - loss: 0.2077 - acc: 0.906 - 1s 52us/step - loss: 0.2081 - acc: 0.9059 - val_loss: 0.6655 - val_acc: 0.8255\n",
      "Epoch 198/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.2241 - acc: 0.875 - ETA: 0s - loss: 0.2089 - acc: 0.903 - ETA: 0s - loss: 0.2111 - acc: 0.900 - ETA: 0s - loss: 0.2018 - acc: 0.906 - ETA: 0s - loss: 0.1998 - acc: 0.908 - ETA: 0s - loss: 0.2022 - acc: 0.907 - ETA: 0s - loss: 0.2029 - acc: 0.906 - ETA: 0s - loss: 0.2045 - acc: 0.904 - ETA: 0s - loss: 0.2025 - acc: 0.905 - ETA: 0s - loss: 0.2023 - acc: 0.905 - ETA: 0s - loss: 0.2021 - acc: 0.905 - ETA: 0s - loss: 0.2052 - acc: 0.904 - ETA: 0s - loss: 0.2064 - acc: 0.904 - ETA: 0s - loss: 0.2064 - acc: 0.904 - ETA: 0s - loss: 0.2060 - acc: 0.905 - ETA: 0s - loss: 0.2074 - acc: 0.904 - ETA: 0s - loss: 0.2072 - acc: 0.904 - ETA: 0s - loss: 0.2076 - acc: 0.904 - 1s 49us/step - loss: 0.2084 - acc: 0.9039 - val_loss: 0.6813 - val_acc: 0.8359\n",
      "Epoch 199/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1749 - acc: 0.875 - ETA: 0s - loss: 0.1987 - acc: 0.904 - ETA: 0s - loss: 0.2012 - acc: 0.905 - ETA: 0s - loss: 0.1968 - acc: 0.906 - ETA: 0s - loss: 0.2041 - acc: 0.905 - ETA: 0s - loss: 0.2019 - acc: 0.907 - ETA: 0s - loss: 0.2012 - acc: 0.907 - ETA: 0s - loss: 0.2001 - acc: 0.909 - ETA: 0s - loss: 0.2008 - acc: 0.908 - ETA: 0s - loss: 0.2034 - acc: 0.906 - ETA: 0s - loss: 0.2027 - acc: 0.907 - ETA: 0s - loss: 0.2040 - acc: 0.906 - ETA: 0s - loss: 0.2038 - acc: 0.906 - ETA: 0s - loss: 0.2038 - acc: 0.906 - ETA: 0s - loss: 0.2057 - acc: 0.905 - ETA: 0s - loss: 0.2072 - acc: 0.905 - ETA: 0s - loss: 0.2066 - acc: 0.906 - ETA: 0s - loss: 0.2078 - acc: 0.905 - 1s 50us/step - loss: 0.2080 - acc: 0.9060 - val_loss: 0.6705 - val_acc: 0.8308\n",
      "Epoch 200/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.2248 - acc: 0.921 - ETA: 0s - loss: 0.2228 - acc: 0.904 - ETA: 0s - loss: 0.2040 - acc: 0.912 - ETA: 0s - loss: 0.2009 - acc: 0.910 - ETA: 0s - loss: 0.2009 - acc: 0.910 - ETA: 0s - loss: 0.2013 - acc: 0.909 - ETA: 0s - loss: 0.2039 - acc: 0.907 - ETA: 0s - loss: 0.2087 - acc: 0.907 - ETA: 0s - loss: 0.2073 - acc: 0.908 - ETA: 0s - loss: 0.2068 - acc: 0.907 - ETA: 0s - loss: 0.2064 - acc: 0.907 - ETA: 0s - loss: 0.2059 - acc: 0.907 - ETA: 0s - loss: 0.2070 - acc: 0.905 - ETA: 0s - loss: 0.2067 - acc: 0.906 - ETA: 0s - loss: 0.2058 - acc: 0.906 - ETA: 0s - loss: 0.2056 - acc: 0.906 - ETA: 0s - loss: 0.2056 - acc: 0.905 - ETA: 0s - loss: 0.2051 - acc: 0.905 - 1s 49us/step - loss: 0.2060 - acc: 0.9054 - val_loss: 0.6771 - val_acc: 0.8313\n",
      "Epoch 201/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.2710 - acc: 0.890 - ETA: 0s - loss: 0.2167 - acc: 0.904 - ETA: 0s - loss: 0.2042 - acc: 0.908 - ETA: 0s - loss: 0.2070 - acc: 0.910 - ETA: 0s - loss: 0.2050 - acc: 0.909 - ETA: 0s - loss: 0.2018 - acc: 0.911 - ETA: 0s - loss: 0.2063 - acc: 0.908 - ETA: 0s - loss: 0.2083 - acc: 0.906 - ETA: 0s - loss: 0.2066 - acc: 0.905 - ETA: 0s - loss: 0.2028 - acc: 0.907 - ETA: 0s - loss: 0.2035 - acc: 0.906 - ETA: 0s - loss: 0.2025 - acc: 0.906 - ETA: 0s - loss: 0.2033 - acc: 0.906 - ETA: 0s - loss: 0.2049 - acc: 0.905 - ETA: 0s - loss: 0.2074 - acc: 0.905 - ETA: 0s - loss: 0.2062 - acc: 0.905 - ETA: 0s - loss: 0.2063 - acc: 0.905 - ETA: 0s - loss: 0.2077 - acc: 0.905 - 1s 49us/step - loss: 0.2075 - acc: 0.9050 - val_loss: 0.6684 - val_acc: 0.8290\n",
      "Epoch 202/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.2212 - acc: 0.906 - ETA: 0s - loss: 0.1966 - acc: 0.903 - ETA: 0s - loss: 0.1882 - acc: 0.909 - ETA: 0s - loss: 0.1971 - acc: 0.909 - ETA: 0s - loss: 0.1994 - acc: 0.909 - ETA: 0s - loss: 0.2015 - acc: 0.910 - ETA: 0s - loss: 0.2020 - acc: 0.909 - ETA: 0s - loss: 0.2008 - acc: 0.909 - ETA: 0s - loss: 0.2019 - acc: 0.907 - ETA: 0s - loss: 0.2026 - acc: 0.907 - ETA: 0s - loss: 0.2022 - acc: 0.907 - ETA: 0s - loss: 0.2041 - acc: 0.906 - ETA: 0s - loss: 0.2043 - acc: 0.906 - ETA: 0s - loss: 0.2048 - acc: 0.905 - ETA: 0s - loss: 0.2050 - acc: 0.906 - ETA: 0s - loss: 0.2056 - acc: 0.905 - ETA: 0s - loss: 0.2060 - acc: 0.905 - ETA: 0s - loss: 0.2066 - acc: 0.905 - 1s 50us/step - loss: 0.2072 - acc: 0.9048 - val_loss: 0.7147 - val_acc: 0.8217\n",
      "Epoch 203/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1184 - acc: 0.937 - ETA: 0s - loss: 0.2077 - acc: 0.903 - ETA: 0s - loss: 0.2005 - acc: 0.909 - ETA: 0s - loss: 0.2053 - acc: 0.905 - ETA: 0s - loss: 0.2045 - acc: 0.904 - ETA: 0s - loss: 0.2042 - acc: 0.906 - ETA: 0s - loss: 0.2023 - acc: 0.907 - ETA: 0s - loss: 0.2017 - acc: 0.907 - ETA: 0s - loss: 0.2042 - acc: 0.905 - ETA: 0s - loss: 0.2061 - acc: 0.905 - ETA: 0s - loss: 0.2061 - acc: 0.905 - ETA: 0s - loss: 0.2072 - acc: 0.904 - ETA: 0s - loss: 0.2072 - acc: 0.904 - ETA: 0s - loss: 0.2055 - acc: 0.905 - ETA: 0s - loss: 0.2058 - acc: 0.905 - ETA: 0s - loss: 0.2088 - acc: 0.905 - ETA: 0s - loss: 0.2088 - acc: 0.905 - ETA: 0s - loss: 0.2082 - acc: 0.905 - 1s 50us/step - loss: 0.2083 - acc: 0.9059 - val_loss: 0.6945 - val_acc: 0.8285\n",
      "Epoch 204/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1808 - acc: 0.921 - ETA: 0s - loss: 0.1812 - acc: 0.913 - ETA: 0s - loss: 0.1970 - acc: 0.909 - ETA: 0s - loss: 0.1990 - acc: 0.906 - ETA: 0s - loss: 0.2023 - acc: 0.903 - ETA: 0s - loss: 0.2040 - acc: 0.905 - ETA: 0s - loss: 0.2005 - acc: 0.906 - ETA: 0s - loss: 0.1993 - acc: 0.907 - ETA: 0s - loss: 0.2008 - acc: 0.907 - ETA: 0s - loss: 0.2000 - acc: 0.906 - ETA: 0s - loss: 0.2032 - acc: 0.906 - ETA: 0s - loss: 0.2041 - acc: 0.906 - ETA: 0s - loss: 0.2051 - acc: 0.906 - ETA: 0s - loss: 0.2069 - acc: 0.905 - ETA: 0s - loss: 0.2085 - acc: 0.904 - ETA: 0s - loss: 0.2079 - acc: 0.904 - ETA: 0s - loss: 0.2080 - acc: 0.904 - ETA: 0s - loss: 0.2083 - acc: 0.904 - 1s 49us/step - loss: 0.2075 - acc: 0.9052 - val_loss: 0.7043 - val_acc: 0.8367\n",
      "Epoch 205/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.2458 - acc: 0.875 - ETA: 0s - loss: 0.2122 - acc: 0.901 - ETA: 0s - loss: 0.2007 - acc: 0.909 - ETA: 0s - loss: 0.2020 - acc: 0.904 - ETA: 0s - loss: 0.2003 - acc: 0.907 - ETA: 0s - loss: 0.2020 - acc: 0.906 - ETA: 0s - loss: 0.1989 - acc: 0.907 - ETA: 0s - loss: 0.2034 - acc: 0.905 - ETA: 0s - loss: 0.2070 - acc: 0.905 - ETA: 0s - loss: 0.2092 - acc: 0.903 - ETA: 0s - loss: 0.2071 - acc: 0.904 - ETA: 0s - loss: 0.2072 - acc: 0.904 - ETA: 0s - loss: 0.2063 - acc: 0.904 - ETA: 0s - loss: 0.2053 - acc: 0.905 - ETA: 0s - loss: 0.2061 - acc: 0.904 - ETA: 0s - loss: 0.2052 - acc: 0.905 - ETA: 0s - loss: 0.2062 - acc: 0.904 - ETA: 0s - loss: 0.2066 - acc: 0.903 - 1s 50us/step - loss: 0.2074 - acc: 0.9033 - val_loss: 0.6899 - val_acc: 0.8271\n",
      "Epoch 206/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.1271 - acc: 0.968 - ETA: 0s - loss: 0.1939 - acc: 0.922 - ETA: 0s - loss: 0.1884 - acc: 0.913 - ETA: 0s - loss: 0.1969 - acc: 0.909 - ETA: 0s - loss: 0.1932 - acc: 0.912 - ETA: 0s - loss: 0.2011 - acc: 0.907 - ETA: 0s - loss: 0.2036 - acc: 0.907 - ETA: 0s - loss: 0.2060 - acc: 0.906 - ETA: 0s - loss: 0.2048 - acc: 0.906 - ETA: 0s - loss: 0.2050 - acc: 0.906 - ETA: 0s - loss: 0.2029 - acc: 0.907 - ETA: 0s - loss: 0.2038 - acc: 0.907 - ETA: 0s - loss: 0.2019 - acc: 0.907 - ETA: 0s - loss: 0.2012 - acc: 0.907 - ETA: 0s - loss: 0.2024 - acc: 0.907 - ETA: 0s - loss: 0.2024 - acc: 0.906 - ETA: 0s - loss: 0.2032 - acc: 0.906 - ETA: 0s - loss: 0.2049 - acc: 0.905 - 1s 49us/step - loss: 0.2053 - acc: 0.9050 - val_loss: 0.7374 - val_acc: 0.8240\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 207/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.1907 - acc: 0.921 - ETA: 0s - loss: 0.2212 - acc: 0.901 - ETA: 0s - loss: 0.2081 - acc: 0.904 - ETA: 0s - loss: 0.2064 - acc: 0.903 - ETA: 0s - loss: 0.2037 - acc: 0.907 - ETA: 0s - loss: 0.2028 - acc: 0.908 - ETA: 0s - loss: 0.2075 - acc: 0.907 - ETA: 0s - loss: 0.2082 - acc: 0.906 - ETA: 0s - loss: 0.2076 - acc: 0.906 - ETA: 0s - loss: 0.2099 - acc: 0.905 - ETA: 0s - loss: 0.2100 - acc: 0.905 - ETA: 0s - loss: 0.2115 - acc: 0.905 - ETA: 0s - loss: 0.2082 - acc: 0.906 - ETA: 0s - loss: 0.2082 - acc: 0.905 - ETA: 0s - loss: 0.2084 - acc: 0.904 - ETA: 0s - loss: 0.2069 - acc: 0.905 - ETA: 0s - loss: 0.2074 - acc: 0.905 - ETA: 0s - loss: 0.2067 - acc: 0.905 - 1s 51us/step - loss: 0.2073 - acc: 0.9051 - val_loss: 0.7292 - val_acc: 0.8238\n",
      "Epoch 208/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1343 - acc: 0.953 - ETA: 0s - loss: 0.2070 - acc: 0.908 - ETA: 0s - loss: 0.2107 - acc: 0.907 - ETA: 0s - loss: 0.2136 - acc: 0.901 - ETA: 0s - loss: 0.2120 - acc: 0.901 - ETA: 0s - loss: 0.2114 - acc: 0.905 - ETA: 0s - loss: 0.2089 - acc: 0.905 - ETA: 0s - loss: 0.2068 - acc: 0.906 - ETA: 0s - loss: 0.2071 - acc: 0.906 - ETA: 0s - loss: 0.2054 - acc: 0.907 - ETA: 0s - loss: 0.2049 - acc: 0.908 - ETA: 0s - loss: 0.2056 - acc: 0.908 - ETA: 0s - loss: 0.2040 - acc: 0.909 - ETA: 0s - loss: 0.2036 - acc: 0.909 - ETA: 0s - loss: 0.2043 - acc: 0.909 - ETA: 0s - loss: 0.2054 - acc: 0.907 - ETA: 0s - loss: 0.2059 - acc: 0.907 - ETA: 0s - loss: 0.2068 - acc: 0.906 - 1s 50us/step - loss: 0.2062 - acc: 0.9068 - val_loss: 0.7176 - val_acc: 0.8261\n",
      "Epoch 209/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.2188 - acc: 0.875 - ETA: 0s - loss: 0.2151 - acc: 0.907 - ETA: 0s - loss: 0.2149 - acc: 0.903 - ETA: 0s - loss: 0.2143 - acc: 0.901 - ETA: 0s - loss: 0.2163 - acc: 0.901 - ETA: 0s - loss: 0.2163 - acc: 0.899 - ETA: 0s - loss: 0.2162 - acc: 0.901 - ETA: 0s - loss: 0.2142 - acc: 0.902 - ETA: 0s - loss: 0.2148 - acc: 0.902 - ETA: 0s - loss: 0.2136 - acc: 0.903 - ETA: 0s - loss: 0.2122 - acc: 0.903 - ETA: 0s - loss: 0.2109 - acc: 0.903 - ETA: 0s - loss: 0.2102 - acc: 0.903 - ETA: 0s - loss: 0.2107 - acc: 0.903 - ETA: 0s - loss: 0.2099 - acc: 0.904 - ETA: 0s - loss: 0.2111 - acc: 0.903 - ETA: 0s - loss: 0.2095 - acc: 0.904 - ETA: 0s - loss: 0.2101 - acc: 0.904 - 1s 50us/step - loss: 0.2097 - acc: 0.9043 - val_loss: 0.6873 - val_acc: 0.8330\n",
      "Epoch 210/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1355 - acc: 0.953 - ETA: 0s - loss: 0.2126 - acc: 0.903 - ETA: 0s - loss: 0.2056 - acc: 0.907 - ETA: 0s - loss: 0.2069 - acc: 0.904 - ETA: 0s - loss: 0.2014 - acc: 0.908 - ETA: 0s - loss: 0.2025 - acc: 0.907 - ETA: 0s - loss: 0.2033 - acc: 0.905 - ETA: 0s - loss: 0.2036 - acc: 0.905 - ETA: 0s - loss: 0.2019 - acc: 0.906 - ETA: 0s - loss: 0.2002 - acc: 0.907 - ETA: 0s - loss: 0.2012 - acc: 0.907 - ETA: 0s - loss: 0.2048 - acc: 0.906 - ETA: 0s - loss: 0.2029 - acc: 0.908 - ETA: 0s - loss: 0.2035 - acc: 0.907 - ETA: 0s - loss: 0.2048 - acc: 0.906 - ETA: 0s - loss: 0.2053 - acc: 0.906 - ETA: 0s - loss: 0.2052 - acc: 0.906 - ETA: 0s - loss: 0.2076 - acc: 0.906 - ETA: 0s - loss: 0.2062 - acc: 0.907 - 1s 53us/step - loss: 0.2061 - acc: 0.9068 - val_loss: 0.6957 - val_acc: 0.8214\n",
      "Epoch 211/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.2844 - acc: 0.875 - ETA: 0s - loss: 0.1933 - acc: 0.907 - ETA: 0s - loss: 0.1929 - acc: 0.902 - ETA: 0s - loss: 0.1983 - acc: 0.902 - ETA: 0s - loss: 0.1946 - acc: 0.905 - ETA: 0s - loss: 0.1964 - acc: 0.904 - ETA: 0s - loss: 0.2052 - acc: 0.904 - ETA: 0s - loss: 0.2076 - acc: 0.903 - ETA: 0s - loss: 0.2073 - acc: 0.903 - ETA: 0s - loss: 0.2084 - acc: 0.903 - ETA: 0s - loss: 0.2075 - acc: 0.903 - ETA: 0s - loss: 0.2072 - acc: 0.905 - ETA: 0s - loss: 0.2091 - acc: 0.905 - ETA: 0s - loss: 0.2074 - acc: 0.905 - ETA: 0s - loss: 0.2080 - acc: 0.905 - ETA: 0s - loss: 0.2070 - acc: 0.905 - ETA: 0s - loss: 0.2078 - acc: 0.905 - ETA: 0s - loss: 0.2093 - acc: 0.904 - 1s 50us/step - loss: 0.2082 - acc: 0.9045 - val_loss: 0.6984 - val_acc: 0.8270\n",
      "Epoch 212/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1715 - acc: 0.937 - ETA: 0s - loss: 0.1980 - acc: 0.910 - ETA: 0s - loss: 0.2087 - acc: 0.910 - ETA: 0s - loss: 0.1977 - acc: 0.914 - ETA: 0s - loss: 0.2009 - acc: 0.910 - ETA: 0s - loss: 0.1956 - acc: 0.912 - ETA: 0s - loss: 0.1974 - acc: 0.911 - ETA: 0s - loss: 0.1933 - acc: 0.914 - ETA: 0s - loss: 0.1952 - acc: 0.913 - ETA: 0s - loss: 0.1986 - acc: 0.912 - ETA: 0s - loss: 0.2003 - acc: 0.910 - ETA: 0s - loss: 0.2029 - acc: 0.909 - ETA: 0s - loss: 0.2049 - acc: 0.907 - ETA: 0s - loss: 0.2042 - acc: 0.908 - ETA: 0s - loss: 0.2046 - acc: 0.908 - ETA: 0s - loss: 0.2039 - acc: 0.908 - ETA: 0s - loss: 0.2051 - acc: 0.907 - ETA: 0s - loss: 0.2068 - acc: 0.906 - 1s 50us/step - loss: 0.2078 - acc: 0.9059 - val_loss: 0.6801 - val_acc: 0.8268\n",
      "Epoch 213/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.1651 - acc: 0.921 - ETA: 0s - loss: 0.1762 - acc: 0.925 - ETA: 0s - loss: 0.1879 - acc: 0.919 - ETA: 0s - loss: 0.1907 - acc: 0.914 - ETA: 0s - loss: 0.1942 - acc: 0.912 - ETA: 0s - loss: 0.1958 - acc: 0.911 - ETA: 0s - loss: 0.1963 - acc: 0.910 - ETA: 0s - loss: 0.1978 - acc: 0.909 - ETA: 0s - loss: 0.1991 - acc: 0.908 - ETA: 0s - loss: 0.2022 - acc: 0.906 - ETA: 0s - loss: 0.2068 - acc: 0.904 - ETA: 0s - loss: 0.2089 - acc: 0.904 - ETA: 0s - loss: 0.2089 - acc: 0.905 - ETA: 0s - loss: 0.2088 - acc: 0.904 - ETA: 0s - loss: 0.2081 - acc: 0.904 - ETA: 0s - loss: 0.2099 - acc: 0.904 - ETA: 0s - loss: 0.2086 - acc: 0.904 - ETA: 0s - loss: 0.2081 - acc: 0.904 - 1s 49us/step - loss: 0.2078 - acc: 0.9052 - val_loss: 0.7207 - val_acc: 0.8259\n",
      "Epoch 214/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.1750 - acc: 0.859 - ETA: 0s - loss: 0.1742 - acc: 0.915 - ETA: 0s - loss: 0.1869 - acc: 0.912 - ETA: 0s - loss: 0.1883 - acc: 0.910 - ETA: 0s - loss: 0.1992 - acc: 0.906 - ETA: 0s - loss: 0.2027 - acc: 0.907 - ETA: 0s - loss: 0.2033 - acc: 0.908 - ETA: 0s - loss: 0.2016 - acc: 0.909 - ETA: 0s - loss: 0.2014 - acc: 0.908 - ETA: 0s - loss: 0.1997 - acc: 0.909 - ETA: 0s - loss: 0.2000 - acc: 0.908 - ETA: 0s - loss: 0.2028 - acc: 0.907 - ETA: 0s - loss: 0.2020 - acc: 0.907 - ETA: 0s - loss: 0.2034 - acc: 0.907 - ETA: 0s - loss: 0.2046 - acc: 0.907 - ETA: 0s - loss: 0.2041 - acc: 0.907 - ETA: 0s - loss: 0.2070 - acc: 0.905 - ETA: 0s - loss: 0.2064 - acc: 0.905 - 1s 50us/step - loss: 0.2064 - acc: 0.9055 - val_loss: 0.7267 - val_acc: 0.8222\n",
      "Epoch 215/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.2762 - acc: 0.937 - ETA: 0s - loss: 0.2044 - acc: 0.917 - ETA: 0s - loss: 0.2007 - acc: 0.914 - ETA: 0s - loss: 0.1990 - acc: 0.913 - ETA: 0s - loss: 0.1929 - acc: 0.914 - ETA: 0s - loss: 0.1954 - acc: 0.912 - ETA: 0s - loss: 0.1958 - acc: 0.911 - ETA: 0s - loss: 0.1969 - acc: 0.911 - ETA: 0s - loss: 0.2016 - acc: 0.909 - ETA: 0s - loss: 0.2023 - acc: 0.909 - ETA: 0s - loss: 0.2025 - acc: 0.908 - ETA: 0s - loss: 0.2030 - acc: 0.907 - ETA: 0s - loss: 0.2037 - acc: 0.906 - ETA: 0s - loss: 0.2049 - acc: 0.906 - ETA: 0s - loss: 0.2049 - acc: 0.905 - ETA: 0s - loss: 0.2055 - acc: 0.905 - ETA: 0s - loss: 0.2059 - acc: 0.904 - ETA: 0s - loss: 0.2056 - acc: 0.905 - 1s 50us/step - loss: 0.2066 - acc: 0.9046 - val_loss: 0.7152 - val_acc: 0.8252\n",
      "Epoch 216/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.1669 - acc: 0.968 - ETA: 0s - loss: 0.1860 - acc: 0.920 - ETA: 0s - loss: 0.1930 - acc: 0.910 - ETA: 0s - loss: 0.2023 - acc: 0.906 - ETA: 0s - loss: 0.2059 - acc: 0.907 - ETA: 0s - loss: 0.2028 - acc: 0.906 - ETA: 0s - loss: 0.2022 - acc: 0.907 - ETA: 0s - loss: 0.2031 - acc: 0.906 - ETA: 0s - loss: 0.2055 - acc: 0.906 - ETA: 0s - loss: 0.2051 - acc: 0.907 - ETA: 0s - loss: 0.2047 - acc: 0.906 - ETA: 0s - loss: 0.2058 - acc: 0.906 - ETA: 0s - loss: 0.2042 - acc: 0.906 - ETA: 0s - loss: 0.2046 - acc: 0.906 - ETA: 0s - loss: 0.2060 - acc: 0.906 - ETA: 0s - loss: 0.2056 - acc: 0.906 - ETA: 0s - loss: 0.2053 - acc: 0.906 - ETA: 0s - loss: 0.2065 - acc: 0.906 - 1s 51us/step - loss: 0.2067 - acc: 0.9055 - val_loss: 0.7079 - val_acc: 0.8283\n",
      "Epoch 217/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.1788 - acc: 0.890 - ETA: 0s - loss: 0.1850 - acc: 0.908 - ETA: 0s - loss: 0.1890 - acc: 0.909 - ETA: 0s - loss: 0.1907 - acc: 0.908 - ETA: 0s - loss: 0.1926 - acc: 0.907 - ETA: 0s - loss: 0.1960 - acc: 0.906 - ETA: 0s - loss: 0.2000 - acc: 0.906 - ETA: 0s - loss: 0.1976 - acc: 0.908 - ETA: 0s - loss: 0.1988 - acc: 0.909 - ETA: 0s - loss: 0.1978 - acc: 0.909 - ETA: 0s - loss: 0.1993 - acc: 0.909 - ETA: 0s - loss: 0.2010 - acc: 0.908 - ETA: 0s - loss: 0.2015 - acc: 0.908 - ETA: 0s - loss: 0.2021 - acc: 0.908 - ETA: 0s - loss: 0.2029 - acc: 0.907 - ETA: 0s - loss: 0.2028 - acc: 0.907 - ETA: 0s - loss: 0.2032 - acc: 0.907 - ETA: 0s - loss: 0.2038 - acc: 0.906 - 1s 50us/step - loss: 0.2043 - acc: 0.9066 - val_loss: 0.6934 - val_acc: 0.8259\n",
      "Epoch 218/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1569 - acc: 0.937 - ETA: 0s - loss: 0.1981 - acc: 0.904 - ETA: 0s - loss: 0.2109 - acc: 0.903 - ETA: 0s - loss: 0.2059 - acc: 0.905 - ETA: 0s - loss: 0.2034 - acc: 0.906 - ETA: 0s - loss: 0.2039 - acc: 0.907 - ETA: 0s - loss: 0.2067 - acc: 0.907 - ETA: 0s - loss: 0.2070 - acc: 0.906 - ETA: 0s - loss: 0.2078 - acc: 0.905 - ETA: 0s - loss: 0.2096 - acc: 0.903 - ETA: 0s - loss: 0.2088 - acc: 0.903 - ETA: 0s - loss: 0.2074 - acc: 0.903 - ETA: 0s - loss: 0.2061 - acc: 0.904 - ETA: 0s - loss: 0.2065 - acc: 0.906 - ETA: 0s - loss: 0.2071 - acc: 0.905 - ETA: 0s - loss: 0.2083 - acc: 0.906 - ETA: 0s - loss: 0.2064 - acc: 0.907 - ETA: 0s - loss: 0.2059 - acc: 0.907 - 1s 50us/step - loss: 0.2063 - acc: 0.9066 - val_loss: 0.7084 - val_acc: 0.8264\n",
      "Epoch 219/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.2115 - acc: 0.875 - ETA: 0s - loss: 0.2026 - acc: 0.905 - ETA: 0s - loss: 0.1969 - acc: 0.910 - ETA: 0s - loss: 0.1999 - acc: 0.908 - ETA: 0s - loss: 0.2037 - acc: 0.906 - ETA: 0s - loss: 0.2025 - acc: 0.907 - ETA: 0s - loss: 0.2005 - acc: 0.907 - ETA: 0s - loss: 0.2022 - acc: 0.908 - ETA: 0s - loss: 0.2046 - acc: 0.906 - ETA: 0s - loss: 0.2058 - acc: 0.906 - ETA: 0s - loss: 0.2026 - acc: 0.907 - ETA: 0s - loss: 0.2029 - acc: 0.908 - ETA: 0s - loss: 0.2035 - acc: 0.907 - ETA: 0s - loss: 0.2049 - acc: 0.908 - ETA: 0s - loss: 0.2044 - acc: 0.908 - ETA: 0s - loss: 0.2072 - acc: 0.908 - ETA: 0s - loss: 0.2079 - acc: 0.906 - ETA: 0s - loss: 0.2092 - acc: 0.905 - 1s 50us/step - loss: 0.2091 - acc: 0.9060 - val_loss: 0.7146 - val_acc: 0.8261\n",
      "Epoch 220/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.0993 - acc: 0.968 - ETA: 0s - loss: 0.1669 - acc: 0.925 - ETA: 0s - loss: 0.1846 - acc: 0.914 - ETA: 0s - loss: 0.1906 - acc: 0.912 - ETA: 0s - loss: 0.2003 - acc: 0.909 - ETA: 0s - loss: 0.2010 - acc: 0.909 - ETA: 0s - loss: 0.2030 - acc: 0.910 - ETA: 0s - loss: 0.2018 - acc: 0.910 - ETA: 0s - loss: 0.2027 - acc: 0.910 - ETA: 0s - loss: 0.2038 - acc: 0.909 - ETA: 0s - loss: 0.2072 - acc: 0.907 - ETA: 0s - loss: 0.2087 - acc: 0.905 - ETA: 0s - loss: 0.2092 - acc: 0.904 - ETA: 0s - loss: 0.2070 - acc: 0.905 - ETA: 0s - loss: 0.2062 - acc: 0.906 - ETA: 0s - loss: 0.2053 - acc: 0.906 - ETA: 0s - loss: 0.2065 - acc: 0.905 - ETA: 0s - loss: 0.2055 - acc: 0.905 - 1s 49us/step - loss: 0.2058 - acc: 0.9057 - val_loss: 0.7245 - val_acc: 0.8154\n",
      "Epoch 221/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.2319 - acc: 0.859 - ETA: 0s - loss: 0.1980 - acc: 0.903 - ETA: 0s - loss: 0.2214 - acc: 0.901 - ETA: 0s - loss: 0.2100 - acc: 0.904 - ETA: 0s - loss: 0.2157 - acc: 0.899 - ETA: 0s - loss: 0.2127 - acc: 0.900 - ETA: 0s - loss: 0.2092 - acc: 0.902 - ETA: 0s - loss: 0.2078 - acc: 0.904 - ETA: 0s - loss: 0.2076 - acc: 0.904 - ETA: 0s - loss: 0.2079 - acc: 0.903 - ETA: 0s - loss: 0.2052 - acc: 0.905 - ETA: 0s - loss: 0.2044 - acc: 0.907 - ETA: 0s - loss: 0.2047 - acc: 0.907 - ETA: 0s - loss: 0.2038 - acc: 0.907 - ETA: 0s - loss: 0.2064 - acc: 0.905 - ETA: 0s - loss: 0.2061 - acc: 0.905 - ETA: 0s - loss: 0.2053 - acc: 0.905 - ETA: 0s - loss: 0.2052 - acc: 0.905 - 1s 50us/step - loss: 0.2053 - acc: 0.9062 - val_loss: 0.7054 - val_acc: 0.8246\n",
      "Epoch 222/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1554 - acc: 0.953 - ETA: 0s - loss: 0.1928 - acc: 0.909 - ETA: 0s - loss: 0.1994 - acc: 0.908 - ETA: 0s - loss: 0.1981 - acc: 0.908 - ETA: 0s - loss: 0.2013 - acc: 0.908 - ETA: 0s - loss: 0.1999 - acc: 0.907 - ETA: 0s - loss: 0.1999 - acc: 0.906 - ETA: 0s - loss: 0.1980 - acc: 0.907 - ETA: 0s - loss: 0.1981 - acc: 0.907 - ETA: 0s - loss: 0.1950 - acc: 0.909 - ETA: 0s - loss: 0.1971 - acc: 0.907 - ETA: 0s - loss: 0.1976 - acc: 0.907 - ETA: 0s - loss: 0.1989 - acc: 0.906 - ETA: 0s - loss: 0.2002 - acc: 0.905 - ETA: 0s - loss: 0.2013 - acc: 0.905 - ETA: 0s - loss: 0.2026 - acc: 0.905 - ETA: 0s - loss: 0.2034 - acc: 0.905 - ETA: 0s - loss: 0.2039 - acc: 0.905 - 1s 50us/step - loss: 0.2047 - acc: 0.9056 - val_loss: 0.7154 - val_acc: 0.8271\n",
      "Epoch 223/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.2174 - acc: 0.859 - ETA: 0s - loss: 0.2098 - acc: 0.912 - ETA: 0s - loss: 0.2075 - acc: 0.904 - ETA: 0s - loss: 0.2119 - acc: 0.902 - ETA: 0s - loss: 0.2074 - acc: 0.902 - ETA: 0s - loss: 0.2081 - acc: 0.902 - ETA: 0s - loss: 0.2057 - acc: 0.904 - ETA: 0s - loss: 0.2055 - acc: 0.905 - ETA: 0s - loss: 0.2035 - acc: 0.907 - ETA: 0s - loss: 0.2014 - acc: 0.907 - ETA: 0s - loss: 0.2008 - acc: 0.908 - ETA: 0s - loss: 0.2017 - acc: 0.907 - ETA: 0s - loss: 0.2016 - acc: 0.908 - ETA: 0s - loss: 0.2014 - acc: 0.908 - ETA: 0s - loss: 0.2051 - acc: 0.906 - ETA: 0s - loss: 0.2049 - acc: 0.906 - ETA: 0s - loss: 0.2057 - acc: 0.906 - 1s 48us/step - loss: 0.2055 - acc: 0.9062 - val_loss: 0.7099 - val_acc: 0.8284\n",
      "Epoch 224/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.2341 - acc: 0.906 - ETA: 0s - loss: 0.1766 - acc: 0.928 - ETA: 0s - loss: 0.1937 - acc: 0.915 - ETA: 0s - loss: 0.1932 - acc: 0.913 - ETA: 0s - loss: 0.1934 - acc: 0.912 - ETA: 0s - loss: 0.1939 - acc: 0.911 - ETA: 0s - loss: 0.1965 - acc: 0.910 - ETA: 0s - loss: 0.1955 - acc: 0.910 - ETA: 0s - loss: 0.1957 - acc: 0.910 - ETA: 0s - loss: 0.1956 - acc: 0.910 - ETA: 0s - loss: 0.1950 - acc: 0.911 - ETA: 0s - loss: 0.1978 - acc: 0.909 - ETA: 0s - loss: 0.2009 - acc: 0.908 - ETA: 0s - loss: 0.2015 - acc: 0.907 - ETA: 0s - loss: 0.2022 - acc: 0.907 - ETA: 0s - loss: 0.2021 - acc: 0.907 - ETA: 0s - loss: 0.2033 - acc: 0.907 - ETA: 0s - loss: 0.2040 - acc: 0.907 - 1s 50us/step - loss: 0.2040 - acc: 0.9070 - val_loss: 0.7146 - val_acc: 0.8245\n",
      "Epoch 225/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.1850 - acc: 0.890 - ETA: 0s - loss: 0.2156 - acc: 0.907 - ETA: 0s - loss: 0.2021 - acc: 0.913 - ETA: 0s - loss: 0.2066 - acc: 0.913 - ETA: 0s - loss: 0.2085 - acc: 0.911 - ETA: 0s - loss: 0.2055 - acc: 0.912 - ETA: 0s - loss: 0.2047 - acc: 0.910 - ETA: 0s - loss: 0.2066 - acc: 0.908 - ETA: 0s - loss: 0.2042 - acc: 0.909 - ETA: 0s - loss: 0.2041 - acc: 0.909 - ETA: 0s - loss: 0.2038 - acc: 0.907 - ETA: 0s - loss: 0.2037 - acc: 0.907 - ETA: 0s - loss: 0.2027 - acc: 0.907 - ETA: 0s - loss: 0.2040 - acc: 0.906 - ETA: 0s - loss: 0.2017 - acc: 0.907 - ETA: 0s - loss: 0.2029 - acc: 0.907 - ETA: 0s - loss: 0.2034 - acc: 0.906 - ETA: 0s - loss: 0.2036 - acc: 0.906 - 1s 50us/step - loss: 0.2031 - acc: 0.9062 - val_loss: 0.7149 - val_acc: 0.8271\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 226/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.2376 - acc: 0.875 - ETA: 0s - loss: 0.1952 - acc: 0.909 - ETA: 0s - loss: 0.1902 - acc: 0.913 - ETA: 0s - loss: 0.1997 - acc: 0.906 - ETA: 0s - loss: 0.1976 - acc: 0.909 - ETA: 0s - loss: 0.1972 - acc: 0.908 - ETA: 0s - loss: 0.1983 - acc: 0.908 - ETA: 0s - loss: 0.2003 - acc: 0.906 - ETA: 0s - loss: 0.2010 - acc: 0.905 - ETA: 0s - loss: 0.2018 - acc: 0.905 - ETA: 0s - loss: 0.2017 - acc: 0.905 - ETA: 0s - loss: 0.2002 - acc: 0.906 - ETA: 0s - loss: 0.1994 - acc: 0.907 - ETA: 0s - loss: 0.2000 - acc: 0.906 - ETA: 0s - loss: 0.1999 - acc: 0.906 - ETA: 0s - loss: 0.2030 - acc: 0.905 - ETA: 0s - loss: 0.2027 - acc: 0.905 - ETA: 0s - loss: 0.2029 - acc: 0.905 - 1s 50us/step - loss: 0.2047 - acc: 0.9055 - val_loss: 0.7667 - val_acc: 0.8168\n",
      "Epoch 227/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.2040 - acc: 0.937 - ETA: 0s - loss: 0.1910 - acc: 0.917 - ETA: 0s - loss: 0.1947 - acc: 0.911 - ETA: 0s - loss: 0.1943 - acc: 0.913 - ETA: 0s - loss: 0.2028 - acc: 0.907 - ETA: 0s - loss: 0.1997 - acc: 0.907 - ETA: 0s - loss: 0.2002 - acc: 0.907 - ETA: 0s - loss: 0.2005 - acc: 0.907 - ETA: 0s - loss: 0.2012 - acc: 0.907 - ETA: 0s - loss: 0.1997 - acc: 0.907 - ETA: 0s - loss: 0.1980 - acc: 0.908 - ETA: 0s - loss: 0.1985 - acc: 0.908 - ETA: 0s - loss: 0.1974 - acc: 0.909 - ETA: 0s - loss: 0.1986 - acc: 0.909 - ETA: 0s - loss: 0.1999 - acc: 0.909 - ETA: 0s - loss: 0.2012 - acc: 0.909 - ETA: 0s - loss: 0.2017 - acc: 0.908 - ETA: 0s - loss: 0.2028 - acc: 0.908 - 1s 50us/step - loss: 0.2040 - acc: 0.9076 - val_loss: 0.7249 - val_acc: 0.8271\n",
      "Epoch 228/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.2850 - acc: 0.859 - ETA: 0s - loss: 0.1955 - acc: 0.908 - ETA: 0s - loss: 0.2077 - acc: 0.909 - ETA: 0s - loss: 0.2045 - acc: 0.908 - ETA: 0s - loss: 0.2084 - acc: 0.908 - ETA: 0s - loss: 0.2072 - acc: 0.907 - ETA: 0s - loss: 0.2055 - acc: 0.907 - ETA: 0s - loss: 0.2072 - acc: 0.908 - ETA: 0s - loss: 0.2084 - acc: 0.906 - ETA: 0s - loss: 0.2044 - acc: 0.908 - ETA: 0s - loss: 0.2042 - acc: 0.907 - ETA: 0s - loss: 0.2045 - acc: 0.907 - ETA: 0s - loss: 0.2036 - acc: 0.907 - ETA: 0s - loss: 0.2035 - acc: 0.907 - ETA: 0s - loss: 0.2040 - acc: 0.906 - ETA: 0s - loss: 0.2034 - acc: 0.906 - ETA: 0s - loss: 0.2043 - acc: 0.906 - ETA: 0s - loss: 0.2042 - acc: 0.906 - 1s 49us/step - loss: 0.2039 - acc: 0.9068 - val_loss: 0.7258 - val_acc: 0.8288\n",
      "Epoch 229/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.2675 - acc: 0.843 - ETA: 0s - loss: 0.1760 - acc: 0.914 - ETA: 0s - loss: 0.1821 - acc: 0.913 - ETA: 0s - loss: 0.1849 - acc: 0.910 - ETA: 0s - loss: 0.1897 - acc: 0.910 - ETA: 0s - loss: 0.1931 - acc: 0.908 - ETA: 0s - loss: 0.1928 - acc: 0.907 - ETA: 0s - loss: 0.1929 - acc: 0.907 - ETA: 0s - loss: 0.1951 - acc: 0.907 - ETA: 0s - loss: 0.1965 - acc: 0.907 - ETA: 0s - loss: 0.2008 - acc: 0.905 - ETA: 0s - loss: 0.1997 - acc: 0.906 - ETA: 0s - loss: 0.2030 - acc: 0.905 - ETA: 0s - loss: 0.2033 - acc: 0.905 - ETA: 0s - loss: 0.2046 - acc: 0.906 - ETA: 0s - loss: 0.2052 - acc: 0.905 - ETA: 0s - loss: 0.2047 - acc: 0.905 - ETA: 0s - loss: 0.2041 - acc: 0.906 - 1s 49us/step - loss: 0.2035 - acc: 0.9066 - val_loss: 0.7476 - val_acc: 0.8265\n",
      "Epoch 230/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1173 - acc: 0.968 - ETA: 0s - loss: 0.1905 - acc: 0.903 - ETA: 0s - loss: 0.1902 - acc: 0.909 - ETA: 0s - loss: 0.1925 - acc: 0.911 - ETA: 0s - loss: 0.2034 - acc: 0.907 - ETA: 0s - loss: 0.1985 - acc: 0.909 - ETA: 0s - loss: 0.1987 - acc: 0.909 - ETA: 0s - loss: 0.1982 - acc: 0.910 - ETA: 0s - loss: 0.1975 - acc: 0.910 - ETA: 0s - loss: 0.1994 - acc: 0.909 - ETA: 0s - loss: 0.1984 - acc: 0.910 - ETA: 0s - loss: 0.1989 - acc: 0.909 - ETA: 0s - loss: 0.2006 - acc: 0.909 - ETA: 0s - loss: 0.2011 - acc: 0.908 - ETA: 0s - loss: 0.2031 - acc: 0.907 - ETA: 0s - loss: 0.2048 - acc: 0.906 - ETA: 0s - loss: 0.2039 - acc: 0.907 - ETA: 0s - loss: 0.2051 - acc: 0.907 - 1s 49us/step - loss: 0.2051 - acc: 0.9070 - val_loss: 0.7136 - val_acc: 0.8263\n",
      "Epoch 231/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1934 - acc: 0.937 - ETA: 0s - loss: 0.1899 - acc: 0.921 - ETA: 0s - loss: 0.1908 - acc: 0.917 - ETA: 0s - loss: 0.1947 - acc: 0.914 - ETA: 0s - loss: 0.1915 - acc: 0.914 - ETA: 0s - loss: 0.1956 - acc: 0.912 - ETA: 0s - loss: 0.2009 - acc: 0.909 - ETA: 0s - loss: 0.2031 - acc: 0.908 - ETA: 0s - loss: 0.2038 - acc: 0.908 - ETA: 0s - loss: 0.2041 - acc: 0.908 - ETA: 0s - loss: 0.2029 - acc: 0.908 - ETA: 0s - loss: 0.2049 - acc: 0.908 - ETA: 0s - loss: 0.2058 - acc: 0.908 - ETA: 0s - loss: 0.2096 - acc: 0.907 - ETA: 0s - loss: 0.2079 - acc: 0.907 - ETA: 0s - loss: 0.2060 - acc: 0.908 - ETA: 0s - loss: 0.2058 - acc: 0.908 - ETA: 0s - loss: 0.2050 - acc: 0.908 - 1s 49us/step - loss: 0.2045 - acc: 0.9082 - val_loss: 0.7195 - val_acc: 0.8310\n",
      "Epoch 232/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1564 - acc: 0.921 - ETA: 0s - loss: 0.1888 - acc: 0.910 - ETA: 0s - loss: 0.2121 - acc: 0.904 - ETA: 0s - loss: 0.2083 - acc: 0.906 - ETA: 0s - loss: 0.2051 - acc: 0.908 - ETA: 0s - loss: 0.2095 - acc: 0.907 - ETA: 0s - loss: 0.2062 - acc: 0.908 - ETA: 0s - loss: 0.2059 - acc: 0.907 - ETA: 0s - loss: 0.2033 - acc: 0.909 - ETA: 0s - loss: 0.2048 - acc: 0.908 - ETA: 0s - loss: 0.2022 - acc: 0.909 - ETA: 0s - loss: 0.2031 - acc: 0.909 - ETA: 0s - loss: 0.2027 - acc: 0.909 - ETA: 0s - loss: 0.2042 - acc: 0.908 - ETA: 0s - loss: 0.2043 - acc: 0.908 - ETA: 0s - loss: 0.2041 - acc: 0.908 - ETA: 0s - loss: 0.2044 - acc: 0.907 - ETA: 0s - loss: 0.2043 - acc: 0.907 - 1s 50us/step - loss: 0.2043 - acc: 0.9072 - val_loss: 0.7287 - val_acc: 0.8253\n",
      "Epoch 233/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.1430 - acc: 0.953 - ETA: 0s - loss: 0.1957 - acc: 0.915 - ETA: 0s - loss: 0.1974 - acc: 0.911 - ETA: 0s - loss: 0.1879 - acc: 0.915 - ETA: 0s - loss: 0.2032 - acc: 0.911 - ETA: 0s - loss: 0.1985 - acc: 0.913 - ETA: 0s - loss: 0.2004 - acc: 0.913 - ETA: 0s - loss: 0.1992 - acc: 0.912 - ETA: 0s - loss: 0.1999 - acc: 0.911 - ETA: 0s - loss: 0.1994 - acc: 0.910 - ETA: 0s - loss: 0.1985 - acc: 0.911 - ETA: 0s - loss: 0.1994 - acc: 0.911 - ETA: 0s - loss: 0.1989 - acc: 0.912 - ETA: 0s - loss: 0.2013 - acc: 0.910 - ETA: 0s - loss: 0.2020 - acc: 0.910 - ETA: 0s - loss: 0.2011 - acc: 0.910 - ETA: 0s - loss: 0.2024 - acc: 0.908 - ETA: 0s - loss: 0.2036 - acc: 0.907 - 1s 49us/step - loss: 0.2040 - acc: 0.9076 - val_loss: 0.6998 - val_acc: 0.8281\n",
      "Epoch 234/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.2310 - acc: 0.890 - ETA: 0s - loss: 0.1949 - acc: 0.905 - ETA: 0s - loss: 0.2017 - acc: 0.904 - ETA: 0s - loss: 0.1964 - acc: 0.909 - ETA: 0s - loss: 0.1999 - acc: 0.906 - ETA: 0s - loss: 0.1979 - acc: 0.905 - ETA: 0s - loss: 0.1979 - acc: 0.906 - ETA: 0s - loss: 0.1992 - acc: 0.904 - ETA: 0s - loss: 0.1989 - acc: 0.906 - ETA: 0s - loss: 0.2007 - acc: 0.905 - ETA: 0s - loss: 0.2020 - acc: 0.905 - ETA: 0s - loss: 0.2044 - acc: 0.904 - ETA: 0s - loss: 0.2029 - acc: 0.905 - ETA: 0s - loss: 0.2033 - acc: 0.905 - ETA: 0s - loss: 0.2032 - acc: 0.907 - ETA: 0s - loss: 0.2035 - acc: 0.907 - ETA: 0s - loss: 0.2053 - acc: 0.906 - ETA: 0s - loss: 0.2068 - acc: 0.905 - 1s 49us/step - loss: 0.2065 - acc: 0.9057 - val_loss: 0.7605 - val_acc: 0.8252\n",
      "Epoch 235/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1195 - acc: 0.953 - ETA: 0s - loss: 0.1949 - acc: 0.913 - ETA: 0s - loss: 0.1874 - acc: 0.912 - ETA: 0s - loss: 0.1856 - acc: 0.913 - ETA: 0s - loss: 0.1925 - acc: 0.912 - ETA: 0s - loss: 0.1915 - acc: 0.914 - ETA: 0s - loss: 0.1977 - acc: 0.911 - ETA: 0s - loss: 0.1973 - acc: 0.911 - ETA: 0s - loss: 0.2017 - acc: 0.910 - ETA: 0s - loss: 0.2036 - acc: 0.910 - ETA: 0s - loss: 0.2015 - acc: 0.911 - ETA: 0s - loss: 0.2009 - acc: 0.910 - ETA: 0s - loss: 0.2018 - acc: 0.910 - ETA: 0s - loss: 0.2018 - acc: 0.910 - ETA: 0s - loss: 0.2032 - acc: 0.909 - ETA: 0s - loss: 0.2045 - acc: 0.908 - ETA: 0s - loss: 0.2047 - acc: 0.908 - ETA: 0s - loss: 0.2046 - acc: 0.908 - 1s 49us/step - loss: 0.2040 - acc: 0.9081 - val_loss: 0.7272 - val_acc: 0.8301\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 236/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1664 - acc: 0.953 - ETA: 0s - loss: 0.1978 - acc: 0.914 - ETA: 0s - loss: 0.1962 - acc: 0.912 - ETA: 0s - loss: 0.1926 - acc: 0.912 - ETA: 0s - loss: 0.1950 - acc: 0.911 - ETA: 0s - loss: 0.2005 - acc: 0.910 - ETA: 0s - loss: 0.2000 - acc: 0.909 - ETA: 0s - loss: 0.2028 - acc: 0.908 - ETA: 0s - loss: 0.2017 - acc: 0.908 - ETA: 0s - loss: 0.2051 - acc: 0.907 - ETA: 0s - loss: 0.2020 - acc: 0.909 - ETA: 0s - loss: 0.2016 - acc: 0.909 - ETA: 0s - loss: 0.2039 - acc: 0.909 - ETA: 0s - loss: 0.2045 - acc: 0.909 - ETA: 0s - loss: 0.2045 - acc: 0.908 - ETA: 0s - loss: 0.2049 - acc: 0.908 - ETA: 0s - loss: 0.2054 - acc: 0.907 - ETA: 0s - loss: 0.2063 - acc: 0.907 - 1s 49us/step - loss: 0.2063 - acc: 0.9081 - val_loss: 0.6914 - val_acc: 0.8322\n",
      "Epoch 237/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1946 - acc: 0.859 - ETA: 0s - loss: 0.1824 - acc: 0.914 - ETA: 0s - loss: 0.1831 - acc: 0.916 - ETA: 0s - loss: 0.1855 - acc: 0.914 - ETA: 0s - loss: 0.1878 - acc: 0.912 - ETA: 0s - loss: 0.1891 - acc: 0.912 - ETA: 0s - loss: 0.1936 - acc: 0.912 - ETA: 0s - loss: 0.1937 - acc: 0.911 - ETA: 0s - loss: 0.1934 - acc: 0.912 - ETA: 0s - loss: 0.1925 - acc: 0.912 - ETA: 0s - loss: 0.1938 - acc: 0.911 - ETA: 0s - loss: 0.1952 - acc: 0.911 - ETA: 0s - loss: 0.1972 - acc: 0.910 - ETA: 0s - loss: 0.1976 - acc: 0.909 - ETA: 0s - loss: 0.1984 - acc: 0.909 - ETA: 0s - loss: 0.1996 - acc: 0.908 - ETA: 0s - loss: 0.2011 - acc: 0.907 - ETA: 0s - loss: 0.2018 - acc: 0.906 - 1s 50us/step - loss: 0.2015 - acc: 0.9069 - val_loss: 0.7345 - val_acc: 0.8292\n",
      "Epoch 238/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.1877 - acc: 0.906 - ETA: 0s - loss: 0.1754 - acc: 0.921 - ETA: 0s - loss: 0.1881 - acc: 0.914 - ETA: 0s - loss: 0.1931 - acc: 0.910 - ETA: 0s - loss: 0.1966 - acc: 0.909 - ETA: 0s - loss: 0.1956 - acc: 0.911 - ETA: 0s - loss: 0.1926 - acc: 0.913 - ETA: 0s - loss: 0.1936 - acc: 0.912 - ETA: 0s - loss: 0.1996 - acc: 0.909 - ETA: 0s - loss: 0.1990 - acc: 0.909 - ETA: 0s - loss: 0.2013 - acc: 0.909 - ETA: 0s - loss: 0.2003 - acc: 0.910 - ETA: 0s - loss: 0.2009 - acc: 0.910 - ETA: 0s - loss: 0.2020 - acc: 0.909 - ETA: 0s - loss: 0.2021 - acc: 0.909 - ETA: 0s - loss: 0.2032 - acc: 0.908 - ETA: 0s - loss: 0.2046 - acc: 0.907 - ETA: 0s - loss: 0.2044 - acc: 0.906 - 1s 49us/step - loss: 0.2036 - acc: 0.9073 - val_loss: 0.7433 - val_acc: 0.8308\n",
      "Epoch 239/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.2911 - acc: 0.875 - ETA: 0s - loss: 0.2200 - acc: 0.903 - ETA: 0s - loss: 0.1983 - acc: 0.906 - ETA: 0s - loss: 0.1945 - acc: 0.909 - ETA: 0s - loss: 0.1933 - acc: 0.910 - ETA: 0s - loss: 0.1951 - acc: 0.909 - ETA: 0s - loss: 0.1946 - acc: 0.909 - ETA: 0s - loss: 0.1953 - acc: 0.910 - ETA: 0s - loss: 0.1970 - acc: 0.910 - ETA: 0s - loss: 0.1960 - acc: 0.911 - ETA: 0s - loss: 0.1949 - acc: 0.911 - ETA: 0s - loss: 0.1967 - acc: 0.911 - ETA: 0s - loss: 0.1996 - acc: 0.909 - ETA: 0s - loss: 0.2006 - acc: 0.907 - ETA: 0s - loss: 0.2000 - acc: 0.908 - ETA: 0s - loss: 0.1986 - acc: 0.909 - ETA: 0s - loss: 0.2021 - acc: 0.907 - ETA: 0s - loss: 0.2022 - acc: 0.907 - 1s 49us/step - loss: 0.2022 - acc: 0.9080 - val_loss: 0.7161 - val_acc: 0.8334\n",
      "Epoch 240/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.1906 - acc: 0.921 - ETA: 0s - loss: 0.1962 - acc: 0.905 - ETA: 0s - loss: 0.2044 - acc: 0.902 - ETA: 0s - loss: 0.1977 - acc: 0.907 - ETA: 0s - loss: 0.1973 - acc: 0.908 - ETA: 0s - loss: 0.1953 - acc: 0.909 - ETA: 0s - loss: 0.1953 - acc: 0.910 - ETA: 0s - loss: 0.2043 - acc: 0.908 - ETA: 0s - loss: 0.2044 - acc: 0.907 - ETA: 0s - loss: 0.2043 - acc: 0.907 - ETA: 0s - loss: 0.2060 - acc: 0.906 - ETA: 0s - loss: 0.2071 - acc: 0.905 - ETA: 0s - loss: 0.2051 - acc: 0.907 - ETA: 0s - loss: 0.2030 - acc: 0.908 - ETA: 0s - loss: 0.2041 - acc: 0.908 - ETA: 0s - loss: 0.2047 - acc: 0.908 - ETA: 0s - loss: 0.2071 - acc: 0.907 - ETA: 0s - loss: 0.2067 - acc: 0.907 - 1s 50us/step - loss: 0.2058 - acc: 0.9075 - val_loss: 0.7223 - val_acc: 0.8288\n",
      "Epoch 241/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.1576 - acc: 0.921 - ETA: 0s - loss: 0.1792 - acc: 0.917 - ETA: 0s - loss: 0.1877 - acc: 0.911 - ETA: 0s - loss: 0.1981 - acc: 0.911 - ETA: 0s - loss: 0.2056 - acc: 0.908 - ETA: 0s - loss: 0.2032 - acc: 0.907 - ETA: 0s - loss: 0.2046 - acc: 0.907 - ETA: 0s - loss: 0.2071 - acc: 0.908 - ETA: 0s - loss: 0.2006 - acc: 0.911 - ETA: 0s - loss: 0.2011 - acc: 0.910 - ETA: 0s - loss: 0.1976 - acc: 0.911 - ETA: 0s - loss: 0.1988 - acc: 0.910 - ETA: 0s - loss: 0.2012 - acc: 0.909 - ETA: 0s - loss: 0.2030 - acc: 0.908 - ETA: 0s - loss: 0.1999 - acc: 0.910 - ETA: 0s - loss: 0.1995 - acc: 0.910 - ETA: 0s - loss: 0.2017 - acc: 0.909 - ETA: 0s - loss: 0.2021 - acc: 0.908 - 1s 50us/step - loss: 0.2015 - acc: 0.9086 - val_loss: 0.7010 - val_acc: 0.8383\n",
      "Epoch 242/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1063 - acc: 0.953 - ETA: 0s - loss: 0.1982 - acc: 0.898 - ETA: 0s - loss: 0.2033 - acc: 0.903 - ETA: 0s - loss: 0.2041 - acc: 0.904 - ETA: 0s - loss: 0.2049 - acc: 0.902 - ETA: 0s - loss: 0.2042 - acc: 0.905 - ETA: 0s - loss: 0.2079 - acc: 0.905 - ETA: 0s - loss: 0.2063 - acc: 0.906 - ETA: 0s - loss: 0.2059 - acc: 0.907 - ETA: 0s - loss: 0.2050 - acc: 0.907 - ETA: 0s - loss: 0.2054 - acc: 0.906 - ETA: 0s - loss: 0.2049 - acc: 0.906 - ETA: 0s - loss: 0.2041 - acc: 0.906 - ETA: 0s - loss: 0.2034 - acc: 0.907 - ETA: 0s - loss: 0.2046 - acc: 0.907 - ETA: 0s - loss: 0.2049 - acc: 0.907 - ETA: 0s - loss: 0.2045 - acc: 0.907 - ETA: 0s - loss: 0.2044 - acc: 0.908 - 1s 50us/step - loss: 0.2046 - acc: 0.9080 - val_loss: 0.7157 - val_acc: 0.8255\n",
      "Epoch 243/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1532 - acc: 0.937 - ETA: 0s - loss: 0.1788 - acc: 0.910 - ETA: 0s - loss: 0.1784 - acc: 0.908 - ETA: 0s - loss: 0.1864 - acc: 0.906 - ETA: 0s - loss: 0.1829 - acc: 0.913 - ETA: 0s - loss: 0.1871 - acc: 0.910 - ETA: 0s - loss: 0.1895 - acc: 0.907 - ETA: 0s - loss: 0.1892 - acc: 0.908 - ETA: 0s - loss: 0.1894 - acc: 0.909 - ETA: 0s - loss: 0.1929 - acc: 0.908 - ETA: 0s - loss: 0.1950 - acc: 0.907 - ETA: 0s - loss: 0.1998 - acc: 0.906 - ETA: 0s - loss: 0.2011 - acc: 0.905 - ETA: 0s - loss: 0.2020 - acc: 0.906 - ETA: 0s - loss: 0.2009 - acc: 0.906 - ETA: 0s - loss: 0.2013 - acc: 0.906 - ETA: 0s - loss: 0.2036 - acc: 0.906 - ETA: 0s - loss: 0.2039 - acc: 0.906 - 1s 50us/step - loss: 0.2034 - acc: 0.9065 - val_loss: 0.7356 - val_acc: 0.8259\n",
      "Epoch 244/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1805 - acc: 0.953 - ETA: 1s - loss: 0.1801 - acc: 0.924 - ETA: 0s - loss: 0.1911 - acc: 0.906 - ETA: 0s - loss: 0.1884 - acc: 0.911 - ETA: 0s - loss: 0.1890 - acc: 0.910 - ETA: 0s - loss: 0.1873 - acc: 0.911 - ETA: 0s - loss: 0.1905 - acc: 0.910 - ETA: 0s - loss: 0.1926 - acc: 0.909 - ETA: 0s - loss: 0.1955 - acc: 0.908 - ETA: 0s - loss: 0.1950 - acc: 0.909 - ETA: 0s - loss: 0.1992 - acc: 0.907 - ETA: 0s - loss: 0.1978 - acc: 0.908 - ETA: 0s - loss: 0.1971 - acc: 0.908 - ETA: 0s - loss: 0.1985 - acc: 0.909 - ETA: 0s - loss: 0.1983 - acc: 0.909 - ETA: 0s - loss: 0.1989 - acc: 0.908 - ETA: 0s - loss: 0.2016 - acc: 0.908 - ETA: 0s - loss: 0.2019 - acc: 0.908 - 1s 50us/step - loss: 0.2025 - acc: 0.9078 - val_loss: 0.7275 - val_acc: 0.8226\n",
      "Epoch 245/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.1715 - acc: 0.937 - ETA: 0s - loss: 0.1745 - acc: 0.916 - ETA: 0s - loss: 0.1843 - acc: 0.912 - ETA: 0s - loss: 0.1907 - acc: 0.910 - ETA: 0s - loss: 0.1939 - acc: 0.910 - ETA: 0s - loss: 0.1936 - acc: 0.911 - ETA: 0s - loss: 0.1969 - acc: 0.910 - ETA: 0s - loss: 0.1975 - acc: 0.910 - ETA: 0s - loss: 0.1997 - acc: 0.910 - ETA: 0s - loss: 0.2038 - acc: 0.909 - ETA: 0s - loss: 0.2032 - acc: 0.909 - ETA: 0s - loss: 0.2042 - acc: 0.909 - ETA: 0s - loss: 0.2042 - acc: 0.909 - ETA: 0s - loss: 0.2021 - acc: 0.909 - ETA: 0s - loss: 0.2019 - acc: 0.909 - ETA: 0s - loss: 0.2027 - acc: 0.909 - ETA: 0s - loss: 0.2025 - acc: 0.909 - ETA: 0s - loss: 0.2028 - acc: 0.909 - 1s 50us/step - loss: 0.2030 - acc: 0.9092 - val_loss: 0.7399 - val_acc: 0.8255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 246/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.2194 - acc: 0.906 - ETA: 0s - loss: 0.1957 - acc: 0.910 - ETA: 0s - loss: 0.2007 - acc: 0.912 - ETA: 0s - loss: 0.1900 - acc: 0.911 - ETA: 0s - loss: 0.1872 - acc: 0.913 - ETA: 0s - loss: 0.1907 - acc: 0.910 - ETA: 0s - loss: 0.1919 - acc: 0.910 - ETA: 0s - loss: 0.1919 - acc: 0.911 - ETA: 0s - loss: 0.1929 - acc: 0.909 - ETA: 0s - loss: 0.1960 - acc: 0.909 - ETA: 0s - loss: 0.1971 - acc: 0.909 - ETA: 0s - loss: 0.1977 - acc: 0.908 - ETA: 0s - loss: 0.1987 - acc: 0.907 - ETA: 0s - loss: 0.1981 - acc: 0.907 - ETA: 0s - loss: 0.1991 - acc: 0.907 - ETA: 0s - loss: 0.1990 - acc: 0.907 - ETA: 0s - loss: 0.1999 - acc: 0.906 - ETA: 0s - loss: 0.2022 - acc: 0.906 - 1s 51us/step - loss: 0.2019 - acc: 0.9069 - val_loss: 0.7222 - val_acc: 0.8316\n",
      "Epoch 247/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.1752 - acc: 0.921 - ETA: 0s - loss: 0.2088 - acc: 0.908 - ETA: 0s - loss: 0.2087 - acc: 0.906 - ETA: 0s - loss: 0.2035 - acc: 0.913 - ETA: 0s - loss: 0.2067 - acc: 0.911 - ETA: 0s - loss: 0.2015 - acc: 0.911 - ETA: 0s - loss: 0.2000 - acc: 0.910 - ETA: 0s - loss: 0.1962 - acc: 0.913 - ETA: 0s - loss: 0.1969 - acc: 0.911 - ETA: 0s - loss: 0.1965 - acc: 0.911 - ETA: 0s - loss: 0.1968 - acc: 0.911 - ETA: 0s - loss: 0.1972 - acc: 0.910 - ETA: 0s - loss: 0.1998 - acc: 0.910 - ETA: 0s - loss: 0.1991 - acc: 0.911 - ETA: 0s - loss: 0.2004 - acc: 0.910 - ETA: 0s - loss: 0.2006 - acc: 0.909 - ETA: 0s - loss: 0.2012 - acc: 0.909 - ETA: 0s - loss: 0.2019 - acc: 0.908 - 1s 49us/step - loss: 0.2024 - acc: 0.9086 - val_loss: 0.7108 - val_acc: 0.8263\n",
      "Epoch 248/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.2214 - acc: 0.890 - ETA: 0s - loss: 0.2170 - acc: 0.906 - ETA: 0s - loss: 0.2157 - acc: 0.904 - ETA: 0s - loss: 0.2025 - acc: 0.909 - ETA: 0s - loss: 0.2001 - acc: 0.908 - ETA: 0s - loss: 0.1977 - acc: 0.908 - ETA: 0s - loss: 0.1997 - acc: 0.908 - ETA: 0s - loss: 0.2029 - acc: 0.906 - ETA: 0s - loss: 0.2027 - acc: 0.908 - ETA: 0s - loss: 0.2008 - acc: 0.909 - ETA: 0s - loss: 0.2021 - acc: 0.908 - ETA: 0s - loss: 0.2024 - acc: 0.908 - ETA: 0s - loss: 0.2039 - acc: 0.907 - ETA: 0s - loss: 0.2045 - acc: 0.906 - ETA: 0s - loss: 0.2052 - acc: 0.906 - ETA: 0s - loss: 0.2042 - acc: 0.906 - ETA: 0s - loss: 0.2053 - acc: 0.906 - ETA: 0s - loss: 0.2042 - acc: 0.907 - 1s 49us/step - loss: 0.2051 - acc: 0.9071 - val_loss: 0.7587 - val_acc: 0.8188\n",
      "Epoch 249/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.1775 - acc: 0.921 - ETA: 0s - loss: 0.1836 - acc: 0.908 - ETA: 0s - loss: 0.1921 - acc: 0.910 - ETA: 0s - loss: 0.1913 - acc: 0.909 - ETA: 0s - loss: 0.1990 - acc: 0.909 - ETA: 0s - loss: 0.2020 - acc: 0.906 - ETA: 0s - loss: 0.2026 - acc: 0.906 - ETA: 0s - loss: 0.2013 - acc: 0.906 - ETA: 0s - loss: 0.2025 - acc: 0.905 - ETA: 0s - loss: 0.2009 - acc: 0.907 - ETA: 0s - loss: 0.2016 - acc: 0.907 - ETA: 0s - loss: 0.2017 - acc: 0.907 - ETA: 0s - loss: 0.2020 - acc: 0.907 - ETA: 0s - loss: 0.2016 - acc: 0.908 - ETA: 0s - loss: 0.2034 - acc: 0.906 - ETA: 0s - loss: 0.2030 - acc: 0.906 - ETA: 0s - loss: 0.2016 - acc: 0.907 - ETA: 0s - loss: 0.2015 - acc: 0.906 - 1s 50us/step - loss: 0.2022 - acc: 0.9069 - val_loss: 0.7210 - val_acc: 0.8238\n",
      "Epoch 250/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1379 - acc: 0.953 - ETA: 0s - loss: 0.1986 - acc: 0.902 - ETA: 0s - loss: 0.2004 - acc: 0.904 - ETA: 0s - loss: 0.1936 - acc: 0.908 - ETA: 0s - loss: 0.1979 - acc: 0.912 - ETA: 0s - loss: 0.1965 - acc: 0.910 - ETA: 0s - loss: 0.1936 - acc: 0.911 - ETA: 0s - loss: 0.1999 - acc: 0.908 - ETA: 0s - loss: 0.2005 - acc: 0.907 - ETA: 0s - loss: 0.2009 - acc: 0.905 - ETA: 0s - loss: 0.2015 - acc: 0.904 - ETA: 0s - loss: 0.2000 - acc: 0.905 - ETA: 0s - loss: 0.1986 - acc: 0.905 - ETA: 0s - loss: 0.1993 - acc: 0.905 - ETA: 0s - loss: 0.2004 - acc: 0.906 - ETA: 0s - loss: 0.2010 - acc: 0.906 - ETA: 0s - loss: 0.2014 - acc: 0.906 - ETA: 0s - loss: 0.2024 - acc: 0.906 - 1s 50us/step - loss: 0.2020 - acc: 0.9070 - val_loss: 0.7415 - val_acc: 0.8314\n",
      "Epoch 251/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.2177 - acc: 0.906 - ETA: 0s - loss: 0.1820 - acc: 0.917 - ETA: 0s - loss: 0.1972 - acc: 0.913 - ETA: 0s - loss: 0.1981 - acc: 0.912 - ETA: 0s - loss: 0.1969 - acc: 0.910 - ETA: 0s - loss: 0.2008 - acc: 0.909 - ETA: 0s - loss: 0.1960 - acc: 0.911 - ETA: 0s - loss: 0.1970 - acc: 0.910 - ETA: 0s - loss: 0.1986 - acc: 0.910 - ETA: 0s - loss: 0.1974 - acc: 0.910 - ETA: 0s - loss: 0.1986 - acc: 0.908 - ETA: 0s - loss: 0.2001 - acc: 0.909 - ETA: 0s - loss: 0.1997 - acc: 0.909 - ETA: 0s - loss: 0.2011 - acc: 0.908 - ETA: 0s - loss: 0.2032 - acc: 0.907 - ETA: 0s - loss: 0.2012 - acc: 0.909 - ETA: 0s - loss: 0.2036 - acc: 0.908 - ETA: 0s - loss: 0.2038 - acc: 0.908 - ETA: 0s - loss: 0.2044 - acc: 0.908 - 1s 51us/step - loss: 0.2043 - acc: 0.9082 - val_loss: 0.7227 - val_acc: 0.8319\n",
      "Epoch 252/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.1621 - acc: 0.953 - ETA: 0s - loss: 0.1728 - acc: 0.916 - ETA: 0s - loss: 0.1870 - acc: 0.919 - ETA: 0s - loss: 0.1930 - acc: 0.912 - ETA: 0s - loss: 0.1981 - acc: 0.912 - ETA: 0s - loss: 0.1950 - acc: 0.913 - ETA: 0s - loss: 0.1966 - acc: 0.910 - ETA: 0s - loss: 0.1964 - acc: 0.911 - ETA: 0s - loss: 0.1977 - acc: 0.911 - ETA: 0s - loss: 0.1966 - acc: 0.911 - ETA: 0s - loss: 0.1947 - acc: 0.912 - ETA: 0s - loss: 0.1958 - acc: 0.911 - ETA: 0s - loss: 0.1976 - acc: 0.910 - ETA: 0s - loss: 0.2000 - acc: 0.910 - ETA: 0s - loss: 0.1999 - acc: 0.910 - ETA: 0s - loss: 0.2025 - acc: 0.909 - ETA: 0s - loss: 0.2014 - acc: 0.909 - ETA: 0s - loss: 0.2036 - acc: 0.908 - 1s 51us/step - loss: 0.2039 - acc: 0.9085 - val_loss: 0.7384 - val_acc: 0.8303\n",
      "Epoch 253/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.4077 - acc: 0.921 - ETA: 0s - loss: 0.2090 - acc: 0.907 - ETA: 0s - loss: 0.2057 - acc: 0.909 - ETA: 0s - loss: 0.1999 - acc: 0.909 - ETA: 0s - loss: 0.1956 - acc: 0.910 - ETA: 0s - loss: 0.1938 - acc: 0.910 - ETA: 0s - loss: 0.1961 - acc: 0.911 - ETA: 0s - loss: 0.2006 - acc: 0.909 - ETA: 0s - loss: 0.2030 - acc: 0.908 - ETA: 0s - loss: 0.2038 - acc: 0.908 - ETA: 0s - loss: 0.2039 - acc: 0.908 - ETA: 0s - loss: 0.2037 - acc: 0.908 - ETA: 0s - loss: 0.2021 - acc: 0.908 - ETA: 0s - loss: 0.2023 - acc: 0.908 - ETA: 0s - loss: 0.2038 - acc: 0.907 - ETA: 0s - loss: 0.2034 - acc: 0.907 - ETA: 0s - loss: 0.2047 - acc: 0.907 - ETA: 0s - loss: 0.2045 - acc: 0.907 - ETA: 0s - loss: 0.2048 - acc: 0.907 - 1s 51us/step - loss: 0.2049 - acc: 0.9073 - val_loss: 0.7350 - val_acc: 0.8336\n",
      "Epoch 254/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.2212 - acc: 0.875 - ETA: 0s - loss: 0.1587 - acc: 0.928 - ETA: 0s - loss: 0.1668 - acc: 0.923 - ETA: 0s - loss: 0.1867 - acc: 0.917 - ETA: 0s - loss: 0.1899 - acc: 0.914 - ETA: 0s - loss: 0.1921 - acc: 0.911 - ETA: 0s - loss: 0.1917 - acc: 0.911 - ETA: 0s - loss: 0.1966 - acc: 0.909 - ETA: 0s - loss: 0.1956 - acc: 0.910 - ETA: 0s - loss: 0.1998 - acc: 0.910 - ETA: 0s - loss: 0.1986 - acc: 0.910 - ETA: 0s - loss: 0.2001 - acc: 0.910 - ETA: 0s - loss: 0.2019 - acc: 0.909 - ETA: 0s - loss: 0.2008 - acc: 0.910 - ETA: 0s - loss: 0.2046 - acc: 0.909 - ETA: 0s - loss: 0.2036 - acc: 0.909 - ETA: 0s - loss: 0.2039 - acc: 0.908 - ETA: 0s - loss: 0.2032 - acc: 0.909 - ETA: 0s - loss: 0.2037 - acc: 0.909 - 1s 59us/step - loss: 0.2028 - acc: 0.9094 - val_loss: 0.7587 - val_acc: 0.8277\n",
      "Epoch 255/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.2859 - acc: 0.859 - ETA: 1s - loss: 0.1965 - acc: 0.913 - ETA: 1s - loss: 0.1991 - acc: 0.914 - ETA: 0s - loss: 0.1901 - acc: 0.917 - ETA: 0s - loss: 0.1930 - acc: 0.914 - ETA: 0s - loss: 0.1992 - acc: 0.913 - ETA: 0s - loss: 0.1989 - acc: 0.913 - ETA: 0s - loss: 0.1952 - acc: 0.914 - ETA: 0s - loss: 0.2006 - acc: 0.912 - ETA: 0s - loss: 0.2025 - acc: 0.911 - ETA: 0s - loss: 0.2014 - acc: 0.909 - ETA: 0s - loss: 0.2017 - acc: 0.909 - ETA: 0s - loss: 0.2007 - acc: 0.909 - ETA: 0s - loss: 0.2017 - acc: 0.908 - ETA: 0s - loss: 0.1998 - acc: 0.909 - ETA: 0s - loss: 0.1992 - acc: 0.910 - ETA: 0s - loss: 0.2016 - acc: 0.908 - ETA: 0s - loss: 0.2020 - acc: 0.908 - ETA: 0s - loss: 0.2013 - acc: 0.909 - 1s 53us/step - loss: 0.2031 - acc: 0.9083 - val_loss: 0.7149 - val_acc: 0.8298\n",
      "Epoch 256/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1996 - acc: 0.906 - ETA: 0s - loss: 0.1811 - acc: 0.910 - ETA: 0s - loss: 0.1741 - acc: 0.917 - ETA: 0s - loss: 0.1916 - acc: 0.912 - ETA: 0s - loss: 0.1940 - acc: 0.909 - ETA: 0s - loss: 0.1932 - acc: 0.911 - ETA: 0s - loss: 0.1961 - acc: 0.911 - ETA: 0s - loss: 0.1935 - acc: 0.911 - ETA: 0s - loss: 0.1965 - acc: 0.911 - ETA: 0s - loss: 0.1969 - acc: 0.910 - ETA: 0s - loss: 0.1997 - acc: 0.909 - ETA: 0s - loss: 0.2018 - acc: 0.907 - ETA: 0s - loss: 0.2040 - acc: 0.906 - ETA: 0s - loss: 0.2027 - acc: 0.907 - ETA: 0s - loss: 0.2029 - acc: 0.907 - ETA: 0s - loss: 0.2019 - acc: 0.908 - ETA: 0s - loss: 0.2019 - acc: 0.908 - ETA: 0s - loss: 0.2027 - acc: 0.908 - 1s 49us/step - loss: 0.2022 - acc: 0.9087 - val_loss: 0.7318 - val_acc: 0.8323\n",
      "Epoch 257/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1394 - acc: 0.953 - ETA: 0s - loss: 0.1710 - acc: 0.918 - ETA: 0s - loss: 0.1884 - acc: 0.915 - ETA: 0s - loss: 0.1884 - acc: 0.916 - ETA: 0s - loss: 0.1916 - acc: 0.914 - ETA: 0s - loss: 0.1994 - acc: 0.910 - ETA: 0s - loss: 0.1968 - acc: 0.911 - ETA: 0s - loss: 0.2032 - acc: 0.907 - ETA: 0s - loss: 0.2026 - acc: 0.907 - ETA: 0s - loss: 0.2028 - acc: 0.907 - ETA: 0s - loss: 0.2030 - acc: 0.908 - ETA: 0s - loss: 0.2024 - acc: 0.908 - ETA: 0s - loss: 0.2011 - acc: 0.908 - ETA: 0s - loss: 0.2012 - acc: 0.908 - ETA: 0s - loss: 0.2003 - acc: 0.908 - ETA: 0s - loss: 0.2005 - acc: 0.908 - ETA: 0s - loss: 0.2019 - acc: 0.908 - ETA: 0s - loss: 0.2029 - acc: 0.908 - 1s 50us/step - loss: 0.2026 - acc: 0.9082 - val_loss: 0.7223 - val_acc: 0.8304\n",
      "Epoch 258/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1239 - acc: 0.953 - ETA: 0s - loss: 0.2038 - acc: 0.908 - ETA: 0s - loss: 0.1964 - acc: 0.911 - ETA: 0s - loss: 0.2071 - acc: 0.906 - ETA: 0s - loss: 0.2045 - acc: 0.908 - ETA: 0s - loss: 0.2008 - acc: 0.909 - ETA: 0s - loss: 0.1975 - acc: 0.911 - ETA: 0s - loss: 0.1943 - acc: 0.911 - ETA: 0s - loss: 0.1961 - acc: 0.910 - ETA: 0s - loss: 0.1960 - acc: 0.910 - ETA: 0s - loss: 0.1956 - acc: 0.911 - ETA: 0s - loss: 0.1991 - acc: 0.910 - ETA: 0s - loss: 0.2005 - acc: 0.909 - ETA: 0s - loss: 0.2012 - acc: 0.909 - ETA: 0s - loss: 0.2014 - acc: 0.909 - ETA: 0s - loss: 0.2039 - acc: 0.908 - ETA: 0s - loss: 0.2044 - acc: 0.907 - ETA: 0s - loss: 0.2040 - acc: 0.907 - ETA: 0s - loss: 0.2037 - acc: 0.908 - 1s 54us/step - loss: 0.2045 - acc: 0.9079 - val_loss: 0.7569 - val_acc: 0.8199\n",
      "Epoch 259/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.2281 - acc: 0.890 - ETA: 1s - loss: 0.1864 - acc: 0.909 - ETA: 1s - loss: 0.1778 - acc: 0.914 - ETA: 1s - loss: 0.1878 - acc: 0.914 - ETA: 1s - loss: 0.1927 - acc: 0.911 - ETA: 1s - loss: 0.1992 - acc: 0.909 - ETA: 0s - loss: 0.1974 - acc: 0.909 - ETA: 0s - loss: 0.1946 - acc: 0.910 - ETA: 0s - loss: 0.1951 - acc: 0.911 - ETA: 0s - loss: 0.1972 - acc: 0.912 - ETA: 0s - loss: 0.1977 - acc: 0.911 - ETA: 0s - loss: 0.1977 - acc: 0.911 - ETA: 0s - loss: 0.1975 - acc: 0.911 - ETA: 0s - loss: 0.2003 - acc: 0.911 - ETA: 0s - loss: 0.2007 - acc: 0.911 - ETA: 0s - loss: 0.2010 - acc: 0.911 - ETA: 0s - loss: 0.2010 - acc: 0.910 - ETA: 0s - loss: 0.2013 - acc: 0.909 - ETA: 0s - loss: 0.2011 - acc: 0.909 - ETA: 0s - loss: 0.2024 - acc: 0.909 - 1s 55us/step - loss: 0.2025 - acc: 0.9090 - val_loss: 0.7341 - val_acc: 0.8263\n",
      "Epoch 260/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.3012 - acc: 0.828 - ETA: 0s - loss: 0.1900 - acc: 0.914 - ETA: 0s - loss: 0.1970 - acc: 0.911 - ETA: 0s - loss: 0.1916 - acc: 0.913 - ETA: 0s - loss: 0.1955 - acc: 0.910 - ETA: 0s - loss: 0.1990 - acc: 0.910 - ETA: 0s - loss: 0.1990 - acc: 0.910 - ETA: 0s - loss: 0.1994 - acc: 0.909 - ETA: 0s - loss: 0.1983 - acc: 0.911 - ETA: 0s - loss: 0.1992 - acc: 0.911 - ETA: 0s - loss: 0.2007 - acc: 0.911 - ETA: 0s - loss: 0.2031 - acc: 0.910 - ETA: 0s - loss: 0.2034 - acc: 0.909 - ETA: 0s - loss: 0.2020 - acc: 0.910 - ETA: 0s - loss: 0.2028 - acc: 0.909 - ETA: 0s - loss: 0.2032 - acc: 0.909 - ETA: 0s - loss: 0.2032 - acc: 0.908 - ETA: 0s - loss: 0.2034 - acc: 0.909 - 1s 49us/step - loss: 0.2033 - acc: 0.9096 - val_loss: 0.7398 - val_acc: 0.8254\n",
      "Epoch 261/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.1442 - acc: 0.953 - ETA: 0s - loss: 0.1945 - acc: 0.901 - ETA: 0s - loss: 0.1823 - acc: 0.910 - ETA: 0s - loss: 0.1885 - acc: 0.908 - ETA: 0s - loss: 0.1848 - acc: 0.911 - ETA: 0s - loss: 0.1921 - acc: 0.909 - ETA: 0s - loss: 0.1925 - acc: 0.910 - ETA: 0s - loss: 0.1946 - acc: 0.908 - ETA: 0s - loss: 0.1988 - acc: 0.908 - ETA: 0s - loss: 0.1980 - acc: 0.908 - ETA: 0s - loss: 0.2025 - acc: 0.907 - ETA: 0s - loss: 0.2026 - acc: 0.907 - ETA: 0s - loss: 0.2029 - acc: 0.907 - ETA: 0s - loss: 0.2031 - acc: 0.907 - ETA: 0s - loss: 0.2048 - acc: 0.907 - ETA: 0s - loss: 0.2041 - acc: 0.907 - ETA: 0s - loss: 0.2045 - acc: 0.907 - ETA: 0s - loss: 0.2039 - acc: 0.907 - ETA: 0s - loss: 0.2045 - acc: 0.907 - ETA: 0s - loss: 0.2038 - acc: 0.907 - 1s 55us/step - loss: 0.2035 - acc: 0.9072 - val_loss: 0.8133 - val_acc: 0.8250\n",
      "Epoch 262/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1273 - acc: 0.953 - ETA: 1s - loss: 0.1727 - acc: 0.916 - ETA: 0s - loss: 0.1912 - acc: 0.911 - ETA: 0s - loss: 0.1903 - acc: 0.911 - ETA: 0s - loss: 0.1936 - acc: 0.907 - ETA: 0s - loss: 0.1884 - acc: 0.909 - ETA: 0s - loss: 0.1904 - acc: 0.909 - ETA: 0s - loss: 0.1909 - acc: 0.908 - ETA: 0s - loss: 0.1921 - acc: 0.909 - ETA: 0s - loss: 0.1935 - acc: 0.908 - ETA: 0s - loss: 0.1915 - acc: 0.909 - ETA: 0s - loss: 0.1909 - acc: 0.909 - ETA: 0s - loss: 0.1910 - acc: 0.909 - ETA: 0s - loss: 0.1945 - acc: 0.908 - ETA: 0s - loss: 0.1975 - acc: 0.907 - ETA: 0s - loss: 0.2013 - acc: 0.907 - ETA: 0s - loss: 0.2025 - acc: 0.906 - ETA: 0s - loss: 0.2022 - acc: 0.906 - 1s 50us/step - loss: 0.2018 - acc: 0.9064 - val_loss: 0.7458 - val_acc: 0.8325\n",
      "Epoch 263/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.2165 - acc: 0.875 - ETA: 0s - loss: 0.2017 - acc: 0.903 - ETA: 0s - loss: 0.2020 - acc: 0.905 - ETA: 0s - loss: 0.1977 - acc: 0.906 - ETA: 0s - loss: 0.2005 - acc: 0.903 - ETA: 0s - loss: 0.2006 - acc: 0.904 - ETA: 0s - loss: 0.2010 - acc: 0.906 - ETA: 0s - loss: 0.2038 - acc: 0.907 - ETA: 0s - loss: 0.2058 - acc: 0.906 - ETA: 0s - loss: 0.2009 - acc: 0.908 - ETA: 0s - loss: 0.1999 - acc: 0.910 - ETA: 0s - loss: 0.1988 - acc: 0.910 - ETA: 0s - loss: 0.2011 - acc: 0.908 - ETA: 0s - loss: 0.1989 - acc: 0.909 - ETA: 0s - loss: 0.1998 - acc: 0.909 - ETA: 0s - loss: 0.2021 - acc: 0.908 - ETA: 0s - loss: 0.2016 - acc: 0.908 - ETA: 0s - loss: 0.2021 - acc: 0.908 - 1s 49us/step - loss: 0.2018 - acc: 0.9080 - val_loss: 0.7617 - val_acc: 0.8300\n",
      "Epoch 264/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20513/20513 [==============================] - ETA: 1s - loss: 0.2189 - acc: 0.906 - ETA: 0s - loss: 0.1789 - acc: 0.919 - ETA: 0s - loss: 0.1915 - acc: 0.912 - ETA: 0s - loss: 0.1957 - acc: 0.911 - ETA: 0s - loss: 0.2016 - acc: 0.910 - ETA: 0s - loss: 0.1964 - acc: 0.913 - ETA: 0s - loss: 0.1948 - acc: 0.913 - ETA: 0s - loss: 0.1943 - acc: 0.912 - ETA: 0s - loss: 0.1946 - acc: 0.911 - ETA: 0s - loss: 0.1977 - acc: 0.909 - ETA: 0s - loss: 0.1975 - acc: 0.910 - ETA: 0s - loss: 0.1983 - acc: 0.909 - ETA: 0s - loss: 0.1992 - acc: 0.908 - ETA: 0s - loss: 0.1978 - acc: 0.909 - ETA: 0s - loss: 0.1987 - acc: 0.909 - ETA: 0s - loss: 0.2006 - acc: 0.909 - ETA: 0s - loss: 0.2000 - acc: 0.909 - ETA: 0s - loss: 0.1998 - acc: 0.909 - 1s 49us/step - loss: 0.2002 - acc: 0.9091 - val_loss: 0.8916 - val_acc: 0.8165\n",
      "Epoch 265/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.2171 - acc: 0.890 - ETA: 0s - loss: 0.2174 - acc: 0.892 - ETA: 0s - loss: 0.2107 - acc: 0.905 - ETA: 0s - loss: 0.2050 - acc: 0.906 - ETA: 0s - loss: 0.2069 - acc: 0.906 - ETA: 0s - loss: 0.2050 - acc: 0.907 - ETA: 0s - loss: 0.2061 - acc: 0.909 - ETA: 0s - loss: 0.2045 - acc: 0.910 - ETA: 0s - loss: 0.2035 - acc: 0.910 - ETA: 0s - loss: 0.2037 - acc: 0.910 - ETA: 0s - loss: 0.2027 - acc: 0.909 - ETA: 0s - loss: 0.2026 - acc: 0.909 - ETA: 0s - loss: 0.2024 - acc: 0.910 - ETA: 0s - loss: 0.2016 - acc: 0.910 - ETA: 0s - loss: 0.2050 - acc: 0.909 - ETA: 0s - loss: 0.2049 - acc: 0.908 - ETA: 0s - loss: 0.2044 - acc: 0.909 - ETA: 0s - loss: 0.2031 - acc: 0.910 - 1s 49us/step - loss: 0.2032 - acc: 0.9101 - val_loss: 0.7495 - val_acc: 0.8269\n",
      "Epoch 266/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1591 - acc: 0.937 - ETA: 0s - loss: 0.1846 - acc: 0.913 - ETA: 0s - loss: 0.1820 - acc: 0.914 - ETA: 0s - loss: 0.1850 - acc: 0.914 - ETA: 0s - loss: 0.1810 - acc: 0.918 - ETA: 0s - loss: 0.1833 - acc: 0.918 - ETA: 0s - loss: 0.1838 - acc: 0.917 - ETA: 0s - loss: 0.1888 - acc: 0.914 - ETA: 0s - loss: 0.1994 - acc: 0.911 - ETA: 0s - loss: 0.1975 - acc: 0.911 - ETA: 0s - loss: 0.1988 - acc: 0.911 - ETA: 0s - loss: 0.1983 - acc: 0.910 - ETA: 0s - loss: 0.1993 - acc: 0.910 - ETA: 0s - loss: 0.2000 - acc: 0.911 - ETA: 0s - loss: 0.2009 - acc: 0.910 - ETA: 0s - loss: 0.2016 - acc: 0.911 - ETA: 0s - loss: 0.2010 - acc: 0.911 - ETA: 0s - loss: 0.2017 - acc: 0.911 - 1s 51us/step - loss: 0.2019 - acc: 0.9110 - val_loss: 0.7561 - val_acc: 0.8280\n",
      "Epoch 267/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.2016 - acc: 0.921 - ETA: 1s - loss: 0.2019 - acc: 0.918 - ETA: 1s - loss: 0.1891 - acc: 0.913 - ETA: 1s - loss: 0.1926 - acc: 0.914 - ETA: 1s - loss: 0.1909 - acc: 0.915 - ETA: 1s - loss: 0.1935 - acc: 0.914 - ETA: 0s - loss: 0.1975 - acc: 0.911 - ETA: 0s - loss: 0.1950 - acc: 0.912 - ETA: 0s - loss: 0.1938 - acc: 0.911 - ETA: 0s - loss: 0.1932 - acc: 0.911 - ETA: 0s - loss: 0.1958 - acc: 0.910 - ETA: 0s - loss: 0.1958 - acc: 0.911 - ETA: 0s - loss: 0.1952 - acc: 0.911 - ETA: 0s - loss: 0.1949 - acc: 0.912 - ETA: 0s - loss: 0.1953 - acc: 0.912 - ETA: 0s - loss: 0.1982 - acc: 0.910 - ETA: 0s - loss: 0.1989 - acc: 0.910 - ETA: 0s - loss: 0.2003 - acc: 0.910 - ETA: 0s - loss: 0.2002 - acc: 0.909 - ETA: 0s - loss: 0.2008 - acc: 0.910 - 1s 54us/step - loss: 0.2018 - acc: 0.9097 - val_loss: 0.7016 - val_acc: 0.8286\n",
      "Epoch 268/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1891 - acc: 0.921 - ETA: 0s - loss: 0.1902 - acc: 0.907 - ETA: 0s - loss: 0.1923 - acc: 0.907 - ETA: 0s - loss: 0.1941 - acc: 0.908 - ETA: 0s - loss: 0.1990 - acc: 0.906 - ETA: 0s - loss: 0.1974 - acc: 0.907 - ETA: 0s - loss: 0.1965 - acc: 0.908 - ETA: 0s - loss: 0.1979 - acc: 0.908 - ETA: 0s - loss: 0.1959 - acc: 0.909 - ETA: 0s - loss: 0.1973 - acc: 0.907 - ETA: 0s - loss: 0.1970 - acc: 0.908 - ETA: 0s - loss: 0.2005 - acc: 0.907 - ETA: 0s - loss: 0.2009 - acc: 0.908 - ETA: 0s - loss: 0.1987 - acc: 0.909 - ETA: 0s - loss: 0.1985 - acc: 0.908 - ETA: 0s - loss: 0.2012 - acc: 0.908 - ETA: 0s - loss: 0.2012 - acc: 0.908 - ETA: 0s - loss: 0.2017 - acc: 0.908 - 1s 50us/step - loss: 0.2010 - acc: 0.9084 - val_loss: 0.7026 - val_acc: 0.8330\n",
      "Epoch 269/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.1707 - acc: 0.921 - ETA: 0s - loss: 0.1966 - acc: 0.912 - ETA: 0s - loss: 0.1941 - acc: 0.910 - ETA: 0s - loss: 0.1940 - acc: 0.909 - ETA: 0s - loss: 0.1931 - acc: 0.909 - ETA: 0s - loss: 0.1966 - acc: 0.907 - ETA: 0s - loss: 0.1976 - acc: 0.908 - ETA: 0s - loss: 0.1942 - acc: 0.909 - ETA: 0s - loss: 0.1974 - acc: 0.907 - ETA: 0s - loss: 0.1979 - acc: 0.907 - ETA: 0s - loss: 0.1980 - acc: 0.907 - ETA: 0s - loss: 0.1988 - acc: 0.907 - ETA: 0s - loss: 0.1980 - acc: 0.908 - ETA: 0s - loss: 0.2010 - acc: 0.907 - ETA: 0s - loss: 0.2015 - acc: 0.908 - ETA: 0s - loss: 0.2009 - acc: 0.909 - ETA: 0s - loss: 0.2005 - acc: 0.909 - ETA: 0s - loss: 0.2009 - acc: 0.908 - 1s 49us/step - loss: 0.2007 - acc: 0.9088 - val_loss: 0.7504 - val_acc: 0.8304\n",
      "Epoch 270/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1884 - acc: 0.875 - ETA: 0s - loss: 0.1818 - acc: 0.919 - ETA: 0s - loss: 0.1815 - acc: 0.915 - ETA: 0s - loss: 0.1842 - acc: 0.915 - ETA: 0s - loss: 0.1954 - acc: 0.911 - ETA: 0s - loss: 0.2021 - acc: 0.909 - ETA: 0s - loss: 0.1978 - acc: 0.911 - ETA: 0s - loss: 0.2016 - acc: 0.909 - ETA: 0s - loss: 0.2011 - acc: 0.908 - ETA: 0s - loss: 0.1995 - acc: 0.909 - ETA: 0s - loss: 0.1987 - acc: 0.910 - ETA: 0s - loss: 0.1981 - acc: 0.910 - ETA: 0s - loss: 0.2001 - acc: 0.910 - ETA: 0s - loss: 0.2005 - acc: 0.911 - ETA: 0s - loss: 0.1998 - acc: 0.911 - ETA: 0s - loss: 0.1994 - acc: 0.911 - ETA: 0s - loss: 0.1998 - acc: 0.910 - ETA: 0s - loss: 0.2007 - acc: 0.909 - 1s 50us/step - loss: 0.1996 - acc: 0.9101 - val_loss: 0.7566 - val_acc: 0.8271\n",
      "Epoch 271/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1062 - acc: 0.968 - ETA: 0s - loss: 0.1932 - acc: 0.918 - ETA: 0s - loss: 0.2016 - acc: 0.913 - ETA: 0s - loss: 0.1997 - acc: 0.912 - ETA: 0s - loss: 0.1915 - acc: 0.916 - ETA: 0s - loss: 0.1931 - acc: 0.914 - ETA: 0s - loss: 0.1953 - acc: 0.914 - ETA: 0s - loss: 0.1946 - acc: 0.913 - ETA: 0s - loss: 0.1947 - acc: 0.912 - ETA: 0s - loss: 0.1960 - acc: 0.911 - ETA: 0s - loss: 0.1953 - acc: 0.912 - ETA: 0s - loss: 0.1963 - acc: 0.911 - ETA: 0s - loss: 0.1980 - acc: 0.911 - ETA: 0s - loss: 0.1971 - acc: 0.912 - ETA: 0s - loss: 0.1978 - acc: 0.911 - ETA: 0s - loss: 0.1999 - acc: 0.910 - ETA: 0s - loss: 0.2018 - acc: 0.909 - 1s 48us/step - loss: 0.2017 - acc: 0.9092 - val_loss: 0.7658 - val_acc: 0.8230\n",
      "Epoch 272/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.2466 - acc: 0.875 - ETA: 0s - loss: 0.1728 - acc: 0.916 - ETA: 0s - loss: 0.1897 - acc: 0.915 - ETA: 0s - loss: 0.1815 - acc: 0.920 - ETA: 0s - loss: 0.1804 - acc: 0.922 - ETA: 0s - loss: 0.1902 - acc: 0.917 - ETA: 0s - loss: 0.1916 - acc: 0.917 - ETA: 0s - loss: 0.1952 - acc: 0.913 - ETA: 0s - loss: 0.1952 - acc: 0.913 - ETA: 0s - loss: 0.1939 - acc: 0.913 - ETA: 0s - loss: 0.1910 - acc: 0.914 - ETA: 0s - loss: 0.1907 - acc: 0.914 - ETA: 0s - loss: 0.1925 - acc: 0.914 - ETA: 0s - loss: 0.1914 - acc: 0.914 - ETA: 0s - loss: 0.1929 - acc: 0.913 - ETA: 0s - loss: 0.1932 - acc: 0.913 - ETA: 0s - loss: 0.1936 - acc: 0.913 - ETA: 0s - loss: 0.1950 - acc: 0.911 - ETA: 0s - loss: 0.1985 - acc: 0.910 - 1s 51us/step - loss: 0.1989 - acc: 0.9103 - val_loss: 0.7221 - val_acc: 0.8227\n",
      "Epoch 273/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1652 - acc: 0.921 - ETA: 0s - loss: 0.1901 - acc: 0.918 - ETA: 0s - loss: 0.1969 - acc: 0.913 - ETA: 0s - loss: 0.2040 - acc: 0.911 - ETA: 0s - loss: 0.2018 - acc: 0.911 - ETA: 0s - loss: 0.2021 - acc: 0.911 - ETA: 0s - loss: 0.2042 - acc: 0.909 - ETA: 0s - loss: 0.2023 - acc: 0.910 - ETA: 0s - loss: 0.2007 - acc: 0.910 - ETA: 0s - loss: 0.2022 - acc: 0.909 - ETA: 0s - loss: 0.2042 - acc: 0.907 - ETA: 0s - loss: 0.2026 - acc: 0.907 - ETA: 0s - loss: 0.2008 - acc: 0.909 - ETA: 0s - loss: 0.1988 - acc: 0.909 - ETA: 0s - loss: 0.1981 - acc: 0.909 - ETA: 0s - loss: 0.1967 - acc: 0.911 - ETA: 0s - loss: 0.1968 - acc: 0.911 - ETA: 0s - loss: 0.1990 - acc: 0.910 - 1s 49us/step - loss: 0.1989 - acc: 0.9108 - val_loss: 0.7078 - val_acc: 0.8306\n",
      "Epoch 274/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1847 - acc: 0.890 - ETA: 0s - loss: 0.1801 - acc: 0.919 - ETA: 0s - loss: 0.1793 - acc: 0.918 - ETA: 0s - loss: 0.1867 - acc: 0.916 - ETA: 0s - loss: 0.1894 - acc: 0.913 - ETA: 0s - loss: 0.1888 - acc: 0.912 - ETA: 0s - loss: 0.1924 - acc: 0.910 - ETA: 0s - loss: 0.1968 - acc: 0.909 - ETA: 0s - loss: 0.2003 - acc: 0.909 - ETA: 0s - loss: 0.1991 - acc: 0.909 - ETA: 0s - loss: 0.1967 - acc: 0.910 - ETA: 0s - loss: 0.1961 - acc: 0.911 - ETA: 0s - loss: 0.1954 - acc: 0.911 - ETA: 0s - loss: 0.1979 - acc: 0.910 - ETA: 0s - loss: 0.1998 - acc: 0.910 - ETA: 0s - loss: 0.2008 - acc: 0.909 - ETA: 0s - loss: 0.2010 - acc: 0.910 - ETA: 0s - loss: 0.2022 - acc: 0.909 - 1s 49us/step - loss: 0.2025 - acc: 0.9096 - val_loss: 0.7769 - val_acc: 0.8202\n",
      "Epoch 275/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1564 - acc: 0.921 - ETA: 0s - loss: 0.1836 - acc: 0.916 - ETA: 0s - loss: 0.1906 - acc: 0.915 - ETA: 0s - loss: 0.1931 - acc: 0.913 - ETA: 0s - loss: 0.1977 - acc: 0.912 - ETA: 0s - loss: 0.1987 - acc: 0.909 - ETA: 0s - loss: 0.1977 - acc: 0.911 - ETA: 0s - loss: 0.1970 - acc: 0.911 - ETA: 0s - loss: 0.1955 - acc: 0.911 - ETA: 0s - loss: 0.1962 - acc: 0.910 - ETA: 0s - loss: 0.1994 - acc: 0.908 - ETA: 0s - loss: 0.1990 - acc: 0.909 - ETA: 0s - loss: 0.1998 - acc: 0.909 - ETA: 0s - loss: 0.1978 - acc: 0.909 - ETA: 0s - loss: 0.1979 - acc: 0.909 - ETA: 0s - loss: 0.1991 - acc: 0.908 - ETA: 0s - loss: 0.1988 - acc: 0.908 - ETA: 0s - loss: 0.1988 - acc: 0.909 - 1s 50us/step - loss: 0.1987 - acc: 0.9091 - val_loss: 0.7349 - val_acc: 0.8301\n",
      "Epoch 276/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1373 - acc: 0.937 - ETA: 0s - loss: 0.1793 - acc: 0.924 - ETA: 0s - loss: 0.1909 - acc: 0.919 - ETA: 0s - loss: 0.1930 - acc: 0.915 - ETA: 0s - loss: 0.1892 - acc: 0.915 - ETA: 0s - loss: 0.1910 - acc: 0.915 - ETA: 0s - loss: 0.1915 - acc: 0.913 - ETA: 0s - loss: 0.1942 - acc: 0.912 - ETA: 0s - loss: 0.1949 - acc: 0.912 - ETA: 0s - loss: 0.1939 - acc: 0.912 - ETA: 0s - loss: 0.1948 - acc: 0.912 - ETA: 0s - loss: 0.1958 - acc: 0.911 - ETA: 0s - loss: 0.1972 - acc: 0.910 - ETA: 0s - loss: 0.2003 - acc: 0.909 - ETA: 0s - loss: 0.1998 - acc: 0.909 - ETA: 0s - loss: 0.1996 - acc: 0.909 - ETA: 0s - loss: 0.1991 - acc: 0.910 - ETA: 0s - loss: 0.1988 - acc: 0.910 - 1s 50us/step - loss: 0.1999 - acc: 0.9105 - val_loss: 0.7972 - val_acc: 0.8244\n",
      "Epoch 277/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.2762 - acc: 0.890 - ETA: 0s - loss: 0.1826 - acc: 0.916 - ETA: 0s - loss: 0.1781 - acc: 0.918 - ETA: 0s - loss: 0.1971 - acc: 0.914 - ETA: 0s - loss: 0.1986 - acc: 0.911 - ETA: 0s - loss: 0.1968 - acc: 0.911 - ETA: 0s - loss: 0.1968 - acc: 0.910 - ETA: 0s - loss: 0.2012 - acc: 0.909 - ETA: 0s - loss: 0.1985 - acc: 0.911 - ETA: 0s - loss: 0.1997 - acc: 0.910 - ETA: 0s - loss: 0.2023 - acc: 0.909 - ETA: 0s - loss: 0.2048 - acc: 0.909 - ETA: 0s - loss: 0.2012 - acc: 0.911 - ETA: 0s - loss: 0.2031 - acc: 0.910 - ETA: 0s - loss: 0.2023 - acc: 0.910 - ETA: 0s - loss: 0.2025 - acc: 0.910 - ETA: 0s - loss: 0.2040 - acc: 0.909 - ETA: 0s - loss: 0.2020 - acc: 0.910 - 1s 50us/step - loss: 0.2021 - acc: 0.9099 - val_loss: 0.7735 - val_acc: 0.8280\n",
      "Epoch 278/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.2052 - acc: 0.875 - ETA: 0s - loss: 0.1988 - acc: 0.903 - ETA: 0s - loss: 0.1961 - acc: 0.902 - ETA: 0s - loss: 0.1987 - acc: 0.902 - ETA: 0s - loss: 0.1976 - acc: 0.902 - ETA: 0s - loss: 0.1968 - acc: 0.904 - ETA: 0s - loss: 0.2019 - acc: 0.903 - ETA: 0s - loss: 0.2005 - acc: 0.905 - ETA: 0s - loss: 0.2013 - acc: 0.905 - ETA: 0s - loss: 0.2023 - acc: 0.905 - ETA: 0s - loss: 0.2028 - acc: 0.905 - ETA: 0s - loss: 0.2040 - acc: 0.904 - ETA: 0s - loss: 0.2042 - acc: 0.905 - ETA: 0s - loss: 0.2025 - acc: 0.907 - ETA: 0s - loss: 0.2003 - acc: 0.908 - ETA: 0s - loss: 0.1991 - acc: 0.909 - ETA: 0s - loss: 0.1995 - acc: 0.909 - ETA: 0s - loss: 0.1998 - acc: 0.909 - 1s 50us/step - loss: 0.2003 - acc: 0.9089 - val_loss: 0.7952 - val_acc: 0.8261\n",
      "Epoch 279/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1278 - acc: 0.937 - ETA: 0s - loss: 0.1929 - acc: 0.906 - ETA: 0s - loss: 0.2013 - acc: 0.909 - ETA: 0s - loss: 0.1970 - acc: 0.911 - ETA: 0s - loss: 0.1887 - acc: 0.914 - ETA: 0s - loss: 0.1888 - acc: 0.915 - ETA: 0s - loss: 0.1930 - acc: 0.914 - ETA: 0s - loss: 0.1883 - acc: 0.915 - ETA: 0s - loss: 0.1934 - acc: 0.914 - ETA: 0s - loss: 0.1933 - acc: 0.914 - ETA: 0s - loss: 0.1949 - acc: 0.913 - ETA: 0s - loss: 0.1962 - acc: 0.913 - ETA: 0s - loss: 0.1968 - acc: 0.912 - ETA: 0s - loss: 0.1965 - acc: 0.912 - ETA: 0s - loss: 0.1966 - acc: 0.912 - ETA: 0s - loss: 0.1977 - acc: 0.911 - ETA: 0s - loss: 0.1986 - acc: 0.911 - ETA: 0s - loss: 0.1986 - acc: 0.911 - 1s 51us/step - loss: 0.1990 - acc: 0.9108 - val_loss: 0.7231 - val_acc: 0.8331\n",
      "Epoch 280/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1807 - acc: 0.906 - ETA: 0s - loss: 0.1959 - acc: 0.904 - ETA: 0s - loss: 0.1963 - acc: 0.911 - ETA: 0s - loss: 0.1940 - acc: 0.914 - ETA: 0s - loss: 0.1953 - acc: 0.915 - ETA: 0s - loss: 0.1877 - acc: 0.917 - ETA: 0s - loss: 0.1924 - acc: 0.916 - ETA: 0s - loss: 0.1949 - acc: 0.915 - ETA: 0s - loss: 0.1952 - acc: 0.914 - ETA: 0s - loss: 0.1964 - acc: 0.914 - ETA: 0s - loss: 0.1974 - acc: 0.912 - ETA: 0s - loss: 0.1974 - acc: 0.912 - ETA: 0s - loss: 0.1968 - acc: 0.912 - ETA: 0s - loss: 0.1972 - acc: 0.911 - ETA: 0s - loss: 0.1981 - acc: 0.911 - ETA: 0s - loss: 0.1976 - acc: 0.911 - ETA: 0s - loss: 0.1979 - acc: 0.912 - ETA: 0s - loss: 0.1984 - acc: 0.911 - 1s 49us/step - loss: 0.1987 - acc: 0.9114 - val_loss: 0.8054 - val_acc: 0.8174\n",
      "Epoch 281/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1345 - acc: 0.953 - ETA: 0s - loss: 0.1796 - acc: 0.918 - ETA: 0s - loss: 0.1812 - acc: 0.917 - ETA: 0s - loss: 0.1808 - acc: 0.916 - ETA: 0s - loss: 0.1802 - acc: 0.914 - ETA: 0s - loss: 0.1809 - acc: 0.914 - ETA: 0s - loss: 0.1831 - acc: 0.913 - ETA: 0s - loss: 0.1892 - acc: 0.911 - ETA: 0s - loss: 0.1913 - acc: 0.910 - ETA: 0s - loss: 0.1911 - acc: 0.910 - ETA: 0s - loss: 0.1900 - acc: 0.911 - ETA: 0s - loss: 0.1923 - acc: 0.910 - ETA: 0s - loss: 0.1953 - acc: 0.908 - ETA: 0s - loss: 0.1956 - acc: 0.909 - ETA: 0s - loss: 0.1981 - acc: 0.909 - ETA: 0s - loss: 0.1985 - acc: 0.909 - ETA: 0s - loss: 0.1998 - acc: 0.908 - ETA: 0s - loss: 0.2000 - acc: 0.908 - 1s 50us/step - loss: 0.1994 - acc: 0.9084 - val_loss: 0.7799 - val_acc: 0.8277\n",
      "Epoch 282/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1164 - acc: 0.984 - ETA: 0s - loss: 0.1933 - acc: 0.922 - ETA: 0s - loss: 0.1946 - acc: 0.922 - ETA: 0s - loss: 0.1993 - acc: 0.918 - ETA: 0s - loss: 0.2004 - acc: 0.914 - ETA: 0s - loss: 0.2055 - acc: 0.911 - ETA: 0s - loss: 0.2036 - acc: 0.911 - ETA: 0s - loss: 0.2019 - acc: 0.912 - ETA: 0s - loss: 0.2012 - acc: 0.912 - ETA: 0s - loss: 0.2026 - acc: 0.911 - ETA: 0s - loss: 0.2022 - acc: 0.910 - ETA: 0s - loss: 0.1989 - acc: 0.911 - ETA: 0s - loss: 0.1993 - acc: 0.910 - ETA: 0s - loss: 0.1990 - acc: 0.910 - ETA: 0s - loss: 0.1978 - acc: 0.911 - ETA: 0s - loss: 0.1983 - acc: 0.910 - ETA: 0s - loss: 0.1983 - acc: 0.909 - ETA: 0s - loss: 0.1981 - acc: 0.910 - 1s 50us/step - loss: 0.1985 - acc: 0.9103 - val_loss: 0.8768 - val_acc: 0.8205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 283/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.2325 - acc: 0.875 - ETA: 0s - loss: 0.1972 - acc: 0.915 - ETA: 0s - loss: 0.1952 - acc: 0.917 - ETA: 0s - loss: 0.1884 - acc: 0.918 - ETA: 0s - loss: 0.1873 - acc: 0.917 - ETA: 0s - loss: 0.1959 - acc: 0.914 - ETA: 0s - loss: 0.1941 - acc: 0.915 - ETA: 0s - loss: 0.1945 - acc: 0.914 - ETA: 0s - loss: 0.1951 - acc: 0.914 - ETA: 0s - loss: 0.1957 - acc: 0.913 - ETA: 0s - loss: 0.1930 - acc: 0.914 - ETA: 0s - loss: 0.1953 - acc: 0.913 - ETA: 0s - loss: 0.1958 - acc: 0.913 - ETA: 0s - loss: 0.1946 - acc: 0.913 - ETA: 0s - loss: 0.1983 - acc: 0.912 - ETA: 0s - loss: 0.1998 - acc: 0.911 - ETA: 0s - loss: 0.2014 - acc: 0.910 - ETA: 0s - loss: 0.2016 - acc: 0.910 - 1s 49us/step - loss: 0.2013 - acc: 0.9103 - val_loss: 0.7882 - val_acc: 0.8226\n",
      "Epoch 284/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1560 - acc: 0.921 - ETA: 0s - loss: 0.1688 - acc: 0.922 - ETA: 0s - loss: 0.1743 - acc: 0.918 - ETA: 0s - loss: 0.1838 - acc: 0.913 - ETA: 0s - loss: 0.1906 - acc: 0.911 - ETA: 0s - loss: 0.1947 - acc: 0.910 - ETA: 0s - loss: 0.1916 - acc: 0.911 - ETA: 0s - loss: 0.1963 - acc: 0.909 - ETA: 0s - loss: 0.1941 - acc: 0.910 - ETA: 0s - loss: 0.1946 - acc: 0.909 - ETA: 0s - loss: 0.1960 - acc: 0.909 - ETA: 0s - loss: 0.1958 - acc: 0.910 - ETA: 0s - loss: 0.1964 - acc: 0.909 - ETA: 0s - loss: 0.1972 - acc: 0.909 - ETA: 0s - loss: 0.1951 - acc: 0.911 - ETA: 0s - loss: 0.1966 - acc: 0.910 - ETA: 0s - loss: 0.1973 - acc: 0.910 - ETA: 0s - loss: 0.1984 - acc: 0.910 - 1s 50us/step - loss: 0.1986 - acc: 0.9096 - val_loss: 0.8509 - val_acc: 0.8127\n",
      "Epoch 285/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.2610 - acc: 0.890 - ETA: 0s - loss: 0.1972 - acc: 0.916 - ETA: 0s - loss: 0.1924 - acc: 0.915 - ETA: 0s - loss: 0.1832 - acc: 0.917 - ETA: 0s - loss: 0.1795 - acc: 0.917 - ETA: 0s - loss: 0.1764 - acc: 0.918 - ETA: 0s - loss: 0.1832 - acc: 0.915 - ETA: 0s - loss: 0.1882 - acc: 0.912 - ETA: 0s - loss: 0.1898 - acc: 0.912 - ETA: 0s - loss: 0.1921 - acc: 0.911 - ETA: 0s - loss: 0.1958 - acc: 0.910 - ETA: 0s - loss: 0.1972 - acc: 0.911 - ETA: 0s - loss: 0.1992 - acc: 0.911 - ETA: 0s - loss: 0.1989 - acc: 0.910 - ETA: 0s - loss: 0.1977 - acc: 0.911 - ETA: 0s - loss: 0.1963 - acc: 0.911 - ETA: 0s - loss: 0.1991 - acc: 0.911 - ETA: 0s - loss: 0.1985 - acc: 0.911 - ETA: 0s - loss: 0.1992 - acc: 0.910 - 1s 52us/step - loss: 0.2001 - acc: 0.9103 - val_loss: 0.7906 - val_acc: 0.8236\n",
      "Epoch 286/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.1343 - acc: 0.937 - ETA: 0s - loss: 0.1951 - acc: 0.907 - ETA: 0s - loss: 0.1878 - acc: 0.913 - ETA: 0s - loss: 0.1870 - acc: 0.912 - ETA: 0s - loss: 0.1874 - acc: 0.913 - ETA: 0s - loss: 0.1866 - acc: 0.912 - ETA: 0s - loss: 0.1881 - acc: 0.912 - ETA: 0s - loss: 0.1921 - acc: 0.909 - ETA: 0s - loss: 0.1915 - acc: 0.909 - ETA: 0s - loss: 0.1914 - acc: 0.911 - ETA: 0s - loss: 0.1910 - acc: 0.911 - ETA: 0s - loss: 0.1925 - acc: 0.910 - ETA: 0s - loss: 0.1922 - acc: 0.911 - ETA: 0s - loss: 0.1941 - acc: 0.910 - ETA: 0s - loss: 0.1958 - acc: 0.909 - ETA: 0s - loss: 0.1971 - acc: 0.910 - ETA: 0s - loss: 0.1963 - acc: 0.910 - ETA: 0s - loss: 0.1980 - acc: 0.910 - 1s 50us/step - loss: 0.1977 - acc: 0.9107 - val_loss: 0.8373 - val_acc: 0.8256\n",
      "Epoch 287/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1842 - acc: 0.921 - ETA: 0s - loss: 0.1950 - acc: 0.910 - ETA: 0s - loss: 0.1847 - acc: 0.909 - ETA: 0s - loss: 0.1887 - acc: 0.909 - ETA: 0s - loss: 0.1856 - acc: 0.913 - ETA: 0s - loss: 0.1875 - acc: 0.914 - ETA: 0s - loss: 0.1854 - acc: 0.915 - ETA: 0s - loss: 0.1872 - acc: 0.915 - ETA: 0s - loss: 0.1905 - acc: 0.913 - ETA: 0s - loss: 0.1913 - acc: 0.913 - ETA: 0s - loss: 0.1938 - acc: 0.912 - ETA: 0s - loss: 0.1935 - acc: 0.912 - ETA: 0s - loss: 0.1937 - acc: 0.912 - ETA: 0s - loss: 0.1951 - acc: 0.912 - ETA: 0s - loss: 0.1947 - acc: 0.912 - ETA: 0s - loss: 0.1966 - acc: 0.911 - ETA: 0s - loss: 0.1971 - acc: 0.910 - ETA: 0s - loss: 0.1975 - acc: 0.911 - 1s 51us/step - loss: 0.1983 - acc: 0.9112 - val_loss: 0.7927 - val_acc: 0.8280\n",
      "Epoch 288/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.2149 - acc: 0.906 - ETA: 0s - loss: 0.1982 - acc: 0.908 - ETA: 0s - loss: 0.2133 - acc: 0.911 - ETA: 0s - loss: 0.2115 - acc: 0.905 - ETA: 0s - loss: 0.2044 - acc: 0.907 - ETA: 0s - loss: 0.2036 - acc: 0.907 - ETA: 0s - loss: 0.2098 - acc: 0.906 - ETA: 0s - loss: 0.2097 - acc: 0.906 - ETA: 0s - loss: 0.2086 - acc: 0.907 - ETA: 0s - loss: 0.2067 - acc: 0.907 - ETA: 0s - loss: 0.2028 - acc: 0.909 - ETA: 0s - loss: 0.2006 - acc: 0.911 - ETA: 0s - loss: 0.2023 - acc: 0.910 - ETA: 0s - loss: 0.2011 - acc: 0.910 - ETA: 0s - loss: 0.2013 - acc: 0.910 - ETA: 0s - loss: 0.2008 - acc: 0.910 - ETA: 0s - loss: 0.1993 - acc: 0.910 - ETA: 0s - loss: 0.1998 - acc: 0.909 - 1s 51us/step - loss: 0.2008 - acc: 0.9093 - val_loss: 0.7762 - val_acc: 0.8288\n",
      "Epoch 289/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1371 - acc: 0.921 - ETA: 0s - loss: 0.1784 - acc: 0.921 - ETA: 0s - loss: 0.2008 - acc: 0.906 - ETA: 0s - loss: 0.1970 - acc: 0.907 - ETA: 0s - loss: 0.1917 - acc: 0.911 - ETA: 0s - loss: 0.1993 - acc: 0.908 - ETA: 0s - loss: 0.2000 - acc: 0.909 - ETA: 0s - loss: 0.1993 - acc: 0.908 - ETA: 0s - loss: 0.1958 - acc: 0.909 - ETA: 0s - loss: 0.1965 - acc: 0.908 - ETA: 0s - loss: 0.1968 - acc: 0.908 - ETA: 0s - loss: 0.1971 - acc: 0.908 - ETA: 0s - loss: 0.1970 - acc: 0.908 - ETA: 0s - loss: 0.1965 - acc: 0.910 - ETA: 0s - loss: 0.1944 - acc: 0.911 - ETA: 0s - loss: 0.1935 - acc: 0.911 - ETA: 0s - loss: 0.1970 - acc: 0.910 - ETA: 0s - loss: 0.1967 - acc: 0.910 - 1s 50us/step - loss: 0.1987 - acc: 0.9108 - val_loss: 0.8378 - val_acc: 0.8202\n",
      "Epoch 290/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.1279 - acc: 0.968 - ETA: 0s - loss: 0.1923 - acc: 0.916 - ETA: 0s - loss: 0.1850 - acc: 0.919 - ETA: 0s - loss: 0.1920 - acc: 0.917 - ETA: 0s - loss: 0.1905 - acc: 0.917 - ETA: 0s - loss: 0.1897 - acc: 0.916 - ETA: 0s - loss: 0.1936 - acc: 0.915 - ETA: 0s - loss: 0.1927 - acc: 0.916 - ETA: 0s - loss: 0.1901 - acc: 0.916 - ETA: 0s - loss: 0.1895 - acc: 0.916 - ETA: 0s - loss: 0.1903 - acc: 0.914 - ETA: 0s - loss: 0.1892 - acc: 0.915 - ETA: 0s - loss: 0.1923 - acc: 0.913 - ETA: 0s - loss: 0.1948 - acc: 0.911 - ETA: 0s - loss: 0.1957 - acc: 0.911 - ETA: 0s - loss: 0.1975 - acc: 0.910 - ETA: 0s - loss: 0.1981 - acc: 0.911 - ETA: 0s - loss: 0.1987 - acc: 0.910 - 1s 50us/step - loss: 0.1992 - acc: 0.9107 - val_loss: 0.7864 - val_acc: 0.8230\n",
      "Epoch 291/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.0857 - acc: 1.000 - ETA: 0s - loss: 0.2034 - acc: 0.914 - ETA: 0s - loss: 0.1906 - acc: 0.918 - ETA: 0s - loss: 0.1877 - acc: 0.917 - ETA: 0s - loss: 0.1867 - acc: 0.917 - ETA: 0s - loss: 0.1885 - acc: 0.915 - ETA: 0s - loss: 0.1919 - acc: 0.916 - ETA: 0s - loss: 0.1891 - acc: 0.915 - ETA: 0s - loss: 0.1891 - acc: 0.914 - ETA: 0s - loss: 0.1902 - acc: 0.913 - ETA: 0s - loss: 0.1894 - acc: 0.913 - ETA: 0s - loss: 0.1907 - acc: 0.913 - ETA: 0s - loss: 0.1912 - acc: 0.912 - ETA: 0s - loss: 0.1914 - acc: 0.913 - ETA: 0s - loss: 0.1927 - acc: 0.912 - ETA: 0s - loss: 0.1936 - acc: 0.911 - ETA: 0s - loss: 0.1951 - acc: 0.911 - ETA: 0s - loss: 0.1960 - acc: 0.910 - 1s 51us/step - loss: 0.1967 - acc: 0.9105 - val_loss: 0.8299 - val_acc: 0.8240\n",
      "Epoch 292/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.2503 - acc: 0.843 - ETA: 0s - loss: 0.1869 - acc: 0.910 - ETA: 0s - loss: 0.1845 - acc: 0.912 - ETA: 0s - loss: 0.1829 - acc: 0.914 - ETA: 0s - loss: 0.1860 - acc: 0.912 - ETA: 0s - loss: 0.1887 - acc: 0.911 - ETA: 0s - loss: 0.1916 - acc: 0.910 - ETA: 0s - loss: 0.1915 - acc: 0.910 - ETA: 0s - loss: 0.1889 - acc: 0.912 - ETA: 0s - loss: 0.1899 - acc: 0.911 - ETA: 0s - loss: 0.1926 - acc: 0.910 - ETA: 0s - loss: 0.1944 - acc: 0.910 - ETA: 0s - loss: 0.1944 - acc: 0.909 - ETA: 0s - loss: 0.1975 - acc: 0.909 - ETA: 0s - loss: 0.1978 - acc: 0.910 - ETA: 0s - loss: 0.1980 - acc: 0.909 - ETA: 0s - loss: 0.1981 - acc: 0.910 - ETA: 0s - loss: 0.1982 - acc: 0.910 - 1s 49us/step - loss: 0.1980 - acc: 0.9101 - val_loss: 0.7861 - val_acc: 0.8284\n",
      "Epoch 293/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.1150 - acc: 0.953 - ETA: 0s - loss: 0.1925 - acc: 0.916 - ETA: 0s - loss: 0.1846 - acc: 0.917 - ETA: 0s - loss: 0.1811 - acc: 0.920 - ETA: 0s - loss: 0.1924 - acc: 0.917 - ETA: 0s - loss: 0.1940 - acc: 0.915 - ETA: 0s - loss: 0.1921 - acc: 0.916 - ETA: 0s - loss: 0.1910 - acc: 0.917 - ETA: 0s - loss: 0.1939 - acc: 0.915 - ETA: 0s - loss: 0.1936 - acc: 0.914 - ETA: 0s - loss: 0.1987 - acc: 0.912 - ETA: 0s - loss: 0.1945 - acc: 0.913 - ETA: 0s - loss: 0.1952 - acc: 0.912 - ETA: 0s - loss: 0.1959 - acc: 0.911 - ETA: 0s - loss: 0.1971 - acc: 0.911 - ETA: 0s - loss: 0.1973 - acc: 0.911 - ETA: 0s - loss: 0.1967 - acc: 0.911 - ETA: 0s - loss: 0.1974 - acc: 0.911 - 1s 49us/step - loss: 0.1982 - acc: 0.9119 - val_loss: 0.7961 - val_acc: 0.8250\n",
      "Epoch 294/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.2120 - acc: 0.890 - ETA: 0s - loss: 0.2187 - acc: 0.905 - ETA: 0s - loss: 0.2017 - acc: 0.909 - ETA: 0s - loss: 0.2008 - acc: 0.910 - ETA: 0s - loss: 0.2009 - acc: 0.909 - ETA: 0s - loss: 0.2014 - acc: 0.911 - ETA: 0s - loss: 0.2000 - acc: 0.911 - ETA: 0s - loss: 0.1968 - acc: 0.911 - ETA: 0s - loss: 0.1934 - acc: 0.913 - ETA: 0s - loss: 0.1911 - acc: 0.913 - ETA: 0s - loss: 0.1954 - acc: 0.912 - ETA: 0s - loss: 0.1958 - acc: 0.911 - ETA: 0s - loss: 0.1957 - acc: 0.911 - ETA: 0s - loss: 0.1953 - acc: 0.911 - ETA: 0s - loss: 0.1946 - acc: 0.912 - ETA: 0s - loss: 0.1977 - acc: 0.911 - ETA: 0s - loss: 0.1980 - acc: 0.911 - ETA: 0s - loss: 0.1980 - acc: 0.911 - 1s 49us/step - loss: 0.1984 - acc: 0.9106 - val_loss: 0.8364 - val_acc: 0.8203\n",
      "Epoch 295/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.1425 - acc: 0.921 - ETA: 0s - loss: 0.1703 - acc: 0.923 - ETA: 0s - loss: 0.1796 - acc: 0.919 - ETA: 0s - loss: 0.1902 - acc: 0.914 - ETA: 0s - loss: 0.1935 - acc: 0.912 - ETA: 0s - loss: 0.1913 - acc: 0.912 - ETA: 0s - loss: 0.1923 - acc: 0.912 - ETA: 0s - loss: 0.1921 - acc: 0.912 - ETA: 0s - loss: 0.1932 - acc: 0.913 - ETA: 0s - loss: 0.1935 - acc: 0.912 - ETA: 0s - loss: 0.1915 - acc: 0.912 - ETA: 0s - loss: 0.1917 - acc: 0.913 - ETA: 0s - loss: 0.1954 - acc: 0.911 - ETA: 0s - loss: 0.1966 - acc: 0.911 - ETA: 0s - loss: 0.1964 - acc: 0.911 - ETA: 0s - loss: 0.1952 - acc: 0.912 - ETA: 0s - loss: 0.1939 - acc: 0.913 - 1s 48us/step - loss: 0.1967 - acc: 0.9123 - val_loss: 0.7750 - val_acc: 0.8301\n",
      "Epoch 296/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.4445 - acc: 0.828 - ETA: 0s - loss: 0.2068 - acc: 0.908 - ETA: 0s - loss: 0.1935 - acc: 0.914 - ETA: 0s - loss: 0.1842 - acc: 0.919 - ETA: 0s - loss: 0.1918 - acc: 0.913 - ETA: 0s - loss: 0.1934 - acc: 0.912 - ETA: 0s - loss: 0.1925 - acc: 0.912 - ETA: 0s - loss: 0.1923 - acc: 0.911 - ETA: 0s - loss: 0.1932 - acc: 0.911 - ETA: 0s - loss: 0.1942 - acc: 0.912 - ETA: 0s - loss: 0.1969 - acc: 0.910 - ETA: 0s - loss: 0.1978 - acc: 0.910 - ETA: 0s - loss: 0.1976 - acc: 0.909 - ETA: 0s - loss: 0.1991 - acc: 0.908 - ETA: 0s - loss: 0.1999 - acc: 0.909 - ETA: 0s - loss: 0.2006 - acc: 0.909 - ETA: 0s - loss: 0.2009 - acc: 0.909 - 1s 48us/step - loss: 0.2009 - acc: 0.9093 - val_loss: 0.8160 - val_acc: 0.8194\n",
      "Epoch 297/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1790 - acc: 0.921 - ETA: 0s - loss: 0.2001 - acc: 0.920 - ETA: 0s - loss: 0.1887 - acc: 0.921 - ETA: 0s - loss: 0.1917 - acc: 0.918 - ETA: 0s - loss: 0.1910 - acc: 0.917 - ETA: 0s - loss: 0.1969 - acc: 0.916 - ETA: 0s - loss: 0.1947 - acc: 0.916 - ETA: 0s - loss: 0.1961 - acc: 0.912 - ETA: 0s - loss: 0.1950 - acc: 0.913 - ETA: 0s - loss: 0.1946 - acc: 0.913 - ETA: 0s - loss: 0.1941 - acc: 0.912 - ETA: 0s - loss: 0.1943 - acc: 0.912 - ETA: 0s - loss: 0.1963 - acc: 0.911 - ETA: 0s - loss: 0.1983 - acc: 0.912 - ETA: 0s - loss: 0.1981 - acc: 0.912 - ETA: 0s - loss: 0.1987 - acc: 0.912 - ETA: 0s - loss: 0.2000 - acc: 0.911 - ETA: 0s - loss: 0.2004 - acc: 0.911 - 1s 49us/step - loss: 0.2002 - acc: 0.9111 - val_loss: 0.8138 - val_acc: 0.8202\n",
      "Epoch 298/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.1420 - acc: 0.953 - ETA: 0s - loss: 0.1871 - acc: 0.906 - ETA: 0s - loss: 0.1899 - acc: 0.915 - ETA: 0s - loss: 0.1862 - acc: 0.916 - ETA: 0s - loss: 0.1853 - acc: 0.918 - ETA: 0s - loss: 0.1833 - acc: 0.918 - ETA: 0s - loss: 0.1844 - acc: 0.917 - ETA: 0s - loss: 0.1852 - acc: 0.915 - ETA: 0s - loss: 0.1863 - acc: 0.914 - ETA: 0s - loss: 0.1880 - acc: 0.914 - ETA: 0s - loss: 0.1903 - acc: 0.913 - ETA: 0s - loss: 0.1908 - acc: 0.912 - ETA: 0s - loss: 0.1931 - acc: 0.912 - ETA: 0s - loss: 0.1927 - acc: 0.913 - ETA: 0s - loss: 0.1946 - acc: 0.911 - ETA: 0s - loss: 0.1939 - acc: 0.912 - ETA: 0s - loss: 0.1940 - acc: 0.912 - ETA: 0s - loss: 0.1954 - acc: 0.911 - 1s 51us/step - loss: 0.1959 - acc: 0.9114 - val_loss: 0.7985 - val_acc: 0.8243\n",
      "Epoch 299/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1705 - acc: 0.921 - ETA: 0s - loss: 0.1707 - acc: 0.921 - ETA: 0s - loss: 0.1878 - acc: 0.914 - ETA: 0s - loss: 0.1984 - acc: 0.913 - ETA: 0s - loss: 0.1978 - acc: 0.913 - ETA: 0s - loss: 0.2007 - acc: 0.909 - ETA: 0s - loss: 0.1980 - acc: 0.910 - ETA: 0s - loss: 0.2001 - acc: 0.909 - ETA: 0s - loss: 0.2034 - acc: 0.909 - ETA: 0s - loss: 0.2022 - acc: 0.910 - ETA: 0s - loss: 0.2023 - acc: 0.910 - ETA: 0s - loss: 0.2039 - acc: 0.909 - ETA: 0s - loss: 0.2028 - acc: 0.910 - ETA: 0s - loss: 0.2013 - acc: 0.911 - ETA: 0s - loss: 0.2002 - acc: 0.911 - ETA: 0s - loss: 0.2006 - acc: 0.912 - ETA: 0s - loss: 0.2011 - acc: 0.911 - ETA: 0s - loss: 0.2006 - acc: 0.912 - 1s 52us/step - loss: 0.2010 - acc: 0.9123 - val_loss: 0.7743 - val_acc: 0.8304\n",
      "Epoch 300/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1746 - acc: 0.921 - ETA: 0s - loss: 0.1950 - acc: 0.921 - ETA: 0s - loss: 0.1952 - acc: 0.917 - ETA: 0s - loss: 0.1958 - acc: 0.915 - ETA: 0s - loss: 0.1993 - acc: 0.914 - ETA: 0s - loss: 0.2046 - acc: 0.913 - ETA: 0s - loss: 0.1995 - acc: 0.914 - ETA: 0s - loss: 0.1990 - acc: 0.915 - ETA: 0s - loss: 0.2005 - acc: 0.914 - ETA: 0s - loss: 0.2003 - acc: 0.916 - ETA: 0s - loss: 0.1980 - acc: 0.916 - ETA: 0s - loss: 0.1987 - acc: 0.914 - ETA: 0s - loss: 0.1984 - acc: 0.914 - ETA: 0s - loss: 0.1959 - acc: 0.914 - ETA: 0s - loss: 0.1969 - acc: 0.913 - ETA: 0s - loss: 0.1964 - acc: 0.913 - ETA: 0s - loss: 0.1968 - acc: 0.913 - ETA: 0s - loss: 0.1974 - acc: 0.912 - 1s 50us/step - loss: 0.1982 - acc: 0.9125 - val_loss: 0.8278 - val_acc: 0.8254\n",
      "Epoch 301/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.2152 - acc: 0.906 - ETA: 0s - loss: 0.2066 - acc: 0.907 - ETA: 0s - loss: 0.1999 - acc: 0.909 - ETA: 0s - loss: 0.1932 - acc: 0.910 - ETA: 0s - loss: 0.2012 - acc: 0.909 - ETA: 0s - loss: 0.1974 - acc: 0.910 - ETA: 0s - loss: 0.1974 - acc: 0.909 - ETA: 0s - loss: 0.1967 - acc: 0.911 - ETA: 0s - loss: 0.1944 - acc: 0.911 - ETA: 0s - loss: 0.1918 - acc: 0.912 - ETA: 0s - loss: 0.1923 - acc: 0.912 - ETA: 0s - loss: 0.1933 - acc: 0.912 - ETA: 0s - loss: 0.1943 - acc: 0.912 - ETA: 0s - loss: 0.1963 - acc: 0.911 - ETA: 0s - loss: 0.1985 - acc: 0.910 - ETA: 0s - loss: 0.1979 - acc: 0.911 - ETA: 0s - loss: 0.1973 - acc: 0.911 - ETA: 0s - loss: 0.1968 - acc: 0.911 - 1s 50us/step - loss: 0.1967 - acc: 0.9124 - val_loss: 0.8271 - val_acc: 0.8267\n",
      "Epoch 302/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1949 - acc: 0.890 - ETA: 0s - loss: 0.1992 - acc: 0.903 - ETA: 0s - loss: 0.1937 - acc: 0.908 - ETA: 0s - loss: 0.1975 - acc: 0.909 - ETA: 0s - loss: 0.2014 - acc: 0.908 - ETA: 0s - loss: 0.1989 - acc: 0.909 - ETA: 0s - loss: 0.1943 - acc: 0.910 - ETA: 0s - loss: 0.1933 - acc: 0.911 - ETA: 0s - loss: 0.1922 - acc: 0.912 - ETA: 0s - loss: 0.1931 - acc: 0.912 - ETA: 0s - loss: 0.1948 - acc: 0.911 - ETA: 0s - loss: 0.1951 - acc: 0.911 - ETA: 0s - loss: 0.1961 - acc: 0.911 - ETA: 0s - loss: 0.1972 - acc: 0.911 - ETA: 0s - loss: 0.1964 - acc: 0.912 - ETA: 0s - loss: 0.1967 - acc: 0.912 - ETA: 0s - loss: 0.1978 - acc: 0.911 - ETA: 0s - loss: 0.1976 - acc: 0.911 - 1s 50us/step - loss: 0.1971 - acc: 0.9122 - val_loss: 0.8228 - val_acc: 0.8293\n",
      "Epoch 303/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.1089 - acc: 0.968 - ETA: 0s - loss: 0.1753 - acc: 0.916 - ETA: 0s - loss: 0.1880 - acc: 0.914 - ETA: 0s - loss: 0.1874 - acc: 0.913 - ETA: 0s - loss: 0.1929 - acc: 0.912 - ETA: 0s - loss: 0.1948 - acc: 0.913 - ETA: 0s - loss: 0.1937 - acc: 0.914 - ETA: 0s - loss: 0.1914 - acc: 0.916 - ETA: 0s - loss: 0.1927 - acc: 0.916 - ETA: 0s - loss: 0.1911 - acc: 0.915 - ETA: 0s - loss: 0.1903 - acc: 0.916 - ETA: 0s - loss: 0.1913 - acc: 0.915 - ETA: 0s - loss: 0.1925 - acc: 0.914 - ETA: 0s - loss: 0.1956 - acc: 0.912 - ETA: 0s - loss: 0.1936 - acc: 0.914 - ETA: 0s - loss: 0.1941 - acc: 0.913 - ETA: 0s - loss: 0.1967 - acc: 0.911 - ETA: 0s - loss: 0.1971 - acc: 0.911 - 1s 51us/step - loss: 0.1967 - acc: 0.9121 - val_loss: 0.8474 - val_acc: 0.8254\n",
      "Epoch 304/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1599 - acc: 0.968 - ETA: 0s - loss: 0.1734 - acc: 0.922 - ETA: 0s - loss: 0.1988 - acc: 0.912 - ETA: 0s - loss: 0.1917 - acc: 0.916 - ETA: 0s - loss: 0.1950 - acc: 0.914 - ETA: 0s - loss: 0.1898 - acc: 0.914 - ETA: 0s - loss: 0.1895 - acc: 0.914 - ETA: 0s - loss: 0.1915 - acc: 0.913 - ETA: 0s - loss: 0.1906 - acc: 0.915 - ETA: 0s - loss: 0.1914 - acc: 0.913 - ETA: 0s - loss: 0.1929 - acc: 0.913 - ETA: 0s - loss: 0.1919 - acc: 0.914 - ETA: 0s - loss: 0.1946 - acc: 0.914 - ETA: 0s - loss: 0.1947 - acc: 0.913 - ETA: 0s - loss: 0.1957 - acc: 0.912 - ETA: 0s - loss: 0.1963 - acc: 0.912 - ETA: 0s - loss: 0.1946 - acc: 0.913 - ETA: 0s - loss: 0.1950 - acc: 0.913 - 1s 50us/step - loss: 0.1958 - acc: 0.9122 - val_loss: 0.8840 - val_acc: 0.8071\n",
      "Epoch 305/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.1404 - acc: 0.953 - ETA: 0s - loss: 0.2020 - acc: 0.910 - ETA: 0s - loss: 0.1798 - acc: 0.918 - ETA: 0s - loss: 0.1745 - acc: 0.920 - ETA: 0s - loss: 0.1759 - acc: 0.919 - ETA: 0s - loss: 0.1802 - acc: 0.915 - ETA: 0s - loss: 0.1857 - acc: 0.914 - ETA: 0s - loss: 0.1886 - acc: 0.911 - ETA: 0s - loss: 0.1893 - acc: 0.911 - ETA: 0s - loss: 0.1922 - acc: 0.912 - ETA: 0s - loss: 0.1903 - acc: 0.913 - ETA: 0s - loss: 0.1911 - acc: 0.913 - ETA: 0s - loss: 0.1924 - acc: 0.914 - ETA: 0s - loss: 0.1934 - acc: 0.913 - ETA: 0s - loss: 0.1953 - acc: 0.913 - ETA: 0s - loss: 0.1948 - acc: 0.913 - ETA: 0s - loss: 0.1973 - acc: 0.912 - ETA: 0s - loss: 0.1992 - acc: 0.911 - 1s 49us/step - loss: 0.1992 - acc: 0.9115 - val_loss: 0.7683 - val_acc: 0.8220\n",
      "Epoch 306/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.2244 - acc: 0.875 - ETA: 0s - loss: 0.1914 - acc: 0.909 - ETA: 0s - loss: 0.1782 - acc: 0.918 - ETA: 0s - loss: 0.1928 - acc: 0.912 - ETA: 0s - loss: 0.1903 - acc: 0.912 - ETA: 0s - loss: 0.1934 - acc: 0.912 - ETA: 0s - loss: 0.1948 - acc: 0.910 - ETA: 0s - loss: 0.1925 - acc: 0.912 - ETA: 0s - loss: 0.1923 - acc: 0.912 - ETA: 0s - loss: 0.1906 - acc: 0.912 - ETA: 0s - loss: 0.1937 - acc: 0.911 - ETA: 0s - loss: 0.1932 - acc: 0.911 - ETA: 0s - loss: 0.1913 - acc: 0.913 - ETA: 0s - loss: 0.1939 - acc: 0.912 - ETA: 0s - loss: 0.1930 - acc: 0.912 - ETA: 0s - loss: 0.1921 - acc: 0.913 - ETA: 0s - loss: 0.1943 - acc: 0.912 - ETA: 0s - loss: 0.1964 - acc: 0.911 - 1s 50us/step - loss: 0.1970 - acc: 0.9120 - val_loss: 0.8347 - val_acc: 0.8283\n",
      "Epoch 307/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.2236 - acc: 0.890 - ETA: 0s - loss: 0.1647 - acc: 0.921 - ETA: 0s - loss: 0.1820 - acc: 0.911 - ETA: 0s - loss: 0.1860 - acc: 0.911 - ETA: 0s - loss: 0.1846 - acc: 0.912 - ETA: 0s - loss: 0.1861 - acc: 0.913 - ETA: 0s - loss: 0.1874 - acc: 0.913 - ETA: 0s - loss: 0.1909 - acc: 0.912 - ETA: 0s - loss: 0.1929 - acc: 0.911 - ETA: 0s - loss: 0.1906 - acc: 0.912 - ETA: 0s - loss: 0.1918 - acc: 0.912 - ETA: 0s - loss: 0.1922 - acc: 0.912 - ETA: 0s - loss: 0.1933 - acc: 0.912 - ETA: 0s - loss: 0.1938 - acc: 0.913 - ETA: 0s - loss: 0.1953 - acc: 0.912 - ETA: 0s - loss: 0.1971 - acc: 0.911 - ETA: 0s - loss: 0.1988 - acc: 0.911 - ETA: 0s - loss: 0.1987 - acc: 0.911 - 1s 51us/step - loss: 0.1983 - acc: 0.9107 - val_loss: 0.8272 - val_acc: 0.8292\n",
      "Epoch 308/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.1602 - acc: 0.906 - ETA: 0s - loss: 0.2054 - acc: 0.916 - ETA: 0s - loss: 0.1896 - acc: 0.921 - ETA: 0s - loss: 0.1921 - acc: 0.918 - ETA: 0s - loss: 0.1894 - acc: 0.916 - ETA: 0s - loss: 0.1861 - acc: 0.917 - ETA: 0s - loss: 0.1879 - acc: 0.915 - ETA: 0s - loss: 0.1911 - acc: 0.915 - ETA: 0s - loss: 0.1879 - acc: 0.916 - ETA: 0s - loss: 0.1902 - acc: 0.915 - ETA: 0s - loss: 0.1922 - acc: 0.913 - ETA: 0s - loss: 0.1933 - acc: 0.913 - ETA: 0s - loss: 0.1914 - acc: 0.914 - ETA: 0s - loss: 0.1913 - acc: 0.913 - ETA: 0s - loss: 0.1935 - acc: 0.912 - ETA: 0s - loss: 0.1969 - acc: 0.911 - ETA: 0s - loss: 0.1987 - acc: 0.911 - ETA: 0s - loss: 0.2000 - acc: 0.910 - 1s 51us/step - loss: 0.1996 - acc: 0.9110 - val_loss: 0.8199 - val_acc: 0.8167\n",
      "Epoch 309/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.2259 - acc: 0.890 - ETA: 0s - loss: 0.1985 - acc: 0.910 - ETA: 0s - loss: 0.1954 - acc: 0.910 - ETA: 0s - loss: 0.1958 - acc: 0.913 - ETA: 0s - loss: 0.1986 - acc: 0.909 - ETA: 0s - loss: 0.1979 - acc: 0.910 - ETA: 0s - loss: 0.1926 - acc: 0.913 - ETA: 0s - loss: 0.1952 - acc: 0.912 - ETA: 0s - loss: 0.1960 - acc: 0.912 - ETA: 0s - loss: 0.1980 - acc: 0.911 - ETA: 0s - loss: 0.1977 - acc: 0.912 - ETA: 0s - loss: 0.1976 - acc: 0.913 - ETA: 0s - loss: 0.1977 - acc: 0.912 - ETA: 0s - loss: 0.1983 - acc: 0.912 - ETA: 0s - loss: 0.1989 - acc: 0.912 - ETA: 0s - loss: 0.1993 - acc: 0.911 - ETA: 0s - loss: 0.1984 - acc: 0.911 - ETA: 0s - loss: 0.1966 - acc: 0.912 - ETA: 0s - loss: 0.1965 - acc: 0.912 - 1s 51us/step - loss: 0.1963 - acc: 0.9121 - val_loss: 0.8752 - val_acc: 0.8199\n",
      "Epoch 310/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.0998 - acc: 0.937 - ETA: 0s - loss: 0.1759 - acc: 0.919 - ETA: 0s - loss: 0.1762 - acc: 0.921 - ETA: 0s - loss: 0.1786 - acc: 0.919 - ETA: 0s - loss: 0.1767 - acc: 0.919 - ETA: 0s - loss: 0.1767 - acc: 0.918 - ETA: 0s - loss: 0.1810 - acc: 0.915 - ETA: 0s - loss: 0.1852 - acc: 0.914 - ETA: 0s - loss: 0.1884 - acc: 0.913 - ETA: 0s - loss: 0.1869 - acc: 0.914 - ETA: 0s - loss: 0.1921 - acc: 0.912 - ETA: 0s - loss: 0.1912 - acc: 0.913 - ETA: 0s - loss: 0.1920 - acc: 0.912 - ETA: 0s - loss: 0.1934 - acc: 0.912 - ETA: 0s - loss: 0.1940 - acc: 0.911 - ETA: 0s - loss: 0.1931 - acc: 0.911 - ETA: 0s - loss: 0.1946 - acc: 0.911 - ETA: 0s - loss: 0.1954 - acc: 0.911 - 1s 49us/step - loss: 0.1954 - acc: 0.9120 - val_loss: 0.8438 - val_acc: 0.8235\n",
      "Epoch 311/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1380 - acc: 0.921 - ETA: 0s - loss: 0.1860 - acc: 0.904 - ETA: 0s - loss: 0.2040 - acc: 0.905 - ETA: 0s - loss: 0.1949 - acc: 0.910 - ETA: 0s - loss: 0.1876 - acc: 0.914 - ETA: 0s - loss: 0.1901 - acc: 0.911 - ETA: 0s - loss: 0.1927 - acc: 0.913 - ETA: 0s - loss: 0.1968 - acc: 0.912 - ETA: 0s - loss: 0.1965 - acc: 0.913 - ETA: 0s - loss: 0.1959 - acc: 0.912 - ETA: 0s - loss: 0.1960 - acc: 0.911 - ETA: 0s - loss: 0.1943 - acc: 0.913 - ETA: 0s - loss: 0.1947 - acc: 0.913 - ETA: 0s - loss: 0.1970 - acc: 0.912 - ETA: 0s - loss: 0.1972 - acc: 0.912 - ETA: 0s - loss: 0.1980 - acc: 0.911 - ETA: 0s - loss: 0.1971 - acc: 0.911 - ETA: 0s - loss: 0.1983 - acc: 0.911 - 1s 51us/step - loss: 0.1986 - acc: 0.9103 - val_loss: 0.8323 - val_acc: 0.8260\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 312/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.1047 - acc: 0.984 - ETA: 0s - loss: 0.1697 - acc: 0.921 - ETA: 0s - loss: 0.1706 - acc: 0.923 - ETA: 0s - loss: 0.1773 - acc: 0.918 - ETA: 0s - loss: 0.1833 - acc: 0.917 - ETA: 0s - loss: 0.1858 - acc: 0.918 - ETA: 0s - loss: 0.1876 - acc: 0.916 - ETA: 0s - loss: 0.1894 - acc: 0.914 - ETA: 0s - loss: 0.1914 - acc: 0.916 - ETA: 0s - loss: 0.1915 - acc: 0.913 - ETA: 0s - loss: 0.1909 - acc: 0.913 - ETA: 0s - loss: 0.1904 - acc: 0.914 - ETA: 0s - loss: 0.1901 - acc: 0.914 - ETA: 0s - loss: 0.1897 - acc: 0.914 - ETA: 0s - loss: 0.1924 - acc: 0.913 - ETA: 0s - loss: 0.1950 - acc: 0.912 - ETA: 0s - loss: 0.1939 - acc: 0.913 - ETA: 0s - loss: 0.1961 - acc: 0.912 - 1s 50us/step - loss: 0.1973 - acc: 0.9123 - val_loss: 0.9600 - val_acc: 0.8049\n",
      "Epoch 313/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.5567 - acc: 0.812 - ETA: 0s - loss: 0.2231 - acc: 0.907 - ETA: 0s - loss: 0.2086 - acc: 0.911 - ETA: 0s - loss: 0.2059 - acc: 0.914 - ETA: 0s - loss: 0.1975 - acc: 0.915 - ETA: 0s - loss: 0.1982 - acc: 0.914 - ETA: 0s - loss: 0.2010 - acc: 0.913 - ETA: 0s - loss: 0.2020 - acc: 0.912 - ETA: 0s - loss: 0.1994 - acc: 0.912 - ETA: 0s - loss: 0.2009 - acc: 0.912 - ETA: 0s - loss: 0.1989 - acc: 0.912 - ETA: 0s - loss: 0.1981 - acc: 0.912 - ETA: 0s - loss: 0.1982 - acc: 0.912 - ETA: 0s - loss: 0.1982 - acc: 0.912 - ETA: 0s - loss: 0.2010 - acc: 0.911 - ETA: 0s - loss: 0.1998 - acc: 0.911 - ETA: 0s - loss: 0.2003 - acc: 0.910 - ETA: 0s - loss: 0.2021 - acc: 0.910 - 1s 51us/step - loss: 0.2010 - acc: 0.9107 - val_loss: 0.8306 - val_acc: 0.8215\n",
      "Epoch 314/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.3448 - acc: 0.843 - ETA: 0s - loss: 0.1846 - acc: 0.912 - ETA: 0s - loss: 0.2014 - acc: 0.909 - ETA: 0s - loss: 0.1995 - acc: 0.909 - ETA: 0s - loss: 0.1968 - acc: 0.911 - ETA: 0s - loss: 0.1989 - acc: 0.910 - ETA: 0s - loss: 0.2031 - acc: 0.909 - ETA: 0s - loss: 0.2013 - acc: 0.910 - ETA: 0s - loss: 0.2010 - acc: 0.911 - ETA: 0s - loss: 0.1989 - acc: 0.912 - ETA: 0s - loss: 0.1986 - acc: 0.912 - ETA: 0s - loss: 0.1983 - acc: 0.912 - ETA: 0s - loss: 0.1986 - acc: 0.912 - ETA: 0s - loss: 0.1983 - acc: 0.912 - ETA: 0s - loss: 0.1979 - acc: 0.912 - ETA: 0s - loss: 0.1968 - acc: 0.913 - ETA: 0s - loss: 0.1959 - acc: 0.913 - ETA: 0s - loss: 0.1961 - acc: 0.913 - 1s 50us/step - loss: 0.1963 - acc: 0.9131 - val_loss: 0.8605 - val_acc: 0.8234\n",
      "Epoch 315/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.1261 - acc: 0.953 - ETA: 0s - loss: 0.2145 - acc: 0.918 - ETA: 0s - loss: 0.1991 - acc: 0.909 - ETA: 0s - loss: 0.1971 - acc: 0.913 - ETA: 0s - loss: 0.1947 - acc: 0.913 - ETA: 0s - loss: 0.1945 - acc: 0.913 - ETA: 0s - loss: 0.1950 - acc: 0.912 - ETA: 0s - loss: 0.1937 - acc: 0.913 - ETA: 0s - loss: 0.1974 - acc: 0.911 - ETA: 0s - loss: 0.1968 - acc: 0.912 - ETA: 0s - loss: 0.1977 - acc: 0.912 - ETA: 0s - loss: 0.1977 - acc: 0.911 - ETA: 0s - loss: 0.1957 - acc: 0.912 - ETA: 0s - loss: 0.1974 - acc: 0.911 - ETA: 0s - loss: 0.1984 - acc: 0.910 - ETA: 0s - loss: 0.1981 - acc: 0.910 - ETA: 0s - loss: 0.1982 - acc: 0.910 - ETA: 0s - loss: 0.1983 - acc: 0.910 - 1s 50us/step - loss: 0.1986 - acc: 0.9103 - val_loss: 0.8069 - val_acc: 0.8278\n",
      "Epoch 316/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1523 - acc: 0.953 - ETA: 0s - loss: 0.1624 - acc: 0.928 - ETA: 0s - loss: 0.1834 - acc: 0.915 - ETA: 0s - loss: 0.1804 - acc: 0.915 - ETA: 0s - loss: 0.1818 - acc: 0.917 - ETA: 0s - loss: 0.1893 - acc: 0.916 - ETA: 0s - loss: 0.1912 - acc: 0.914 - ETA: 0s - loss: 0.1898 - acc: 0.915 - ETA: 0s - loss: 0.1911 - acc: 0.914 - ETA: 0s - loss: 0.1889 - acc: 0.914 - ETA: 0s - loss: 0.1904 - acc: 0.914 - ETA: 0s - loss: 0.1950 - acc: 0.912 - ETA: 0s - loss: 0.1949 - acc: 0.912 - ETA: 0s - loss: 0.1933 - acc: 0.912 - ETA: 0s - loss: 0.1933 - acc: 0.913 - ETA: 0s - loss: 0.1930 - acc: 0.913 - ETA: 0s - loss: 0.1937 - acc: 0.913 - ETA: 0s - loss: 0.1939 - acc: 0.913 - 1s 50us/step - loss: 0.1945 - acc: 0.9136 - val_loss: 0.8253 - val_acc: 0.8297\n",
      "Epoch 317/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1885 - acc: 0.890 - ETA: 0s - loss: 0.2047 - acc: 0.903 - ETA: 0s - loss: 0.1932 - acc: 0.913 - ETA: 0s - loss: 0.2136 - acc: 0.907 - ETA: 0s - loss: 0.2132 - acc: 0.909 - ETA: 0s - loss: 0.2095 - acc: 0.909 - ETA: 0s - loss: 0.2085 - acc: 0.909 - ETA: 0s - loss: 0.2029 - acc: 0.911 - ETA: 0s - loss: 0.2008 - acc: 0.912 - ETA: 0s - loss: 0.2004 - acc: 0.913 - ETA: 0s - loss: 0.1970 - acc: 0.913 - ETA: 0s - loss: 0.1958 - acc: 0.913 - ETA: 0s - loss: 0.1958 - acc: 0.912 - ETA: 0s - loss: 0.1951 - acc: 0.912 - ETA: 0s - loss: 0.1972 - acc: 0.912 - ETA: 0s - loss: 0.1970 - acc: 0.913 - ETA: 0s - loss: 0.1977 - acc: 0.912 - ETA: 0s - loss: 0.1977 - acc: 0.911 - 1s 50us/step - loss: 0.1989 - acc: 0.9116 - val_loss: 0.8985 - val_acc: 0.8177\n",
      "Epoch 318/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.1662 - acc: 0.953 - ETA: 0s - loss: 0.1718 - acc: 0.919 - ETA: 0s - loss: 0.1737 - acc: 0.918 - ETA: 0s - loss: 0.1822 - acc: 0.913 - ETA: 0s - loss: 0.1835 - acc: 0.914 - ETA: 0s - loss: 0.1875 - acc: 0.916 - ETA: 0s - loss: 0.1905 - acc: 0.916 - ETA: 0s - loss: 0.1916 - acc: 0.915 - ETA: 0s - loss: 0.1897 - acc: 0.915 - ETA: 0s - loss: 0.1874 - acc: 0.915 - ETA: 0s - loss: 0.1902 - acc: 0.914 - ETA: 0s - loss: 0.1912 - acc: 0.913 - ETA: 0s - loss: 0.1931 - acc: 0.912 - ETA: 0s - loss: 0.1965 - acc: 0.911 - ETA: 0s - loss: 0.1990 - acc: 0.910 - ETA: 0s - loss: 0.1981 - acc: 0.910 - ETA: 0s - loss: 0.1991 - acc: 0.911 - 1s 48us/step - loss: 0.1994 - acc: 0.9119 - val_loss: 0.8113 - val_acc: 0.8289\n",
      "Epoch 319/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.2363 - acc: 0.875 - ETA: 0s - loss: 0.1916 - acc: 0.906 - ETA: 0s - loss: 0.1868 - acc: 0.913 - ETA: 0s - loss: 0.1810 - acc: 0.916 - ETA: 0s - loss: 0.1793 - acc: 0.916 - ETA: 0s - loss: 0.1782 - acc: 0.917 - ETA: 0s - loss: 0.1805 - acc: 0.915 - ETA: 0s - loss: 0.1832 - acc: 0.916 - ETA: 0s - loss: 0.1879 - acc: 0.914 - ETA: 0s - loss: 0.1903 - acc: 0.913 - ETA: 0s - loss: 0.1889 - acc: 0.913 - ETA: 0s - loss: 0.1897 - acc: 0.913 - ETA: 0s - loss: 0.1906 - acc: 0.913 - ETA: 0s - loss: 0.1906 - acc: 0.914 - ETA: 0s - loss: 0.1928 - acc: 0.913 - ETA: 0s - loss: 0.1926 - acc: 0.913 - ETA: 0s - loss: 0.1934 - acc: 0.913 - 1s 48us/step - loss: 0.1975 - acc: 0.9130 - val_loss: 0.8467 - val_acc: 0.8210\n",
      "Epoch 320/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1859 - acc: 0.921 - ETA: 0s - loss: 0.1662 - acc: 0.926 - ETA: 0s - loss: 0.1658 - acc: 0.923 - ETA: 0s - loss: 0.1804 - acc: 0.920 - ETA: 0s - loss: 0.1927 - acc: 0.917 - ETA: 0s - loss: 0.2024 - acc: 0.911 - ETA: 0s - loss: 0.1983 - acc: 0.912 - ETA: 0s - loss: 0.1962 - acc: 0.913 - ETA: 0s - loss: 0.1967 - acc: 0.913 - ETA: 0s - loss: 0.1958 - acc: 0.913 - ETA: 0s - loss: 0.1951 - acc: 0.915 - ETA: 0s - loss: 0.1962 - acc: 0.915 - ETA: 0s - loss: 0.1960 - acc: 0.914 - ETA: 0s - loss: 0.1940 - acc: 0.915 - ETA: 0s - loss: 0.1938 - acc: 0.915 - ETA: 0s - loss: 0.1947 - acc: 0.914 - ETA: 0s - loss: 0.1958 - acc: 0.913 - ETA: 0s - loss: 0.1966 - acc: 0.912 - 1s 49us/step - loss: 0.1960 - acc: 0.9127 - val_loss: 0.8825 - val_acc: 0.8203\n",
      "Epoch 321/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1945 - acc: 0.890 - ETA: 0s - loss: 0.1670 - acc: 0.926 - ETA: 0s - loss: 0.1900 - acc: 0.923 - ETA: 0s - loss: 0.1973 - acc: 0.912 - ETA: 0s - loss: 0.1971 - acc: 0.909 - ETA: 0s - loss: 0.1977 - acc: 0.912 - ETA: 0s - loss: 0.1943 - acc: 0.914 - ETA: 0s - loss: 0.1952 - acc: 0.914 - ETA: 0s - loss: 0.1950 - acc: 0.914 - ETA: 0s - loss: 0.1955 - acc: 0.913 - ETA: 0s - loss: 0.1968 - acc: 0.913 - ETA: 0s - loss: 0.1991 - acc: 0.913 - ETA: 0s - loss: 0.1981 - acc: 0.913 - ETA: 0s - loss: 0.2008 - acc: 0.912 - ETA: 0s - loss: 0.1995 - acc: 0.912 - ETA: 0s - loss: 0.1978 - acc: 0.912 - ETA: 0s - loss: 0.1973 - acc: 0.913 - ETA: 0s - loss: 0.1972 - acc: 0.912 - 1s 50us/step - loss: 0.1987 - acc: 0.9123 - val_loss: 0.8601 - val_acc: 0.8186\n",
      "Epoch 322/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.1547 - acc: 0.921 - ETA: 0s - loss: 0.1758 - acc: 0.909 - ETA: 0s - loss: 0.1857 - acc: 0.916 - ETA: 0s - loss: 0.1880 - acc: 0.913 - ETA: 0s - loss: 0.1851 - acc: 0.916 - ETA: 0s - loss: 0.1833 - acc: 0.915 - ETA: 0s - loss: 0.1853 - acc: 0.914 - ETA: 0s - loss: 0.1901 - acc: 0.912 - ETA: 0s - loss: 0.1914 - acc: 0.911 - ETA: 0s - loss: 0.1915 - acc: 0.912 - ETA: 0s - loss: 0.1905 - acc: 0.912 - ETA: 0s - loss: 0.1954 - acc: 0.910 - ETA: 0s - loss: 0.1963 - acc: 0.910 - ETA: 0s - loss: 0.1946 - acc: 0.911 - ETA: 0s - loss: 0.1943 - acc: 0.912 - ETA: 0s - loss: 0.1933 - acc: 0.912 - ETA: 0s - loss: 0.1943 - acc: 0.912 - ETA: 0s - loss: 0.1957 - acc: 0.912 - 1s 49us/step - loss: 0.1949 - acc: 0.9124 - val_loss: 0.8783 - val_acc: 0.8268\n",
      "Epoch 323/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.2220 - acc: 0.937 - ETA: 0s - loss: 0.2202 - acc: 0.911 - ETA: 0s - loss: 0.2057 - acc: 0.911 - ETA: 0s - loss: 0.2075 - acc: 0.910 - ETA: 0s - loss: 0.2016 - acc: 0.913 - ETA: 0s - loss: 0.2003 - acc: 0.914 - ETA: 0s - loss: 0.1952 - acc: 0.915 - ETA: 0s - loss: 0.1932 - acc: 0.917 - ETA: 0s - loss: 0.1918 - acc: 0.916 - ETA: 0s - loss: 0.1883 - acc: 0.918 - ETA: 0s - loss: 0.1892 - acc: 0.917 - ETA: 0s - loss: 0.1912 - acc: 0.916 - ETA: 0s - loss: 0.1913 - acc: 0.916 - ETA: 0s - loss: 0.1915 - acc: 0.915 - ETA: 0s - loss: 0.1946 - acc: 0.914 - ETA: 0s - loss: 0.1946 - acc: 0.914 - ETA: 0s - loss: 0.1953 - acc: 0.913 - ETA: 0s - loss: 0.1957 - acc: 0.913 - 1s 50us/step - loss: 0.1955 - acc: 0.9135 - val_loss: 0.8804 - val_acc: 0.8203\n",
      "Epoch 324/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.2529 - acc: 0.906 - ETA: 0s - loss: 0.1888 - acc: 0.911 - ETA: 0s - loss: 0.1878 - acc: 0.912 - ETA: 0s - loss: 0.1928 - acc: 0.913 - ETA: 0s - loss: 0.1921 - acc: 0.912 - ETA: 0s - loss: 0.1900 - acc: 0.913 - ETA: 0s - loss: 0.1939 - acc: 0.912 - ETA: 0s - loss: 0.1971 - acc: 0.909 - ETA: 0s - loss: 0.1932 - acc: 0.911 - ETA: 0s - loss: 0.1949 - acc: 0.912 - ETA: 0s - loss: 0.1953 - acc: 0.911 - ETA: 0s - loss: 0.1963 - acc: 0.911 - ETA: 0s - loss: 0.1958 - acc: 0.911 - ETA: 0s - loss: 0.1976 - acc: 0.910 - ETA: 0s - loss: 0.1969 - acc: 0.911 - ETA: 0s - loss: 0.1954 - acc: 0.911 - ETA: 0s - loss: 0.1950 - acc: 0.912 - ETA: 0s - loss: 0.1958 - acc: 0.912 - 1s 50us/step - loss: 0.1954 - acc: 0.9123 - val_loss: 0.9563 - val_acc: 0.8204\n",
      "Epoch 325/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.4397 - acc: 0.921 - ETA: 0s - loss: 0.1945 - acc: 0.928 - ETA: 0s - loss: 0.1970 - acc: 0.920 - ETA: 0s - loss: 0.1991 - acc: 0.913 - ETA: 0s - loss: 0.1959 - acc: 0.915 - ETA: 0s - loss: 0.1949 - acc: 0.914 - ETA: 0s - loss: 0.1998 - acc: 0.913 - ETA: 0s - loss: 0.1989 - acc: 0.913 - ETA: 0s - loss: 0.1994 - acc: 0.912 - ETA: 0s - loss: 0.1984 - acc: 0.913 - ETA: 0s - loss: 0.1981 - acc: 0.912 - ETA: 0s - loss: 0.1986 - acc: 0.913 - ETA: 0s - loss: 0.1973 - acc: 0.913 - ETA: 0s - loss: 0.1962 - acc: 0.914 - ETA: 0s - loss: 0.1953 - acc: 0.914 - ETA: 0s - loss: 0.1953 - acc: 0.914 - ETA: 0s - loss: 0.1957 - acc: 0.913 - ETA: 0s - loss: 0.1977 - acc: 0.913 - 1s 50us/step - loss: 0.1969 - acc: 0.9132 - val_loss: 0.8350 - val_acc: 0.8287\n",
      "Epoch 326/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.1843 - acc: 0.937 - ETA: 0s - loss: 0.1885 - acc: 0.921 - ETA: 0s - loss: 0.1875 - acc: 0.914 - ETA: 0s - loss: 0.1800 - acc: 0.919 - ETA: 0s - loss: 0.1882 - acc: 0.915 - ETA: 0s - loss: 0.1849 - acc: 0.915 - ETA: 0s - loss: 0.1869 - acc: 0.915 - ETA: 0s - loss: 0.1870 - acc: 0.914 - ETA: 0s - loss: 0.1890 - acc: 0.913 - ETA: 0s - loss: 0.1905 - acc: 0.912 - ETA: 0s - loss: 0.1901 - acc: 0.912 - ETA: 0s - loss: 0.1935 - acc: 0.911 - ETA: 0s - loss: 0.1937 - acc: 0.911 - ETA: 0s - loss: 0.1929 - acc: 0.911 - ETA: 0s - loss: 0.1929 - acc: 0.911 - ETA: 0s - loss: 0.1949 - acc: 0.910 - ETA: 0s - loss: 0.1946 - acc: 0.911 - ETA: 0s - loss: 0.1937 - acc: 0.911 - 1s 50us/step - loss: 0.1938 - acc: 0.9118 - val_loss: 0.8439 - val_acc: 0.8295\n",
      "Epoch 327/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.2131 - acc: 0.890 - ETA: 0s - loss: 0.1900 - acc: 0.912 - ETA: 0s - loss: 0.1923 - acc: 0.914 - ETA: 0s - loss: 0.1900 - acc: 0.913 - ETA: 0s - loss: 0.1919 - acc: 0.910 - ETA: 0s - loss: 0.1919 - acc: 0.911 - ETA: 0s - loss: 0.1934 - acc: 0.910 - ETA: 0s - loss: 0.1898 - acc: 0.911 - ETA: 0s - loss: 0.1913 - acc: 0.912 - ETA: 0s - loss: 0.1932 - acc: 0.912 - ETA: 0s - loss: 0.1945 - acc: 0.911 - ETA: 0s - loss: 0.1949 - acc: 0.911 - ETA: 0s - loss: 0.1941 - acc: 0.912 - ETA: 0s - loss: 0.1951 - acc: 0.912 - ETA: 0s - loss: 0.1951 - acc: 0.913 - ETA: 0s - loss: 0.1945 - acc: 0.913 - ETA: 0s - loss: 0.1963 - acc: 0.912 - ETA: 0s - loss: 0.1976 - acc: 0.912 - 1s 49us/step - loss: 0.1983 - acc: 0.9122 - val_loss: 0.8478 - val_acc: 0.8289\n",
      "Epoch 328/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.2833 - acc: 0.906 - ETA: 0s - loss: 0.1824 - acc: 0.920 - ETA: 0s - loss: 0.1721 - acc: 0.923 - ETA: 0s - loss: 0.1769 - acc: 0.922 - ETA: 0s - loss: 0.1899 - acc: 0.916 - ETA: 0s - loss: 0.1917 - acc: 0.916 - ETA: 0s - loss: 0.1912 - acc: 0.917 - ETA: 0s - loss: 0.1932 - acc: 0.915 - ETA: 0s - loss: 0.1927 - acc: 0.914 - ETA: 0s - loss: 0.1972 - acc: 0.913 - ETA: 0s - loss: 0.1949 - acc: 0.914 - ETA: 0s - loss: 0.1963 - acc: 0.914 - ETA: 0s - loss: 0.1953 - acc: 0.914 - ETA: 0s - loss: 0.1991 - acc: 0.912 - ETA: 0s - loss: 0.1985 - acc: 0.913 - ETA: 0s - loss: 0.1987 - acc: 0.912 - ETA: 0s - loss: 0.1973 - acc: 0.912 - ETA: 0s - loss: 0.1985 - acc: 0.912 - 1s 50us/step - loss: 0.1980 - acc: 0.9128 - val_loss: 0.8514 - val_acc: 0.8270\n",
      "Epoch 329/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.1105 - acc: 0.921 - ETA: 0s - loss: 0.1897 - acc: 0.918 - ETA: 0s - loss: 0.1916 - acc: 0.915 - ETA: 0s - loss: 0.1858 - acc: 0.915 - ETA: 0s - loss: 0.1849 - acc: 0.915 - ETA: 0s - loss: 0.1858 - acc: 0.914 - ETA: 0s - loss: 0.1875 - acc: 0.912 - ETA: 0s - loss: 0.1877 - acc: 0.913 - ETA: 0s - loss: 0.1878 - acc: 0.914 - ETA: 0s - loss: 0.1876 - acc: 0.915 - ETA: 0s - loss: 0.1895 - acc: 0.914 - ETA: 0s - loss: 0.1905 - acc: 0.913 - ETA: 0s - loss: 0.1910 - acc: 0.912 - ETA: 0s - loss: 0.1952 - acc: 0.912 - ETA: 0s - loss: 0.1954 - acc: 0.911 - ETA: 0s - loss: 0.1941 - acc: 0.912 - ETA: 0s - loss: 0.1940 - acc: 0.912 - ETA: 0s - loss: 0.1940 - acc: 0.912 - 1s 49us/step - loss: 0.1955 - acc: 0.9117 - val_loss: 0.8471 - val_acc: 0.8223\n",
      "Epoch 330/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.1800 - acc: 0.906 - ETA: 0s - loss: 0.1819 - acc: 0.920 - ETA: 0s - loss: 0.1854 - acc: 0.920 - ETA: 0s - loss: 0.1808 - acc: 0.919 - ETA: 0s - loss: 0.1835 - acc: 0.915 - ETA: 0s - loss: 0.1870 - acc: 0.914 - ETA: 0s - loss: 0.1857 - acc: 0.916 - ETA: 0s - loss: 0.1866 - acc: 0.915 - ETA: 0s - loss: 0.1889 - acc: 0.914 - ETA: 0s - loss: 0.1909 - acc: 0.912 - ETA: 0s - loss: 0.1936 - acc: 0.912 - ETA: 0s - loss: 0.1945 - acc: 0.911 - ETA: 0s - loss: 0.1944 - acc: 0.911 - ETA: 0s - loss: 0.1939 - acc: 0.911 - ETA: 0s - loss: 0.1952 - acc: 0.910 - ETA: 0s - loss: 0.1948 - acc: 0.911 - ETA: 0s - loss: 0.1932 - acc: 0.912 - ETA: 0s - loss: 0.1932 - acc: 0.912 - 1s 50us/step - loss: 0.1939 - acc: 0.9121 - val_loss: 0.8471 - val_acc: 0.8281\n",
      "Epoch 331/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.2507 - acc: 0.875 - ETA: 0s - loss: 0.1731 - acc: 0.917 - ETA: 0s - loss: 0.1959 - acc: 0.907 - ETA: 0s - loss: 0.1925 - acc: 0.910 - ETA: 0s - loss: 0.1931 - acc: 0.913 - ETA: 0s - loss: 0.1911 - acc: 0.912 - ETA: 0s - loss: 0.1897 - acc: 0.914 - ETA: 0s - loss: 0.1915 - acc: 0.914 - ETA: 0s - loss: 0.1924 - acc: 0.913 - ETA: 0s - loss: 0.1903 - acc: 0.913 - ETA: 0s - loss: 0.1924 - acc: 0.912 - ETA: 0s - loss: 0.1930 - acc: 0.912 - ETA: 0s - loss: 0.1923 - acc: 0.912 - ETA: 0s - loss: 0.1916 - acc: 0.913 - ETA: 0s - loss: 0.1925 - acc: 0.912 - ETA: 0s - loss: 0.1927 - acc: 0.913 - ETA: 0s - loss: 0.1932 - acc: 0.913 - ETA: 0s - loss: 0.1943 - acc: 0.912 - 1s 50us/step - loss: 0.1927 - acc: 0.9134 - val_loss: 0.8412 - val_acc: 0.8289\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 332/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.1455 - acc: 0.968 - ETA: 0s - loss: 0.1810 - acc: 0.927 - ETA: 0s - loss: 0.1848 - acc: 0.919 - ETA: 0s - loss: 0.1856 - acc: 0.918 - ETA: 0s - loss: 0.1874 - acc: 0.915 - ETA: 0s - loss: 0.1812 - acc: 0.918 - ETA: 0s - loss: 0.1834 - acc: 0.918 - ETA: 0s - loss: 0.1877 - acc: 0.916 - ETA: 0s - loss: 0.1911 - acc: 0.913 - ETA: 0s - loss: 0.1961 - acc: 0.912 - ETA: 0s - loss: 0.1976 - acc: 0.912 - ETA: 0s - loss: 0.1978 - acc: 0.912 - ETA: 0s - loss: 0.1994 - acc: 0.911 - ETA: 0s - loss: 0.1988 - acc: 0.911 - ETA: 0s - loss: 0.1968 - acc: 0.913 - ETA: 0s - loss: 0.1970 - acc: 0.912 - ETA: 0s - loss: 0.1971 - acc: 0.912 - ETA: 0s - loss: 0.1970 - acc: 0.912 - 1s 49us/step - loss: 0.1975 - acc: 0.9129 - val_loss: 0.8566 - val_acc: 0.8245\n",
      "Epoch 333/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.2765 - acc: 0.890 - ETA: 0s - loss: 0.1719 - acc: 0.925 - ETA: 0s - loss: 0.1966 - acc: 0.907 - ETA: 0s - loss: 0.1953 - acc: 0.909 - ETA: 0s - loss: 0.1957 - acc: 0.910 - ETA: 0s - loss: 0.1946 - acc: 0.914 - ETA: 0s - loss: 0.2023 - acc: 0.912 - ETA: 0s - loss: 0.1980 - acc: 0.914 - ETA: 0s - loss: 0.1969 - acc: 0.913 - ETA: 0s - loss: 0.1966 - acc: 0.914 - ETA: 0s - loss: 0.1967 - acc: 0.915 - ETA: 0s - loss: 0.1981 - acc: 0.914 - ETA: 0s - loss: 0.2000 - acc: 0.914 - ETA: 0s - loss: 0.1993 - acc: 0.914 - ETA: 0s - loss: 0.2010 - acc: 0.912 - ETA: 0s - loss: 0.1992 - acc: 0.913 - ETA: 0s - loss: 0.1978 - acc: 0.914 - ETA: 0s - loss: 0.1982 - acc: 0.914 - 1s 50us/step - loss: 0.1969 - acc: 0.9150 - val_loss: 0.8782 - val_acc: 0.8244\n",
      "Epoch 334/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.2442 - acc: 0.921 - ETA: 0s - loss: 0.2022 - acc: 0.915 - ETA: 0s - loss: 0.2005 - acc: 0.915 - ETA: 0s - loss: 0.2012 - acc: 0.910 - ETA: 0s - loss: 0.1951 - acc: 0.915 - ETA: 0s - loss: 0.1915 - acc: 0.915 - ETA: 0s - loss: 0.1911 - acc: 0.916 - ETA: 0s - loss: 0.1937 - acc: 0.915 - ETA: 0s - loss: 0.1921 - acc: 0.915 - ETA: 0s - loss: 0.1912 - acc: 0.915 - ETA: 0s - loss: 0.1937 - acc: 0.914 - ETA: 0s - loss: 0.1920 - acc: 0.914 - ETA: 0s - loss: 0.1922 - acc: 0.914 - ETA: 0s - loss: 0.1907 - acc: 0.915 - ETA: 0s - loss: 0.1923 - acc: 0.914 - ETA: 0s - loss: 0.1914 - acc: 0.915 - ETA: 0s - loss: 0.1936 - acc: 0.914 - ETA: 0s - loss: 0.1941 - acc: 0.914 - 1s 50us/step - loss: 0.1945 - acc: 0.9139 - val_loss: 0.8691 - val_acc: 0.8230\n",
      "Epoch 335/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.2103 - acc: 0.937 - ETA: 0s - loss: 0.1964 - acc: 0.918 - ETA: 0s - loss: 0.1951 - acc: 0.916 - ETA: 0s - loss: 0.2002 - acc: 0.914 - ETA: 0s - loss: 0.1992 - acc: 0.912 - ETA: 0s - loss: 0.1991 - acc: 0.914 - ETA: 0s - loss: 0.1980 - acc: 0.915 - ETA: 0s - loss: 0.1963 - acc: 0.916 - ETA: 0s - loss: 0.1952 - acc: 0.916 - ETA: 0s - loss: 0.1960 - acc: 0.916 - ETA: 0s - loss: 0.1956 - acc: 0.916 - ETA: 0s - loss: 0.1950 - acc: 0.916 - ETA: 0s - loss: 0.1949 - acc: 0.914 - ETA: 0s - loss: 0.1955 - acc: 0.914 - ETA: 0s - loss: 0.1942 - acc: 0.914 - ETA: 0s - loss: 0.1948 - acc: 0.913 - ETA: 0s - loss: 0.1950 - acc: 0.913 - ETA: 0s - loss: 0.1960 - acc: 0.912 - 1s 49us/step - loss: 0.1951 - acc: 0.9127 - val_loss: 0.8600 - val_acc: 0.8259\n",
      "Epoch 336/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.0842 - acc: 0.968 - ETA: 0s - loss: 0.1750 - acc: 0.929 - ETA: 0s - loss: 0.1762 - acc: 0.921 - ETA: 0s - loss: 0.1766 - acc: 0.921 - ETA: 0s - loss: 0.1850 - acc: 0.917 - ETA: 0s - loss: 0.1857 - acc: 0.917 - ETA: 0s - loss: 0.1895 - acc: 0.914 - ETA: 0s - loss: 0.1929 - acc: 0.912 - ETA: 0s - loss: 0.1942 - acc: 0.911 - ETA: 0s - loss: 0.1939 - acc: 0.911 - ETA: 0s - loss: 0.1963 - acc: 0.912 - ETA: 0s - loss: 0.1998 - acc: 0.911 - ETA: 0s - loss: 0.1981 - acc: 0.912 - ETA: 0s - loss: 0.2007 - acc: 0.911 - ETA: 0s - loss: 0.1991 - acc: 0.912 - ETA: 0s - loss: 0.1983 - acc: 0.912 - ETA: 0s - loss: 0.1970 - acc: 0.913 - ETA: 0s - loss: 0.1966 - acc: 0.912 - 1s 49us/step - loss: 0.1972 - acc: 0.9125 - val_loss: 0.8467 - val_acc: 0.8202\n",
      "Epoch 337/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.2293 - acc: 0.875 - ETA: 0s - loss: 0.1873 - acc: 0.914 - ETA: 0s - loss: 0.1870 - acc: 0.911 - ETA: 0s - loss: 0.1980 - acc: 0.908 - ETA: 0s - loss: 0.1959 - acc: 0.907 - ETA: 0s - loss: 0.1961 - acc: 0.909 - ETA: 0s - loss: 0.1991 - acc: 0.907 - ETA: 0s - loss: 0.2007 - acc: 0.908 - ETA: 0s - loss: 0.1992 - acc: 0.910 - ETA: 0s - loss: 0.1948 - acc: 0.912 - ETA: 0s - loss: 0.1944 - acc: 0.912 - ETA: 0s - loss: 0.1952 - acc: 0.911 - ETA: 0s - loss: 0.1953 - acc: 0.911 - ETA: 0s - loss: 0.1931 - acc: 0.912 - ETA: 0s - loss: 0.1933 - acc: 0.913 - ETA: 0s - loss: 0.1915 - acc: 0.913 - ETA: 0s - loss: 0.1925 - acc: 0.913 - ETA: 0s - loss: 0.1939 - acc: 0.913 - 1s 49us/step - loss: 0.1946 - acc: 0.9129 - val_loss: 0.9140 - val_acc: 0.8145\n",
      "Epoch 338/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.2469 - acc: 0.890 - ETA: 0s - loss: 0.2031 - acc: 0.911 - ETA: 0s - loss: 0.1996 - acc: 0.913 - ETA: 0s - loss: 0.2035 - acc: 0.912 - ETA: 0s - loss: 0.2006 - acc: 0.912 - ETA: 0s - loss: 0.1955 - acc: 0.911 - ETA: 0s - loss: 0.1949 - acc: 0.912 - ETA: 0s - loss: 0.1927 - acc: 0.913 - ETA: 0s - loss: 0.1905 - acc: 0.913 - ETA: 0s - loss: 0.1921 - acc: 0.914 - ETA: 0s - loss: 0.1925 - acc: 0.914 - ETA: 0s - loss: 0.1919 - acc: 0.914 - ETA: 0s - loss: 0.1915 - acc: 0.913 - ETA: 0s - loss: 0.1911 - acc: 0.914 - ETA: 0s - loss: 0.1919 - acc: 0.913 - ETA: 0s - loss: 0.1931 - acc: 0.913 - ETA: 0s - loss: 0.1931 - acc: 0.913 - ETA: 0s - loss: 0.1949 - acc: 0.912 - 1s 49us/step - loss: 0.1946 - acc: 0.9123 - val_loss: 0.8784 - val_acc: 0.8265\n",
      "Epoch 339/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.1368 - acc: 0.953 - ETA: 0s - loss: 0.1931 - acc: 0.923 - ETA: 0s - loss: 0.1986 - acc: 0.919 - ETA: 0s - loss: 0.1913 - acc: 0.920 - ETA: 0s - loss: 0.1837 - acc: 0.922 - ETA: 0s - loss: 0.1837 - acc: 0.922 - ETA: 0s - loss: 0.1901 - acc: 0.919 - ETA: 0s - loss: 0.1886 - acc: 0.919 - ETA: 0s - loss: 0.1912 - acc: 0.917 - ETA: 0s - loss: 0.1934 - acc: 0.916 - ETA: 0s - loss: 0.1937 - acc: 0.915 - ETA: 0s - loss: 0.1953 - acc: 0.914 - ETA: 0s - loss: 0.1969 - acc: 0.913 - ETA: 0s - loss: 0.1973 - acc: 0.912 - ETA: 0s - loss: 0.1975 - acc: 0.912 - ETA: 0s - loss: 0.1959 - acc: 0.912 - ETA: 0s - loss: 0.1957 - acc: 0.912 - ETA: 0s - loss: 0.1971 - acc: 0.911 - 1s 50us/step - loss: 0.1963 - acc: 0.9121 - val_loss: 0.8642 - val_acc: 0.8205\n",
      "Epoch 340/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.1287 - acc: 0.937 - ETA: 0s - loss: 0.1699 - acc: 0.918 - ETA: 0s - loss: 0.1772 - acc: 0.920 - ETA: 0s - loss: 0.1745 - acc: 0.922 - ETA: 0s - loss: 0.1802 - acc: 0.921 - ETA: 0s - loss: 0.1812 - acc: 0.922 - ETA: 0s - loss: 0.1834 - acc: 0.920 - ETA: 0s - loss: 0.1891 - acc: 0.920 - ETA: 0s - loss: 0.1885 - acc: 0.918 - ETA: 0s - loss: 0.1884 - acc: 0.918 - ETA: 0s - loss: 0.1895 - acc: 0.917 - ETA: 0s - loss: 0.1918 - acc: 0.917 - ETA: 0s - loss: 0.1927 - acc: 0.915 - ETA: 0s - loss: 0.1915 - acc: 0.916 - ETA: 0s - loss: 0.1939 - acc: 0.914 - ETA: 0s - loss: 0.1944 - acc: 0.914 - ETA: 0s - loss: 0.1963 - acc: 0.913 - ETA: 0s - loss: 0.1984 - acc: 0.913 - 1s 49us/step - loss: 0.1982 - acc: 0.9133 - val_loss: 0.8447 - val_acc: 0.8252\n",
      "Epoch 341/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1669 - acc: 0.921 - ETA: 0s - loss: 0.1703 - acc: 0.918 - ETA: 0s - loss: 0.1717 - acc: 0.918 - ETA: 0s - loss: 0.1813 - acc: 0.918 - ETA: 0s - loss: 0.1814 - acc: 0.918 - ETA: 0s - loss: 0.1902 - acc: 0.915 - ETA: 0s - loss: 0.1887 - acc: 0.915 - ETA: 0s - loss: 0.1889 - acc: 0.914 - ETA: 0s - loss: 0.1894 - acc: 0.914 - ETA: 0s - loss: 0.1914 - acc: 0.914 - ETA: 0s - loss: 0.1924 - acc: 0.914 - ETA: 0s - loss: 0.1918 - acc: 0.914 - ETA: 0s - loss: 0.1919 - acc: 0.914 - ETA: 0s - loss: 0.1916 - acc: 0.913 - ETA: 0s - loss: 0.1896 - acc: 0.914 - ETA: 0s - loss: 0.1907 - acc: 0.914 - ETA: 0s - loss: 0.1918 - acc: 0.913 - ETA: 0s - loss: 0.1946 - acc: 0.913 - 1s 49us/step - loss: 0.1945 - acc: 0.9135 - val_loss: 0.8729 - val_acc: 0.8259\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 342/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.2216 - acc: 0.859 - ETA: 0s - loss: 0.1718 - acc: 0.920 - ETA: 0s - loss: 0.1879 - acc: 0.916 - ETA: 0s - loss: 0.1804 - acc: 0.915 - ETA: 0s - loss: 0.1798 - acc: 0.915 - ETA: 0s - loss: 0.1851 - acc: 0.913 - ETA: 0s - loss: 0.1847 - acc: 0.914 - ETA: 0s - loss: 0.1847 - acc: 0.915 - ETA: 0s - loss: 0.1871 - acc: 0.914 - ETA: 0s - loss: 0.1877 - acc: 0.914 - ETA: 0s - loss: 0.1865 - acc: 0.915 - ETA: 0s - loss: 0.1905 - acc: 0.913 - ETA: 0s - loss: 0.1948 - acc: 0.912 - ETA: 0s - loss: 0.1953 - acc: 0.912 - ETA: 0s - loss: 0.1938 - acc: 0.912 - ETA: 0s - loss: 0.1965 - acc: 0.912 - ETA: 0s - loss: 0.1963 - acc: 0.911 - ETA: 0s - loss: 0.1970 - acc: 0.911 - 1s 49us/step - loss: 0.1970 - acc: 0.9117 - val_loss: 0.8512 - val_acc: 0.8277\n",
      "Epoch 343/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.2466 - acc: 0.875 - ETA: 0s - loss: 0.1904 - acc: 0.917 - ETA: 0s - loss: 0.1779 - acc: 0.923 - ETA: 0s - loss: 0.1715 - acc: 0.927 - ETA: 0s - loss: 0.1748 - acc: 0.921 - ETA: 0s - loss: 0.1805 - acc: 0.919 - ETA: 0s - loss: 0.1844 - acc: 0.917 - ETA: 0s - loss: 0.1884 - acc: 0.916 - ETA: 0s - loss: 0.1847 - acc: 0.917 - ETA: 0s - loss: 0.1872 - acc: 0.916 - ETA: 0s - loss: 0.1920 - acc: 0.914 - ETA: 0s - loss: 0.1933 - acc: 0.914 - ETA: 0s - loss: 0.1952 - acc: 0.913 - ETA: 0s - loss: 0.1947 - acc: 0.914 - ETA: 0s - loss: 0.1961 - acc: 0.914 - ETA: 0s - loss: 0.1966 - acc: 0.913 - ETA: 0s - loss: 0.1965 - acc: 0.913 - ETA: 0s - loss: 0.1957 - acc: 0.914 - 1s 50us/step - loss: 0.1954 - acc: 0.9145 - val_loss: 0.8390 - val_acc: 0.8295\n",
      "Epoch 344/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.1350 - acc: 0.937 - ETA: 0s - loss: 0.1704 - acc: 0.921 - ETA: 0s - loss: 0.1877 - acc: 0.914 - ETA: 0s - loss: 0.2009 - acc: 0.910 - ETA: 0s - loss: 0.1943 - acc: 0.913 - ETA: 0s - loss: 0.1947 - acc: 0.913 - ETA: 0s - loss: 0.1998 - acc: 0.910 - ETA: 0s - loss: 0.1963 - acc: 0.913 - ETA: 0s - loss: 0.1944 - acc: 0.914 - ETA: 0s - loss: 0.1944 - acc: 0.915 - ETA: 0s - loss: 0.1925 - acc: 0.915 - ETA: 0s - loss: 0.1939 - acc: 0.914 - ETA: 0s - loss: 0.1945 - acc: 0.914 - ETA: 0s - loss: 0.1939 - acc: 0.914 - ETA: 0s - loss: 0.1929 - acc: 0.915 - ETA: 0s - loss: 0.1933 - acc: 0.914 - ETA: 0s - loss: 0.1976 - acc: 0.914 - ETA: 0s - loss: 0.1980 - acc: 0.913 - 1s 50us/step - loss: 0.1986 - acc: 0.9131 - val_loss: 0.8582 - val_acc: 0.8212\n",
      "Epoch 345/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.2290 - acc: 0.890 - ETA: 0s - loss: 0.1875 - acc: 0.916 - ETA: 0s - loss: 0.1761 - acc: 0.921 - ETA: 0s - loss: 0.1787 - acc: 0.917 - ETA: 0s - loss: 0.1841 - acc: 0.914 - ETA: 0s - loss: 0.1874 - acc: 0.913 - ETA: 0s - loss: 0.1885 - acc: 0.914 - ETA: 0s - loss: 0.1932 - acc: 0.912 - ETA: 0s - loss: 0.1948 - acc: 0.912 - ETA: 0s - loss: 0.1941 - acc: 0.912 - ETA: 0s - loss: 0.1929 - acc: 0.913 - ETA: 0s - loss: 0.1902 - acc: 0.914 - ETA: 0s - loss: 0.1905 - acc: 0.914 - ETA: 0s - loss: 0.1931 - acc: 0.913 - ETA: 0s - loss: 0.1939 - acc: 0.913 - ETA: 0s - loss: 0.1946 - acc: 0.913 - ETA: 0s - loss: 0.1936 - acc: 0.914 - ETA: 0s - loss: 0.1935 - acc: 0.914 - 1s 50us/step - loss: 0.1939 - acc: 0.9143 - val_loss: 0.8292 - val_acc: 0.8287\n",
      "Epoch 346/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.1871 - acc: 0.906 - ETA: 0s - loss: 0.1905 - acc: 0.915 - ETA: 0s - loss: 0.2004 - acc: 0.909 - ETA: 0s - loss: 0.1891 - acc: 0.915 - ETA: 0s - loss: 0.1907 - acc: 0.913 - ETA: 0s - loss: 0.1993 - acc: 0.913 - ETA: 0s - loss: 0.1969 - acc: 0.914 - ETA: 0s - loss: 0.1969 - acc: 0.913 - ETA: 0s - loss: 0.1976 - acc: 0.913 - ETA: 0s - loss: 0.1967 - acc: 0.913 - ETA: 0s - loss: 0.1950 - acc: 0.912 - ETA: 0s - loss: 0.1965 - acc: 0.911 - ETA: 0s - loss: 0.1973 - acc: 0.912 - ETA: 0s - loss: 0.1989 - acc: 0.911 - ETA: 0s - loss: 0.1981 - acc: 0.911 - ETA: 0s - loss: 0.1993 - acc: 0.912 - ETA: 0s - loss: 0.1993 - acc: 0.911 - ETA: 0s - loss: 0.1983 - acc: 0.912 - 1s 51us/step - loss: 0.1993 - acc: 0.9120 - val_loss: 0.8427 - val_acc: 0.8271\n",
      "Epoch 347/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.1190 - acc: 0.953 - ETA: 0s - loss: 0.1737 - acc: 0.919 - ETA: 0s - loss: 0.1816 - acc: 0.921 - ETA: 0s - loss: 0.1863 - acc: 0.916 - ETA: 0s - loss: 0.1921 - acc: 0.915 - ETA: 0s - loss: 0.1885 - acc: 0.915 - ETA: 0s - loss: 0.1953 - acc: 0.913 - ETA: 0s - loss: 0.1970 - acc: 0.912 - ETA: 0s - loss: 0.1962 - acc: 0.913 - ETA: 0s - loss: 0.1920 - acc: 0.915 - ETA: 0s - loss: 0.1938 - acc: 0.914 - ETA: 0s - loss: 0.1935 - acc: 0.913 - ETA: 0s - loss: 0.1945 - acc: 0.913 - ETA: 0s - loss: 0.1931 - acc: 0.913 - ETA: 0s - loss: 0.1950 - acc: 0.912 - ETA: 0s - loss: 0.1951 - acc: 0.912 - ETA: 0s - loss: 0.1944 - acc: 0.913 - ETA: 0s - loss: 0.1943 - acc: 0.913 - 1s 49us/step - loss: 0.1940 - acc: 0.9132 - val_loss: 0.8723 - val_acc: 0.8281\n",
      "Epoch 348/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.1909 - acc: 0.906 - ETA: 0s - loss: 0.2057 - acc: 0.924 - ETA: 0s - loss: 0.1983 - acc: 0.917 - ETA: 0s - loss: 0.2087 - acc: 0.910 - ETA: 0s - loss: 0.2008 - acc: 0.912 - ETA: 0s - loss: 0.2022 - acc: 0.912 - ETA: 0s - loss: 0.1983 - acc: 0.913 - ETA: 0s - loss: 0.2044 - acc: 0.912 - ETA: 0s - loss: 0.2025 - acc: 0.912 - ETA: 0s - loss: 0.1981 - acc: 0.913 - ETA: 0s - loss: 0.1957 - acc: 0.914 - ETA: 0s - loss: 0.1948 - acc: 0.914 - ETA: 0s - loss: 0.1962 - acc: 0.913 - ETA: 0s - loss: 0.1971 - acc: 0.913 - ETA: 0s - loss: 0.1954 - acc: 0.913 - ETA: 0s - loss: 0.1947 - acc: 0.913 - ETA: 0s - loss: 0.1946 - acc: 0.912 - ETA: 0s - loss: 0.1939 - acc: 0.913 - 1s 49us/step - loss: 0.1942 - acc: 0.9132 - val_loss: 0.9171 - val_acc: 0.8143\n",
      "Epoch 349/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1900 - acc: 0.875 - ETA: 0s - loss: 0.1945 - acc: 0.919 - ETA: 0s - loss: 0.1893 - acc: 0.917 - ETA: 0s - loss: 0.1938 - acc: 0.911 - ETA: 0s - loss: 0.1981 - acc: 0.911 - ETA: 0s - loss: 0.1926 - acc: 0.914 - ETA: 0s - loss: 0.1947 - acc: 0.912 - ETA: 0s - loss: 0.1939 - acc: 0.913 - ETA: 0s - loss: 0.1948 - acc: 0.913 - ETA: 0s - loss: 0.1962 - acc: 0.912 - ETA: 0s - loss: 0.1979 - acc: 0.911 - ETA: 0s - loss: 0.2005 - acc: 0.911 - ETA: 0s - loss: 0.2034 - acc: 0.911 - ETA: 0s - loss: 0.2024 - acc: 0.911 - ETA: 0s - loss: 0.2022 - acc: 0.911 - ETA: 0s - loss: 0.2007 - acc: 0.912 - ETA: 0s - loss: 0.1991 - acc: 0.913 - ETA: 0s - loss: 0.1999 - acc: 0.913 - 1s 49us/step - loss: 0.1996 - acc: 0.9133 - val_loss: 0.8657 - val_acc: 0.8264\n",
      "Epoch 350/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1644 - acc: 0.937 - ETA: 0s - loss: 0.1977 - acc: 0.913 - ETA: 0s - loss: 0.1941 - acc: 0.913 - ETA: 0s - loss: 0.1837 - acc: 0.917 - ETA: 0s - loss: 0.1848 - acc: 0.916 - ETA: 0s - loss: 0.1842 - acc: 0.917 - ETA: 0s - loss: 0.1883 - acc: 0.914 - ETA: 0s - loss: 0.1903 - acc: 0.913 - ETA: 0s - loss: 0.1884 - acc: 0.914 - ETA: 0s - loss: 0.1907 - acc: 0.913 - ETA: 0s - loss: 0.1913 - acc: 0.912 - ETA: 0s - loss: 0.1889 - acc: 0.914 - ETA: 0s - loss: 0.1903 - acc: 0.913 - ETA: 0s - loss: 0.1908 - acc: 0.913 - ETA: 0s - loss: 0.1928 - acc: 0.913 - ETA: 0s - loss: 0.1922 - acc: 0.913 - ETA: 0s - loss: 0.1922 - acc: 0.913 - ETA: 0s - loss: 0.1939 - acc: 0.913 - 1s 50us/step - loss: 0.1940 - acc: 0.9129 - val_loss: 0.8262 - val_acc: 0.8272\n",
      "Epoch 351/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.2036 - acc: 0.906 - ETA: 0s - loss: 0.2037 - acc: 0.924 - ETA: 0s - loss: 0.1961 - acc: 0.918 - ETA: 0s - loss: 0.1991 - acc: 0.914 - ETA: 0s - loss: 0.2055 - acc: 0.915 - ETA: 0s - loss: 0.1988 - acc: 0.918 - ETA: 0s - loss: 0.2010 - acc: 0.916 - ETA: 0s - loss: 0.2015 - acc: 0.914 - ETA: 0s - loss: 0.1959 - acc: 0.917 - ETA: 0s - loss: 0.1958 - acc: 0.918 - ETA: 0s - loss: 0.1990 - acc: 0.916 - ETA: 0s - loss: 0.1971 - acc: 0.916 - ETA: 0s - loss: 0.1965 - acc: 0.916 - ETA: 0s - loss: 0.1956 - acc: 0.916 - ETA: 0s - loss: 0.1948 - acc: 0.917 - ETA: 0s - loss: 0.1947 - acc: 0.916 - ETA: 0s - loss: 0.1949 - acc: 0.916 - ETA: 0s - loss: 0.1942 - acc: 0.916 - 1s 50us/step - loss: 0.1958 - acc: 0.9160 - val_loss: 0.8335 - val_acc: 0.8234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 352/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.1729 - acc: 0.906 - ETA: 0s - loss: 0.2070 - acc: 0.913 - ETA: 0s - loss: 0.2122 - acc: 0.915 - ETA: 0s - loss: 0.1984 - acc: 0.920 - ETA: 0s - loss: 0.1926 - acc: 0.918 - ETA: 0s - loss: 0.1900 - acc: 0.918 - ETA: 0s - loss: 0.1969 - acc: 0.914 - ETA: 0s - loss: 0.1950 - acc: 0.915 - ETA: 0s - loss: 0.1938 - acc: 0.914 - ETA: 0s - loss: 0.1919 - acc: 0.915 - ETA: 0s - loss: 0.1950 - acc: 0.914 - ETA: 0s - loss: 0.1929 - acc: 0.916 - ETA: 0s - loss: 0.1929 - acc: 0.915 - ETA: 0s - loss: 0.1940 - acc: 0.915 - ETA: 0s - loss: 0.1938 - acc: 0.914 - ETA: 0s - loss: 0.1940 - acc: 0.914 - ETA: 0s - loss: 0.1956 - acc: 0.913 - ETA: 0s - loss: 0.1949 - acc: 0.913 - 1s 49us/step - loss: 0.1950 - acc: 0.9138 - val_loss: 0.8769 - val_acc: 0.8196\n",
      "Epoch 353/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.1203 - acc: 0.937 - ETA: 0s - loss: 0.1902 - acc: 0.903 - ETA: 0s - loss: 0.1976 - acc: 0.900 - ETA: 0s - loss: 0.1904 - acc: 0.913 - ETA: 0s - loss: 0.1926 - acc: 0.911 - ETA: 0s - loss: 0.1924 - acc: 0.912 - ETA: 0s - loss: 0.1944 - acc: 0.910 - ETA: 0s - loss: 0.1940 - acc: 0.912 - ETA: 0s - loss: 0.1953 - acc: 0.912 - ETA: 0s - loss: 0.1922 - acc: 0.914 - ETA: 0s - loss: 0.1892 - acc: 0.915 - ETA: 0s - loss: 0.1907 - acc: 0.914 - ETA: 0s - loss: 0.1915 - acc: 0.914 - ETA: 0s - loss: 0.1944 - acc: 0.913 - ETA: 0s - loss: 0.1952 - acc: 0.913 - ETA: 0s - loss: 0.1935 - acc: 0.914 - ETA: 0s - loss: 0.1941 - acc: 0.913 - ETA: 0s - loss: 0.1938 - acc: 0.913 - 1s 49us/step - loss: 0.1936 - acc: 0.9138 - val_loss: 0.9088 - val_acc: 0.8270\n",
      "Epoch 354/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1065 - acc: 0.953 - ETA: 0s - loss: 0.1863 - acc: 0.918 - ETA: 0s - loss: 0.1829 - acc: 0.917 - ETA: 0s - loss: 0.1814 - acc: 0.914 - ETA: 0s - loss: 0.1868 - acc: 0.915 - ETA: 0s - loss: 0.1889 - acc: 0.912 - ETA: 0s - loss: 0.1880 - acc: 0.912 - ETA: 0s - loss: 0.1848 - acc: 0.914 - ETA: 0s - loss: 0.1842 - acc: 0.914 - ETA: 0s - loss: 0.1821 - acc: 0.916 - ETA: 0s - loss: 0.1851 - acc: 0.916 - ETA: 0s - loss: 0.1889 - acc: 0.914 - ETA: 0s - loss: 0.1891 - acc: 0.914 - ETA: 0s - loss: 0.1901 - acc: 0.914 - ETA: 0s - loss: 0.1929 - acc: 0.913 - ETA: 0s - loss: 0.1931 - acc: 0.914 - ETA: 0s - loss: 0.1950 - acc: 0.914 - ETA: 0s - loss: 0.1951 - acc: 0.913 - 1s 49us/step - loss: 0.1947 - acc: 0.9139 - val_loss: 0.8646 - val_acc: 0.8276\n",
      "Epoch 355/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.1976 - acc: 0.921 - ETA: 0s - loss: 0.1768 - acc: 0.914 - ETA: 0s - loss: 0.1953 - acc: 0.915 - ETA: 0s - loss: 0.1947 - acc: 0.913 - ETA: 0s - loss: 0.1938 - acc: 0.914 - ETA: 0s - loss: 0.1878 - acc: 0.917 - ETA: 0s - loss: 0.1923 - acc: 0.916 - ETA: 0s - loss: 0.1944 - acc: 0.914 - ETA: 0s - loss: 0.1924 - acc: 0.915 - ETA: 0s - loss: 0.1933 - acc: 0.915 - ETA: 0s - loss: 0.1952 - acc: 0.914 - ETA: 0s - loss: 0.1951 - acc: 0.914 - ETA: 0s - loss: 0.1971 - acc: 0.913 - ETA: 0s - loss: 0.1960 - acc: 0.914 - ETA: 0s - loss: 0.1965 - acc: 0.914 - ETA: 0s - loss: 0.1971 - acc: 0.914 - ETA: 0s - loss: 0.1974 - acc: 0.913 - ETA: 0s - loss: 0.1986 - acc: 0.913 - 1s 49us/step - loss: 0.1986 - acc: 0.9136 - val_loss: 0.8781 - val_acc: 0.8202\n",
      "Epoch 356/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1180 - acc: 0.953 - ETA: 0s - loss: 0.1824 - acc: 0.913 - ETA: 0s - loss: 0.1787 - acc: 0.916 - ETA: 0s - loss: 0.1825 - acc: 0.914 - ETA: 0s - loss: 0.1900 - acc: 0.910 - ETA: 0s - loss: 0.1904 - acc: 0.911 - ETA: 0s - loss: 0.1878 - acc: 0.912 - ETA: 0s - loss: 0.1914 - acc: 0.910 - ETA: 0s - loss: 0.1940 - acc: 0.910 - ETA: 0s - loss: 0.1935 - acc: 0.910 - ETA: 0s - loss: 0.1947 - acc: 0.911 - ETA: 0s - loss: 0.1930 - acc: 0.911 - ETA: 0s - loss: 0.1941 - acc: 0.912 - ETA: 0s - loss: 0.1941 - acc: 0.912 - ETA: 0s - loss: 0.1957 - acc: 0.911 - ETA: 0s - loss: 0.1965 - acc: 0.911 - ETA: 0s - loss: 0.1955 - acc: 0.912 - ETA: 0s - loss: 0.1967 - acc: 0.911 - 1s 50us/step - loss: 0.1968 - acc: 0.9119 - val_loss: 0.8966 - val_acc: 0.8199\n",
      "Epoch 357/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.2269 - acc: 0.875 - ETA: 0s - loss: 0.2148 - acc: 0.899 - ETA: 0s - loss: 0.1907 - acc: 0.917 - ETA: 0s - loss: 0.1896 - acc: 0.917 - ETA: 0s - loss: 0.1940 - acc: 0.914 - ETA: 0s - loss: 0.1904 - acc: 0.916 - ETA: 0s - loss: 0.1887 - acc: 0.918 - ETA: 0s - loss: 0.1918 - acc: 0.916 - ETA: 0s - loss: 0.1930 - acc: 0.916 - ETA: 0s - loss: 0.1902 - acc: 0.916 - ETA: 0s - loss: 0.1907 - acc: 0.916 - ETA: 0s - loss: 0.1907 - acc: 0.915 - ETA: 0s - loss: 0.1925 - acc: 0.914 - ETA: 0s - loss: 0.1934 - acc: 0.914 - ETA: 0s - loss: 0.1965 - acc: 0.912 - ETA: 0s - loss: 0.1972 - acc: 0.912 - ETA: 0s - loss: 0.1965 - acc: 0.913 - ETA: 0s - loss: 0.1961 - acc: 0.913 - 1s 50us/step - loss: 0.1967 - acc: 0.9134 - val_loss: 0.9418 - val_acc: 0.8210\n",
      "Epoch 358/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.1527 - acc: 0.937 - ETA: 0s - loss: 0.1907 - acc: 0.916 - ETA: 0s - loss: 0.1871 - acc: 0.918 - ETA: 0s - loss: 0.1884 - acc: 0.918 - ETA: 0s - loss: 0.1849 - acc: 0.919 - ETA: 0s - loss: 0.1930 - acc: 0.918 - ETA: 0s - loss: 0.1934 - acc: 0.918 - ETA: 0s - loss: 0.1925 - acc: 0.917 - ETA: 0s - loss: 0.1919 - acc: 0.917 - ETA: 0s - loss: 0.1942 - acc: 0.916 - ETA: 0s - loss: 0.1941 - acc: 0.916 - ETA: 0s - loss: 0.1932 - acc: 0.916 - ETA: 0s - loss: 0.1934 - acc: 0.915 - ETA: 0s - loss: 0.1939 - acc: 0.915 - ETA: 0s - loss: 0.1944 - acc: 0.915 - ETA: 0s - loss: 0.1951 - acc: 0.914 - ETA: 0s - loss: 0.1928 - acc: 0.915 - ETA: 0s - loss: 0.1917 - acc: 0.915 - 1s 49us/step - loss: 0.1921 - acc: 0.9150 - val_loss: 0.9071 - val_acc: 0.8271\n",
      "Epoch 359/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.1184 - acc: 0.937 - ETA: 0s - loss: 0.1628 - acc: 0.926 - ETA: 0s - loss: 0.1944 - acc: 0.920 - ETA: 0s - loss: 0.1936 - acc: 0.916 - ETA: 0s - loss: 0.1909 - acc: 0.915 - ETA: 0s - loss: 0.1966 - acc: 0.913 - ETA: 0s - loss: 0.1948 - acc: 0.913 - ETA: 0s - loss: 0.1899 - acc: 0.916 - ETA: 0s - loss: 0.1907 - acc: 0.918 - ETA: 0s - loss: 0.1884 - acc: 0.919 - ETA: 0s - loss: 0.1900 - acc: 0.918 - ETA: 0s - loss: 0.1914 - acc: 0.917 - ETA: 0s - loss: 0.1906 - acc: 0.916 - ETA: 0s - loss: 0.1922 - acc: 0.915 - ETA: 0s - loss: 0.1920 - acc: 0.915 - ETA: 0s - loss: 0.1966 - acc: 0.913 - ETA: 0s - loss: 0.1946 - acc: 0.915 - ETA: 0s - loss: 0.1952 - acc: 0.914 - 1s 50us/step - loss: 0.1959 - acc: 0.9140 - val_loss: 0.9169 - val_acc: 0.8272\n",
      "Epoch 360/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.4056 - acc: 0.906 - ETA: 0s - loss: 0.2289 - acc: 0.906 - ETA: 0s - loss: 0.2160 - acc: 0.906 - ETA: 0s - loss: 0.2094 - acc: 0.908 - ETA: 0s - loss: 0.2018 - acc: 0.909 - ETA: 0s - loss: 0.2051 - acc: 0.910 - ETA: 0s - loss: 0.2020 - acc: 0.910 - ETA: 0s - loss: 0.1980 - acc: 0.912 - ETA: 0s - loss: 0.1965 - acc: 0.912 - ETA: 0s - loss: 0.1941 - acc: 0.912 - ETA: 0s - loss: 0.1937 - acc: 0.913 - ETA: 0s - loss: 0.1947 - acc: 0.914 - ETA: 0s - loss: 0.1940 - acc: 0.915 - ETA: 0s - loss: 0.1928 - acc: 0.915 - ETA: 0s - loss: 0.1908 - acc: 0.916 - ETA: 0s - loss: 0.1907 - acc: 0.916 - ETA: 0s - loss: 0.1917 - acc: 0.915 - ETA: 0s - loss: 0.1937 - acc: 0.914 - 1s 49us/step - loss: 0.1935 - acc: 0.9141 - val_loss: 0.9098 - val_acc: 0.8193\n",
      "Epoch 361/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.3648 - acc: 0.890 - ETA: 0s - loss: 0.1972 - acc: 0.916 - ETA: 0s - loss: 0.1806 - acc: 0.921 - ETA: 0s - loss: 0.1808 - acc: 0.917 - ETA: 0s - loss: 0.1805 - acc: 0.917 - ETA: 0s - loss: 0.1782 - acc: 0.920 - ETA: 0s - loss: 0.1843 - acc: 0.917 - ETA: 0s - loss: 0.1820 - acc: 0.919 - ETA: 0s - loss: 0.1826 - acc: 0.919 - ETA: 0s - loss: 0.1834 - acc: 0.920 - ETA: 0s - loss: 0.1859 - acc: 0.920 - ETA: 0s - loss: 0.1875 - acc: 0.918 - ETA: 0s - loss: 0.1912 - acc: 0.917 - ETA: 0s - loss: 0.1899 - acc: 0.917 - ETA: 0s - loss: 0.1906 - acc: 0.917 - ETA: 0s - loss: 0.1912 - acc: 0.916 - ETA: 0s - loss: 0.1915 - acc: 0.916 - ETA: 0s - loss: 0.1928 - acc: 0.915 - 1s 51us/step - loss: 0.1934 - acc: 0.9152 - val_loss: 0.8495 - val_acc: 0.8260\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 362/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1061 - acc: 0.968 - ETA: 0s - loss: 0.2084 - acc: 0.919 - ETA: 0s - loss: 0.1931 - acc: 0.923 - ETA: 0s - loss: 0.1981 - acc: 0.918 - ETA: 0s - loss: 0.1943 - acc: 0.918 - ETA: 0s - loss: 0.1977 - acc: 0.918 - ETA: 0s - loss: 0.1959 - acc: 0.915 - ETA: 0s - loss: 0.1936 - acc: 0.916 - ETA: 0s - loss: 0.1927 - acc: 0.915 - ETA: 0s - loss: 0.1948 - acc: 0.915 - ETA: 0s - loss: 0.1913 - acc: 0.916 - ETA: 0s - loss: 0.1922 - acc: 0.915 - ETA: 0s - loss: 0.1928 - acc: 0.914 - ETA: 0s - loss: 0.1925 - acc: 0.914 - ETA: 0s - loss: 0.1931 - acc: 0.914 - ETA: 0s - loss: 0.1925 - acc: 0.914 - ETA: 0s - loss: 0.1952 - acc: 0.913 - ETA: 0s - loss: 0.1964 - acc: 0.913 - 1s 50us/step - loss: 0.1969 - acc: 0.9135 - val_loss: 0.8600 - val_acc: 0.8252\n",
      "Epoch 363/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.1181 - acc: 0.937 - ETA: 0s - loss: 0.1569 - acc: 0.926 - ETA: 0s - loss: 0.1660 - acc: 0.924 - ETA: 0s - loss: 0.1723 - acc: 0.922 - ETA: 0s - loss: 0.1685 - acc: 0.924 - ETA: 0s - loss: 0.1751 - acc: 0.922 - ETA: 0s - loss: 0.1797 - acc: 0.919 - ETA: 0s - loss: 0.1796 - acc: 0.920 - ETA: 0s - loss: 0.1812 - acc: 0.917 - ETA: 0s - loss: 0.1812 - acc: 0.918 - ETA: 0s - loss: 0.1838 - acc: 0.917 - ETA: 0s - loss: 0.1889 - acc: 0.915 - ETA: 0s - loss: 0.1911 - acc: 0.914 - ETA: 0s - loss: 0.1950 - acc: 0.913 - ETA: 0s - loss: 0.1974 - acc: 0.912 - ETA: 0s - loss: 0.1962 - acc: 0.913 - ETA: 0s - loss: 0.1969 - acc: 0.913 - ETA: 0s - loss: 0.1962 - acc: 0.913 - ETA: 0s - loss: 0.1958 - acc: 0.913 - 1s 51us/step - loss: 0.1959 - acc: 0.9139 - val_loss: 0.8540 - val_acc: 0.8259\n",
      "Epoch 364/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1646 - acc: 0.937 - ETA: 0s - loss: 0.1445 - acc: 0.935 - ETA: 0s - loss: 0.1711 - acc: 0.927 - ETA: 0s - loss: 0.1704 - acc: 0.925 - ETA: 0s - loss: 0.1795 - acc: 0.922 - ETA: 0s - loss: 0.1777 - acc: 0.922 - ETA: 0s - loss: 0.1806 - acc: 0.920 - ETA: 0s - loss: 0.1834 - acc: 0.917 - ETA: 0s - loss: 0.1843 - acc: 0.916 - ETA: 0s - loss: 0.1865 - acc: 0.917 - ETA: 0s - loss: 0.1883 - acc: 0.917 - ETA: 0s - loss: 0.1908 - acc: 0.915 - ETA: 0s - loss: 0.1914 - acc: 0.914 - ETA: 0s - loss: 0.1929 - acc: 0.914 - ETA: 0s - loss: 0.1950 - acc: 0.913 - ETA: 0s - loss: 0.1961 - acc: 0.913 - ETA: 0s - loss: 0.1930 - acc: 0.914 - ETA: 0s - loss: 0.1920 - acc: 0.914 - 1s 49us/step - loss: 0.1931 - acc: 0.9142 - val_loss: 0.8916 - val_acc: 0.8252\n",
      "Epoch 365/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1330 - acc: 0.937 - ETA: 0s - loss: 0.1711 - acc: 0.914 - ETA: 0s - loss: 0.1774 - acc: 0.917 - ETA: 0s - loss: 0.1874 - acc: 0.915 - ETA: 0s - loss: 0.1804 - acc: 0.921 - ETA: 0s - loss: 0.1810 - acc: 0.920 - ETA: 0s - loss: 0.1828 - acc: 0.920 - ETA: 0s - loss: 0.1830 - acc: 0.920 - ETA: 0s - loss: 0.1820 - acc: 0.919 - ETA: 0s - loss: 0.1839 - acc: 0.918 - ETA: 0s - loss: 0.1827 - acc: 0.919 - ETA: 0s - loss: 0.1856 - acc: 0.918 - ETA: 0s - loss: 0.1874 - acc: 0.917 - ETA: 0s - loss: 0.1889 - acc: 0.916 - ETA: 0s - loss: 0.1900 - acc: 0.916 - ETA: 0s - loss: 0.1925 - acc: 0.915 - ETA: 0s - loss: 0.1927 - acc: 0.915 - ETA: 0s - loss: 0.1936 - acc: 0.915 - 1s 50us/step - loss: 0.1933 - acc: 0.9147 - val_loss: 0.8890 - val_acc: 0.8227\n",
      "Epoch 366/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.3878 - acc: 0.937 - ETA: 0s - loss: 0.1917 - acc: 0.914 - ETA: 0s - loss: 0.1761 - acc: 0.922 - ETA: 0s - loss: 0.1794 - acc: 0.919 - ETA: 0s - loss: 0.1794 - acc: 0.918 - ETA: 0s - loss: 0.1850 - acc: 0.914 - ETA: 0s - loss: 0.1905 - acc: 0.913 - ETA: 0s - loss: 0.1906 - acc: 0.914 - ETA: 0s - loss: 0.1901 - acc: 0.915 - ETA: 0s - loss: 0.1898 - acc: 0.914 - ETA: 0s - loss: 0.1871 - acc: 0.916 - ETA: 0s - loss: 0.1852 - acc: 0.916 - ETA: 0s - loss: 0.1896 - acc: 0.915 - ETA: 0s - loss: 0.1888 - acc: 0.915 - ETA: 0s - loss: 0.1875 - acc: 0.915 - ETA: 0s - loss: 0.1890 - acc: 0.915 - ETA: 0s - loss: 0.1912 - acc: 0.914 - ETA: 0s - loss: 0.1907 - acc: 0.914 - 1s 51us/step - loss: 0.1914 - acc: 0.9142 - val_loss: 0.9012 - val_acc: 0.8273\n",
      "Epoch 367/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.0990 - acc: 0.968 - ETA: 0s - loss: 0.1877 - acc: 0.920 - ETA: 0s - loss: 0.1892 - acc: 0.916 - ETA: 0s - loss: 0.1942 - acc: 0.913 - ETA: 0s - loss: 0.1938 - acc: 0.915 - ETA: 0s - loss: 0.1914 - acc: 0.915 - ETA: 0s - loss: 0.1924 - acc: 0.915 - ETA: 0s - loss: 0.1850 - acc: 0.918 - ETA: 0s - loss: 0.1883 - acc: 0.916 - ETA: 0s - loss: 0.1883 - acc: 0.915 - ETA: 0s - loss: 0.1889 - acc: 0.914 - ETA: 0s - loss: 0.1912 - acc: 0.913 - ETA: 0s - loss: 0.1937 - acc: 0.913 - ETA: 0s - loss: 0.1947 - acc: 0.913 - ETA: 0s - loss: 0.1950 - acc: 0.913 - ETA: 0s - loss: 0.1970 - acc: 0.912 - ETA: 0s - loss: 0.1965 - acc: 0.913 - ETA: 0s - loss: 0.1959 - acc: 0.913 - 1s 50us/step - loss: 0.1972 - acc: 0.9128 - val_loss: 0.8404 - val_acc: 0.8290\n",
      "Epoch 368/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.2151 - acc: 0.906 - ETA: 0s - loss: 0.1725 - acc: 0.924 - ETA: 0s - loss: 0.1650 - acc: 0.925 - ETA: 0s - loss: 0.1746 - acc: 0.920 - ETA: 0s - loss: 0.1806 - acc: 0.916 - ETA: 0s - loss: 0.1802 - acc: 0.915 - ETA: 0s - loss: 0.1822 - acc: 0.916 - ETA: 0s - loss: 0.1894 - acc: 0.914 - ETA: 0s - loss: 0.1959 - acc: 0.912 - ETA: 0s - loss: 0.1972 - acc: 0.913 - ETA: 0s - loss: 0.1953 - acc: 0.913 - ETA: 0s - loss: 0.1947 - acc: 0.913 - ETA: 0s - loss: 0.1957 - acc: 0.913 - ETA: 0s - loss: 0.1966 - acc: 0.913 - ETA: 0s - loss: 0.1959 - acc: 0.913 - ETA: 0s - loss: 0.1954 - acc: 0.913 - ETA: 0s - loss: 0.1949 - acc: 0.913 - ETA: 0s - loss: 0.1941 - acc: 0.914 - 1s 50us/step - loss: 0.1943 - acc: 0.9138 - val_loss: 0.9049 - val_acc: 0.8207\n",
      "Epoch 369/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1794 - acc: 0.937 - ETA: 0s - loss: 0.1877 - acc: 0.923 - ETA: 0s - loss: 0.1805 - acc: 0.922 - ETA: 0s - loss: 0.1903 - acc: 0.918 - ETA: 0s - loss: 0.1911 - acc: 0.919 - ETA: 0s - loss: 0.1941 - acc: 0.920 - ETA: 0s - loss: 0.1926 - acc: 0.918 - ETA: 0s - loss: 0.1916 - acc: 0.918 - ETA: 0s - loss: 0.1918 - acc: 0.916 - ETA: 0s - loss: 0.1925 - acc: 0.916 - ETA: 0s - loss: 0.1943 - acc: 0.914 - ETA: 0s - loss: 0.1950 - acc: 0.914 - ETA: 0s - loss: 0.1949 - acc: 0.914 - ETA: 0s - loss: 0.1947 - acc: 0.915 - ETA: 0s - loss: 0.1943 - acc: 0.914 - ETA: 0s - loss: 0.1953 - acc: 0.914 - ETA: 0s - loss: 0.1950 - acc: 0.914 - ETA: 0s - loss: 0.1951 - acc: 0.914 - 1s 49us/step - loss: 0.1956 - acc: 0.9137 - val_loss: 0.8448 - val_acc: 0.8281\n",
      "Epoch 370/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.2166 - acc: 0.890 - ETA: 0s - loss: 0.1702 - acc: 0.929 - ETA: 0s - loss: 0.1825 - acc: 0.925 - ETA: 0s - loss: 0.1888 - acc: 0.920 - ETA: 0s - loss: 0.1963 - acc: 0.917 - ETA: 0s - loss: 0.1986 - acc: 0.914 - ETA: 0s - loss: 0.1985 - acc: 0.913 - ETA: 0s - loss: 0.1988 - acc: 0.912 - ETA: 0s - loss: 0.1978 - acc: 0.912 - ETA: 0s - loss: 0.1945 - acc: 0.915 - ETA: 0s - loss: 0.1915 - acc: 0.915 - ETA: 0s - loss: 0.1917 - acc: 0.915 - ETA: 0s - loss: 0.1926 - acc: 0.916 - ETA: 0s - loss: 0.1931 - acc: 0.914 - ETA: 0s - loss: 0.1932 - acc: 0.914 - ETA: 0s - loss: 0.1924 - acc: 0.914 - ETA: 0s - loss: 0.1923 - acc: 0.914 - ETA: 0s - loss: 0.1933 - acc: 0.914 - 1s 50us/step - loss: 0.1929 - acc: 0.9143 - val_loss: 0.9393 - val_acc: 0.8161\n",
      "Epoch 371/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1700 - acc: 0.921 - ETA: 0s - loss: 0.1671 - acc: 0.921 - ETA: 0s - loss: 0.1701 - acc: 0.927 - ETA: 0s - loss: 0.1764 - acc: 0.925 - ETA: 0s - loss: 0.1811 - acc: 0.920 - ETA: 0s - loss: 0.1851 - acc: 0.919 - ETA: 0s - loss: 0.1829 - acc: 0.918 - ETA: 0s - loss: 0.1829 - acc: 0.918 - ETA: 0s - loss: 0.1883 - acc: 0.916 - ETA: 0s - loss: 0.1893 - acc: 0.915 - ETA: 0s - loss: 0.1902 - acc: 0.914 - ETA: 0s - loss: 0.1926 - acc: 0.914 - ETA: 0s - loss: 0.1942 - acc: 0.913 - ETA: 0s - loss: 0.1942 - acc: 0.913 - ETA: 0s - loss: 0.1935 - acc: 0.913 - ETA: 0s - loss: 0.1932 - acc: 0.913 - ETA: 0s - loss: 0.1936 - acc: 0.914 - ETA: 0s - loss: 0.1944 - acc: 0.913 - 1s 50us/step - loss: 0.1951 - acc: 0.9137 - val_loss: 0.9711 - val_acc: 0.8196\n",
      "Epoch 372/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1487 - acc: 0.921 - ETA: 0s - loss: 0.1751 - acc: 0.922 - ETA: 0s - loss: 0.1911 - acc: 0.915 - ETA: 0s - loss: 0.1893 - acc: 0.915 - ETA: 0s - loss: 0.1889 - acc: 0.913 - ETA: 0s - loss: 0.1888 - acc: 0.916 - ETA: 0s - loss: 0.1887 - acc: 0.915 - ETA: 0s - loss: 0.1854 - acc: 0.917 - ETA: 0s - loss: 0.1874 - acc: 0.914 - ETA: 0s - loss: 0.1880 - acc: 0.914 - ETA: 0s - loss: 0.1892 - acc: 0.913 - ETA: 0s - loss: 0.1908 - acc: 0.913 - ETA: 0s - loss: 0.1905 - acc: 0.913 - ETA: 0s - loss: 0.1909 - acc: 0.913 - ETA: 0s - loss: 0.1942 - acc: 0.912 - ETA: 0s - loss: 0.1967 - acc: 0.913 - ETA: 0s - loss: 0.1965 - acc: 0.913 - ETA: 0s - loss: 0.1968 - acc: 0.913 - 1s 50us/step - loss: 0.1962 - acc: 0.9135 - val_loss: 0.8757 - val_acc: 0.8278\n",
      "Epoch 373/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.2144 - acc: 0.859 - ETA: 0s - loss: 0.1853 - acc: 0.903 - ETA: 0s - loss: 0.1862 - acc: 0.916 - ETA: 0s - loss: 0.1901 - acc: 0.914 - ETA: 0s - loss: 0.1889 - acc: 0.914 - ETA: 0s - loss: 0.1880 - acc: 0.915 - ETA: 0s - loss: 0.1928 - acc: 0.913 - ETA: 0s - loss: 0.1896 - acc: 0.914 - ETA: 0s - loss: 0.1874 - acc: 0.915 - ETA: 0s - loss: 0.1889 - acc: 0.915 - ETA: 0s - loss: 0.1914 - acc: 0.913 - ETA: 0s - loss: 0.1942 - acc: 0.912 - ETA: 0s - loss: 0.1948 - acc: 0.911 - ETA: 0s - loss: 0.1930 - acc: 0.913 - ETA: 0s - loss: 0.1940 - acc: 0.912 - ETA: 0s - loss: 0.1943 - acc: 0.912 - ETA: 0s - loss: 0.1924 - acc: 0.913 - 1s 48us/step - loss: 0.1927 - acc: 0.9132 - val_loss: 0.8795 - val_acc: 0.8229\n",
      "Epoch 374/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.2253 - acc: 0.906 - ETA: 0s - loss: 0.1831 - acc: 0.912 - ETA: 0s - loss: 0.1835 - acc: 0.915 - ETA: 0s - loss: 0.1816 - acc: 0.917 - ETA: 0s - loss: 0.1928 - acc: 0.913 - ETA: 0s - loss: 0.1889 - acc: 0.913 - ETA: 0s - loss: 0.1854 - acc: 0.915 - ETA: 0s - loss: 0.1860 - acc: 0.916 - ETA: 0s - loss: 0.1896 - acc: 0.915 - ETA: 0s - loss: 0.1951 - acc: 0.912 - ETA: 0s - loss: 0.1958 - acc: 0.912 - ETA: 0s - loss: 0.1940 - acc: 0.912 - ETA: 0s - loss: 0.1934 - acc: 0.913 - ETA: 0s - loss: 0.1932 - acc: 0.913 - ETA: 0s - loss: 0.1939 - acc: 0.913 - ETA: 0s - loss: 0.1936 - acc: 0.913 - ETA: 0s - loss: 0.1939 - acc: 0.913 - ETA: 0s - loss: 0.1943 - acc: 0.913 - 1s 51us/step - loss: 0.1955 - acc: 0.9126 - val_loss: 0.8899 - val_acc: 0.8155\n",
      "Epoch 375/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1088 - acc: 0.953 - ETA: 0s - loss: 0.2066 - acc: 0.911 - ETA: 0s - loss: 0.1834 - acc: 0.918 - ETA: 0s - loss: 0.1959 - acc: 0.916 - ETA: 0s - loss: 0.2079 - acc: 0.914 - ETA: 0s - loss: 0.2028 - acc: 0.915 - ETA: 0s - loss: 0.1973 - acc: 0.916 - ETA: 0s - loss: 0.2018 - acc: 0.915 - ETA: 0s - loss: 0.2017 - acc: 0.914 - ETA: 0s - loss: 0.1970 - acc: 0.915 - ETA: 0s - loss: 0.1950 - acc: 0.915 - ETA: 0s - loss: 0.1949 - acc: 0.915 - ETA: 0s - loss: 0.1921 - acc: 0.915 - ETA: 0s - loss: 0.1943 - acc: 0.914 - ETA: 0s - loss: 0.1922 - acc: 0.915 - ETA: 0s - loss: 0.1927 - acc: 0.914 - ETA: 0s - loss: 0.1912 - acc: 0.915 - ETA: 0s - loss: 0.1923 - acc: 0.914 - 1s 50us/step - loss: 0.1922 - acc: 0.9144 - val_loss: 0.8724 - val_acc: 0.8296\n",
      "Epoch 376/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.0825 - acc: 0.953 - ETA: 0s - loss: 0.1780 - acc: 0.920 - ETA: 0s - loss: 0.1985 - acc: 0.917 - ETA: 0s - loss: 0.1930 - acc: 0.917 - ETA: 0s - loss: 0.1885 - acc: 0.916 - ETA: 0s - loss: 0.1881 - acc: 0.915 - ETA: 0s - loss: 0.1882 - acc: 0.915 - ETA: 0s - loss: 0.1910 - acc: 0.915 - ETA: 0s - loss: 0.1931 - acc: 0.914 - ETA: 0s - loss: 0.1944 - acc: 0.915 - ETA: 0s - loss: 0.1934 - acc: 0.915 - ETA: 0s - loss: 0.1915 - acc: 0.916 - ETA: 0s - loss: 0.1919 - acc: 0.916 - ETA: 0s - loss: 0.1908 - acc: 0.915 - ETA: 0s - loss: 0.1941 - acc: 0.914 - ETA: 0s - loss: 0.1937 - acc: 0.914 - ETA: 0s - loss: 0.1946 - acc: 0.913 - ETA: 0s - loss: 0.1934 - acc: 0.914 - 1s 50us/step - loss: 0.1938 - acc: 0.9138 - val_loss: 0.9659 - val_acc: 0.8095\n",
      "Epoch 377/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1167 - acc: 0.921 - ETA: 0s - loss: 0.1856 - acc: 0.924 - ETA: 0s - loss: 0.1921 - acc: 0.917 - ETA: 0s - loss: 0.1908 - acc: 0.918 - ETA: 0s - loss: 0.1904 - acc: 0.915 - ETA: 0s - loss: 0.1888 - acc: 0.916 - ETA: 0s - loss: 0.1925 - acc: 0.913 - ETA: 0s - loss: 0.1919 - acc: 0.913 - ETA: 0s - loss: 0.1910 - acc: 0.912 - ETA: 0s - loss: 0.1933 - acc: 0.912 - ETA: 0s - loss: 0.1934 - acc: 0.912 - ETA: 0s - loss: 0.1975 - acc: 0.911 - ETA: 0s - loss: 0.1967 - acc: 0.912 - ETA: 0s - loss: 0.1947 - acc: 0.912 - ETA: 0s - loss: 0.1933 - acc: 0.913 - ETA: 0s - loss: 0.1942 - acc: 0.913 - ETA: 0s - loss: 0.1941 - acc: 0.913 - ETA: 0s - loss: 0.1947 - acc: 0.913 - ETA: 0s - loss: 0.1939 - acc: 0.914 - 1s 52us/step - loss: 0.1937 - acc: 0.9142 - val_loss: 0.9189 - val_acc: 0.8220\n",
      "Epoch 378/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1433 - acc: 0.937 - ETA: 0s - loss: 0.1804 - acc: 0.923 - ETA: 0s - loss: 0.1888 - acc: 0.918 - ETA: 0s - loss: 0.1873 - acc: 0.917 - ETA: 0s - loss: 0.1949 - acc: 0.916 - ETA: 0s - loss: 0.1913 - acc: 0.917 - ETA: 0s - loss: 0.1911 - acc: 0.916 - ETA: 0s - loss: 0.1905 - acc: 0.917 - ETA: 0s - loss: 0.1957 - acc: 0.916 - ETA: 0s - loss: 0.1954 - acc: 0.915 - ETA: 0s - loss: 0.1931 - acc: 0.915 - ETA: 0s - loss: 0.1951 - acc: 0.916 - ETA: 0s - loss: 0.1953 - acc: 0.916 - ETA: 0s - loss: 0.1967 - acc: 0.914 - ETA: 0s - loss: 0.1954 - acc: 0.914 - ETA: 0s - loss: 0.1960 - acc: 0.914 - ETA: 0s - loss: 0.1974 - acc: 0.914 - ETA: 0s - loss: 0.1976 - acc: 0.913 - 1s 50us/step - loss: 0.1962 - acc: 0.9140 - val_loss: 0.9189 - val_acc: 0.8220\n",
      "Epoch 379/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1690 - acc: 0.921 - ETA: 0s - loss: 0.1775 - acc: 0.915 - ETA: 0s - loss: 0.1934 - acc: 0.917 - ETA: 0s - loss: 0.1858 - acc: 0.919 - ETA: 0s - loss: 0.1785 - acc: 0.920 - ETA: 0s - loss: 0.1825 - acc: 0.921 - ETA: 0s - loss: 0.1854 - acc: 0.919 - ETA: 0s - loss: 0.1876 - acc: 0.917 - ETA: 0s - loss: 0.1902 - acc: 0.916 - ETA: 0s - loss: 0.1917 - acc: 0.915 - ETA: 0s - loss: 0.1920 - acc: 0.915 - ETA: 0s - loss: 0.1887 - acc: 0.916 - ETA: 0s - loss: 0.1896 - acc: 0.915 - ETA: 0s - loss: 0.1917 - acc: 0.916 - ETA: 0s - loss: 0.1910 - acc: 0.916 - ETA: 0s - loss: 0.1923 - acc: 0.915 - ETA: 0s - loss: 0.1921 - acc: 0.915 - ETA: 0s - loss: 0.1927 - acc: 0.915 - 1s 49us/step - loss: 0.1928 - acc: 0.9153 - val_loss: 0.8895 - val_acc: 0.8227\n",
      "Epoch 380/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.2892 - acc: 0.906 - ETA: 0s - loss: 0.1848 - acc: 0.916 - ETA: 0s - loss: 0.1782 - acc: 0.919 - ETA: 0s - loss: 0.1861 - acc: 0.917 - ETA: 0s - loss: 0.1793 - acc: 0.920 - ETA: 0s - loss: 0.1760 - acc: 0.922 - ETA: 0s - loss: 0.1799 - acc: 0.920 - ETA: 0s - loss: 0.1808 - acc: 0.920 - ETA: 0s - loss: 0.1802 - acc: 0.920 - ETA: 0s - loss: 0.1820 - acc: 0.920 - ETA: 0s - loss: 0.1839 - acc: 0.918 - ETA: 0s - loss: 0.1849 - acc: 0.918 - ETA: 0s - loss: 0.1848 - acc: 0.918 - ETA: 0s - loss: 0.1869 - acc: 0.917 - ETA: 0s - loss: 0.1850 - acc: 0.917 - ETA: 0s - loss: 0.1860 - acc: 0.917 - ETA: 0s - loss: 0.1879 - acc: 0.915 - ETA: 0s - loss: 0.1903 - acc: 0.915 - 1s 51us/step - loss: 0.1906 - acc: 0.9153 - val_loss: 0.8573 - val_acc: 0.8261\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 381/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.1613 - acc: 0.890 - ETA: 0s - loss: 0.1706 - acc: 0.911 - ETA: 0s - loss: 0.1712 - acc: 0.909 - ETA: 0s - loss: 0.1772 - acc: 0.909 - ETA: 0s - loss: 0.1779 - acc: 0.913 - ETA: 0s - loss: 0.1828 - acc: 0.910 - ETA: 0s - loss: 0.1909 - acc: 0.908 - ETA: 0s - loss: 0.1903 - acc: 0.909 - ETA: 0s - loss: 0.1910 - acc: 0.910 - ETA: 0s - loss: 0.1900 - acc: 0.910 - ETA: 0s - loss: 0.1876 - acc: 0.911 - ETA: 0s - loss: 0.1872 - acc: 0.912 - ETA: 0s - loss: 0.1871 - acc: 0.912 - ETA: 0s - loss: 0.1887 - acc: 0.912 - ETA: 0s - loss: 0.1873 - acc: 0.913 - ETA: 0s - loss: 0.1907 - acc: 0.912 - ETA: 0s - loss: 0.1903 - acc: 0.913 - ETA: 0s - loss: 0.1906 - acc: 0.913 - ETA: 0s - loss: 0.1898 - acc: 0.914 - 1s 51us/step - loss: 0.1895 - acc: 0.9145 - val_loss: 0.8835 - val_acc: 0.8285\n",
      "Epoch 382/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1390 - acc: 0.921 - ETA: 0s - loss: 0.2009 - acc: 0.919 - ETA: 0s - loss: 0.2087 - acc: 0.913 - ETA: 0s - loss: 0.2119 - acc: 0.909 - ETA: 0s - loss: 0.2063 - acc: 0.909 - ETA: 0s - loss: 0.2024 - acc: 0.912 - ETA: 0s - loss: 0.1972 - acc: 0.914 - ETA: 0s - loss: 0.1973 - acc: 0.915 - ETA: 0s - loss: 0.1927 - acc: 0.916 - ETA: 0s - loss: 0.1935 - acc: 0.916 - ETA: 0s - loss: 0.1978 - acc: 0.916 - ETA: 0s - loss: 0.1980 - acc: 0.915 - ETA: 0s - loss: 0.1958 - acc: 0.915 - ETA: 0s - loss: 0.1934 - acc: 0.916 - ETA: 0s - loss: 0.1956 - acc: 0.916 - ETA: 0s - loss: 0.1941 - acc: 0.916 - ETA: 0s - loss: 0.1952 - acc: 0.915 - ETA: 0s - loss: 0.1947 - acc: 0.915 - 1s 51us/step - loss: 0.1961 - acc: 0.9147 - val_loss: 0.9193 - val_acc: 0.8209\n",
      "Epoch 383/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.1360 - acc: 0.937 - ETA: 0s - loss: 0.1954 - acc: 0.920 - ETA: 0s - loss: 0.1913 - acc: 0.916 - ETA: 0s - loss: 0.1922 - acc: 0.916 - ETA: 0s - loss: 0.1872 - acc: 0.917 - ETA: 0s - loss: 0.1907 - acc: 0.915 - ETA: 0s - loss: 0.1862 - acc: 0.916 - ETA: 0s - loss: 0.1881 - acc: 0.915 - ETA: 0s - loss: 0.1900 - acc: 0.915 - ETA: 0s - loss: 0.1923 - acc: 0.916 - ETA: 0s - loss: 0.1912 - acc: 0.916 - ETA: 0s - loss: 0.1911 - acc: 0.917 - ETA: 0s - loss: 0.1929 - acc: 0.916 - ETA: 0s - loss: 0.1930 - acc: 0.915 - ETA: 0s - loss: 0.1916 - acc: 0.916 - ETA: 0s - loss: 0.1918 - acc: 0.916 - ETA: 0s - loss: 0.1919 - acc: 0.916 - ETA: 0s - loss: 0.1913 - acc: 0.916 - ETA: 0s - loss: 0.1912 - acc: 0.916 - 1s 51us/step - loss: 0.1914 - acc: 0.9165 - val_loss: 0.8853 - val_acc: 0.8193\n",
      "Epoch 384/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.1753 - acc: 0.906 - ETA: 0s - loss: 0.1725 - acc: 0.916 - ETA: 0s - loss: 0.1791 - acc: 0.919 - ETA: 0s - loss: 0.1760 - acc: 0.921 - ETA: 0s - loss: 0.1758 - acc: 0.920 - ETA: 0s - loss: 0.1784 - acc: 0.918 - ETA: 0s - loss: 0.1806 - acc: 0.918 - ETA: 0s - loss: 0.1797 - acc: 0.917 - ETA: 0s - loss: 0.1797 - acc: 0.918 - ETA: 0s - loss: 0.1824 - acc: 0.917 - ETA: 0s - loss: 0.1803 - acc: 0.918 - ETA: 0s - loss: 0.1828 - acc: 0.917 - ETA: 0s - loss: 0.1832 - acc: 0.916 - ETA: 0s - loss: 0.1855 - acc: 0.916 - ETA: 0s - loss: 0.1898 - acc: 0.914 - ETA: 0s - loss: 0.1904 - acc: 0.915 - ETA: 0s - loss: 0.1912 - acc: 0.915 - ETA: 0s - loss: 0.1927 - acc: 0.915 - 1s 49us/step - loss: 0.1939 - acc: 0.9149 - val_loss: 0.9741 - val_acc: 0.8164\n",
      "Epoch 385/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.1636 - acc: 0.968 - ETA: 0s - loss: 0.1576 - acc: 0.923 - ETA: 0s - loss: 0.1812 - acc: 0.919 - ETA: 0s - loss: 0.1814 - acc: 0.918 - ETA: 0s - loss: 0.1813 - acc: 0.920 - ETA: 0s - loss: 0.1846 - acc: 0.916 - ETA: 0s - loss: 0.1886 - acc: 0.915 - ETA: 0s - loss: 0.1922 - acc: 0.914 - ETA: 0s - loss: 0.1939 - acc: 0.913 - ETA: 0s - loss: 0.1930 - acc: 0.913 - ETA: 0s - loss: 0.1905 - acc: 0.914 - ETA: 0s - loss: 0.1913 - acc: 0.915 - ETA: 0s - loss: 0.1909 - acc: 0.915 - ETA: 0s - loss: 0.1902 - acc: 0.916 - ETA: 0s - loss: 0.1897 - acc: 0.916 - ETA: 0s - loss: 0.1897 - acc: 0.915 - ETA: 0s - loss: 0.1902 - acc: 0.916 - ETA: 0s - loss: 0.1912 - acc: 0.915 - 1s 50us/step - loss: 0.1915 - acc: 0.9151 - val_loss: 0.9013 - val_acc: 0.8205\n",
      "Epoch 386/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.1265 - acc: 0.921 - ETA: 0s - loss: 0.1853 - acc: 0.923 - ETA: 0s - loss: 0.1984 - acc: 0.917 - ETA: 0s - loss: 0.1973 - acc: 0.917 - ETA: 0s - loss: 0.1983 - acc: 0.915 - ETA: 0s - loss: 0.1961 - acc: 0.916 - ETA: 0s - loss: 0.1950 - acc: 0.917 - ETA: 0s - loss: 0.1869 - acc: 0.920 - ETA: 0s - loss: 0.1869 - acc: 0.919 - ETA: 0s - loss: 0.1866 - acc: 0.919 - ETA: 0s - loss: 0.1853 - acc: 0.919 - ETA: 0s - loss: 0.1877 - acc: 0.917 - ETA: 0s - loss: 0.1875 - acc: 0.918 - ETA: 0s - loss: 0.1890 - acc: 0.917 - ETA: 0s - loss: 0.1908 - acc: 0.917 - ETA: 0s - loss: 0.1911 - acc: 0.916 - ETA: 0s - loss: 0.1914 - acc: 0.916 - ETA: 0s - loss: 0.1936 - acc: 0.916 - 1s 51us/step - loss: 0.1942 - acc: 0.9158 - val_loss: 0.8538 - val_acc: 0.8311\n",
      "Epoch 387/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1853 - acc: 0.921 - ETA: 0s - loss: 0.1670 - acc: 0.921 - ETA: 0s - loss: 0.1699 - acc: 0.920 - ETA: 0s - loss: 0.1782 - acc: 0.917 - ETA: 0s - loss: 0.1825 - acc: 0.917 - ETA: 0s - loss: 0.1826 - acc: 0.920 - ETA: 0s - loss: 0.1828 - acc: 0.920 - ETA: 0s - loss: 0.1860 - acc: 0.918 - ETA: 0s - loss: 0.1858 - acc: 0.918 - ETA: 0s - loss: 0.1861 - acc: 0.917 - ETA: 0s - loss: 0.1838 - acc: 0.918 - ETA: 0s - loss: 0.1837 - acc: 0.917 - ETA: 0s - loss: 0.1855 - acc: 0.917 - ETA: 0s - loss: 0.1854 - acc: 0.916 - ETA: 0s - loss: 0.1850 - acc: 0.916 - ETA: 0s - loss: 0.1877 - acc: 0.916 - ETA: 0s - loss: 0.1892 - acc: 0.915 - ETA: 0s - loss: 0.1895 - acc: 0.915 - 1s 49us/step - loss: 0.1894 - acc: 0.9154 - val_loss: 0.8474 - val_acc: 0.8289\n",
      "Epoch 388/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1505 - acc: 0.890 - ETA: 0s - loss: 0.1648 - acc: 0.922 - ETA: 0s - loss: 0.1752 - acc: 0.922 - ETA: 0s - loss: 0.1803 - acc: 0.921 - ETA: 0s - loss: 0.1842 - acc: 0.917 - ETA: 0s - loss: 0.1858 - acc: 0.914 - ETA: 0s - loss: 0.1847 - acc: 0.915 - ETA: 0s - loss: 0.1885 - acc: 0.914 - ETA: 0s - loss: 0.1883 - acc: 0.915 - ETA: 0s - loss: 0.1893 - acc: 0.914 - ETA: 0s - loss: 0.1901 - acc: 0.914 - ETA: 0s - loss: 0.1904 - acc: 0.915 - ETA: 0s - loss: 0.1911 - acc: 0.915 - ETA: 0s - loss: 0.1912 - acc: 0.915 - ETA: 0s - loss: 0.1905 - acc: 0.914 - ETA: 0s - loss: 0.1898 - acc: 0.916 - ETA: 0s - loss: 0.1910 - acc: 0.915 - ETA: 0s - loss: 0.1917 - acc: 0.915 - 1s 50us/step - loss: 0.1912 - acc: 0.9154 - val_loss: 0.9288 - val_acc: 0.8231\n",
      "Epoch 389/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1342 - acc: 0.937 - ETA: 0s - loss: 0.1758 - acc: 0.920 - ETA: 0s - loss: 0.1873 - acc: 0.921 - ETA: 0s - loss: 0.1935 - acc: 0.918 - ETA: 0s - loss: 0.1947 - acc: 0.914 - ETA: 0s - loss: 0.2016 - acc: 0.912 - ETA: 0s - loss: 0.1991 - acc: 0.914 - ETA: 0s - loss: 0.1977 - acc: 0.915 - ETA: 0s - loss: 0.1947 - acc: 0.914 - ETA: 0s - loss: 0.1928 - acc: 0.915 - ETA: 0s - loss: 0.1911 - acc: 0.915 - ETA: 0s - loss: 0.1916 - acc: 0.915 - ETA: 0s - loss: 0.1921 - acc: 0.914 - ETA: 0s - loss: 0.1911 - acc: 0.914 - ETA: 0s - loss: 0.1919 - acc: 0.914 - ETA: 0s - loss: 0.1935 - acc: 0.913 - ETA: 0s - loss: 0.1930 - acc: 0.914 - ETA: 0s - loss: 0.1923 - acc: 0.914 - ETA: 0s - loss: 0.1923 - acc: 0.914 - 1s 52us/step - loss: 0.1925 - acc: 0.9147 - val_loss: 0.9156 - val_acc: 0.8265\n",
      "Epoch 390/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.1779 - acc: 0.890 - ETA: 0s - loss: 0.1819 - acc: 0.910 - ETA: 0s - loss: 0.1927 - acc: 0.914 - ETA: 0s - loss: 0.1880 - acc: 0.916 - ETA: 0s - loss: 0.1896 - acc: 0.915 - ETA: 0s - loss: 0.1899 - acc: 0.917 - ETA: 0s - loss: 0.1913 - acc: 0.916 - ETA: 0s - loss: 0.1921 - acc: 0.916 - ETA: 0s - loss: 0.1905 - acc: 0.916 - ETA: 0s - loss: 0.1914 - acc: 0.917 - ETA: 0s - loss: 0.1919 - acc: 0.917 - ETA: 0s - loss: 0.1899 - acc: 0.918 - ETA: 0s - loss: 0.1894 - acc: 0.917 - ETA: 0s - loss: 0.1887 - acc: 0.917 - ETA: 0s - loss: 0.1883 - acc: 0.917 - ETA: 0s - loss: 0.1902 - acc: 0.916 - ETA: 0s - loss: 0.1918 - acc: 0.915 - ETA: 0s - loss: 0.1920 - acc: 0.915 - 1s 49us/step - loss: 0.1921 - acc: 0.9151 - val_loss: 0.8347 - val_acc: 0.8312\n",
      "Epoch 391/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.2893 - acc: 0.859 - ETA: 0s - loss: 0.1573 - acc: 0.928 - ETA: 0s - loss: 0.1813 - acc: 0.916 - ETA: 0s - loss: 0.1829 - acc: 0.919 - ETA: 0s - loss: 0.1828 - acc: 0.918 - ETA: 0s - loss: 0.1825 - acc: 0.917 - ETA: 0s - loss: 0.1834 - acc: 0.919 - ETA: 0s - loss: 0.1839 - acc: 0.918 - ETA: 0s - loss: 0.1843 - acc: 0.917 - ETA: 0s - loss: 0.1851 - acc: 0.917 - ETA: 0s - loss: 0.1874 - acc: 0.917 - ETA: 0s - loss: 0.1889 - acc: 0.917 - ETA: 0s - loss: 0.1874 - acc: 0.918 - ETA: 0s - loss: 0.1901 - acc: 0.917 - ETA: 0s - loss: 0.1880 - acc: 0.917 - ETA: 0s - loss: 0.1883 - acc: 0.917 - ETA: 0s - loss: 0.1901 - acc: 0.916 - ETA: 0s - loss: 0.1910 - acc: 0.915 - 1s 50us/step - loss: 0.1919 - acc: 0.9153 - val_loss: 0.8989 - val_acc: 0.8238\n",
      "Epoch 392/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1394 - acc: 0.953 - ETA: 0s - loss: 0.1728 - acc: 0.921 - ETA: 0s - loss: 0.1775 - acc: 0.917 - ETA: 0s - loss: 0.1831 - acc: 0.917 - ETA: 0s - loss: 0.1819 - acc: 0.917 - ETA: 0s - loss: 0.1860 - acc: 0.917 - ETA: 0s - loss: 0.1833 - acc: 0.918 - ETA: 0s - loss: 0.1855 - acc: 0.918 - ETA: 0s - loss: 0.1866 - acc: 0.917 - ETA: 0s - loss: 0.1874 - acc: 0.917 - ETA: 0s - loss: 0.1897 - acc: 0.916 - ETA: 0s - loss: 0.1886 - acc: 0.916 - ETA: 0s - loss: 0.1883 - acc: 0.917 - ETA: 0s - loss: 0.1896 - acc: 0.916 - ETA: 0s - loss: 0.1890 - acc: 0.916 - ETA: 0s - loss: 0.1900 - acc: 0.916 - ETA: 0s - loss: 0.1908 - acc: 0.915 - ETA: 0s - loss: 0.1908 - acc: 0.915 - 1s 49us/step - loss: 0.1910 - acc: 0.9153 - val_loss: 0.9507 - val_acc: 0.8189\n",
      "Epoch 393/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.1004 - acc: 0.968 - ETA: 0s - loss: 0.2054 - acc: 0.913 - ETA: 0s - loss: 0.2133 - acc: 0.912 - ETA: 0s - loss: 0.1959 - acc: 0.913 - ETA: 0s - loss: 0.1936 - acc: 0.915 - ETA: 0s - loss: 0.1955 - acc: 0.914 - ETA: 0s - loss: 0.1970 - acc: 0.914 - ETA: 0s - loss: 0.1934 - acc: 0.914 - ETA: 0s - loss: 0.1894 - acc: 0.916 - ETA: 0s - loss: 0.1919 - acc: 0.916 - ETA: 0s - loss: 0.1923 - acc: 0.916 - ETA: 0s - loss: 0.1927 - acc: 0.915 - ETA: 0s - loss: 0.1935 - acc: 0.915 - ETA: 0s - loss: 0.1933 - acc: 0.915 - ETA: 0s - loss: 0.1933 - acc: 0.915 - ETA: 0s - loss: 0.1945 - acc: 0.915 - ETA: 0s - loss: 0.1941 - acc: 0.915 - ETA: 0s - loss: 0.1933 - acc: 0.915 - 1s 50us/step - loss: 0.1929 - acc: 0.9151 - val_loss: 0.9417 - val_acc: 0.8210\n",
      "Epoch 394/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1735 - acc: 0.906 - ETA: 0s - loss: 0.1697 - acc: 0.928 - ETA: 0s - loss: 0.1794 - acc: 0.927 - ETA: 0s - loss: 0.1797 - acc: 0.922 - ETA: 0s - loss: 0.1850 - acc: 0.919 - ETA: 0s - loss: 0.1850 - acc: 0.918 - ETA: 0s - loss: 0.1837 - acc: 0.918 - ETA: 0s - loss: 0.1863 - acc: 0.917 - ETA: 0s - loss: 0.1832 - acc: 0.917 - ETA: 0s - loss: 0.1816 - acc: 0.917 - ETA: 0s - loss: 0.1827 - acc: 0.917 - ETA: 0s - loss: 0.1827 - acc: 0.916 - ETA: 0s - loss: 0.1821 - acc: 0.916 - ETA: 0s - loss: 0.1846 - acc: 0.916 - ETA: 0s - loss: 0.1871 - acc: 0.915 - ETA: 0s - loss: 0.1882 - acc: 0.915 - ETA: 0s - loss: 0.1890 - acc: 0.915 - ETA: 0s - loss: 0.1926 - acc: 0.914 - 1s 51us/step - loss: 0.1930 - acc: 0.9138 - val_loss: 0.9199 - val_acc: 0.8259\n",
      "Epoch 395/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1748 - acc: 0.953 - ETA: 0s - loss: 0.1778 - acc: 0.926 - ETA: 0s - loss: 0.1682 - acc: 0.929 - ETA: 0s - loss: 0.1702 - acc: 0.927 - ETA: 0s - loss: 0.1814 - acc: 0.922 - ETA: 0s - loss: 0.1855 - acc: 0.919 - ETA: 0s - loss: 0.1864 - acc: 0.919 - ETA: 0s - loss: 0.1916 - acc: 0.917 - ETA: 0s - loss: 0.1961 - acc: 0.917 - ETA: 0s - loss: 0.1975 - acc: 0.916 - ETA: 0s - loss: 0.1969 - acc: 0.916 - ETA: 0s - loss: 0.1934 - acc: 0.916 - ETA: 0s - loss: 0.1928 - acc: 0.916 - ETA: 0s - loss: 0.1938 - acc: 0.915 - ETA: 0s - loss: 0.1960 - acc: 0.915 - ETA: 0s - loss: 0.1945 - acc: 0.915 - ETA: 0s - loss: 0.1940 - acc: 0.915 - ETA: 0s - loss: 0.1945 - acc: 0.915 - 1s 50us/step - loss: 0.1951 - acc: 0.9147 - val_loss: 1.0193 - val_acc: 0.8068\n",
      "Epoch 396/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.1477 - acc: 0.953 - ETA: 0s - loss: 0.1793 - acc: 0.911 - ETA: 0s - loss: 0.1803 - acc: 0.914 - ETA: 0s - loss: 0.1807 - acc: 0.912 - ETA: 0s - loss: 0.1856 - acc: 0.915 - ETA: 0s - loss: 0.1903 - acc: 0.915 - ETA: 0s - loss: 0.1945 - acc: 0.913 - ETA: 0s - loss: 0.1933 - acc: 0.914 - ETA: 0s - loss: 0.1931 - acc: 0.914 - ETA: 0s - loss: 0.1938 - acc: 0.914 - ETA: 0s - loss: 0.1906 - acc: 0.915 - ETA: 0s - loss: 0.1900 - acc: 0.915 - ETA: 0s - loss: 0.1921 - acc: 0.915 - ETA: 0s - loss: 0.1916 - acc: 0.915 - ETA: 0s - loss: 0.1931 - acc: 0.915 - ETA: 0s - loss: 0.1941 - acc: 0.915 - ETA: 0s - loss: 0.1939 - acc: 0.914 - ETA: 0s - loss: 0.1920 - acc: 0.915 - 1s 50us/step - loss: 0.1905 - acc: 0.9163 - val_loss: 0.9302 - val_acc: 0.8281\n",
      "Epoch 397/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.1544 - acc: 0.921 - ETA: 0s - loss: 0.1879 - acc: 0.912 - ETA: 0s - loss: 0.1871 - acc: 0.915 - ETA: 0s - loss: 0.1876 - acc: 0.918 - ETA: 0s - loss: 0.1861 - acc: 0.916 - ETA: 0s - loss: 0.1861 - acc: 0.915 - ETA: 0s - loss: 0.1879 - acc: 0.914 - ETA: 0s - loss: 0.1883 - acc: 0.913 - ETA: 0s - loss: 0.1890 - acc: 0.914 - ETA: 0s - loss: 0.1914 - acc: 0.913 - ETA: 0s - loss: 0.1905 - acc: 0.914 - ETA: 0s - loss: 0.1902 - acc: 0.914 - ETA: 0s - loss: 0.1908 - acc: 0.914 - ETA: 0s - loss: 0.1926 - acc: 0.913 - ETA: 0s - loss: 0.1933 - acc: 0.913 - ETA: 0s - loss: 0.1922 - acc: 0.913 - ETA: 0s - loss: 0.1924 - acc: 0.913 - ETA: 0s - loss: 0.1918 - acc: 0.914 - 1s 49us/step - loss: 0.1920 - acc: 0.9138 - val_loss: 0.8996 - val_acc: 0.8221\n",
      "Epoch 398/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1149 - acc: 0.968 - ETA: 0s - loss: 0.1683 - acc: 0.925 - ETA: 0s - loss: 0.1786 - acc: 0.923 - ETA: 0s - loss: 0.1822 - acc: 0.919 - ETA: 0s - loss: 0.1825 - acc: 0.917 - ETA: 0s - loss: 0.1772 - acc: 0.920 - ETA: 0s - loss: 0.1796 - acc: 0.919 - ETA: 0s - loss: 0.1792 - acc: 0.918 - ETA: 0s - loss: 0.1800 - acc: 0.918 - ETA: 0s - loss: 0.1798 - acc: 0.919 - ETA: 0s - loss: 0.1828 - acc: 0.918 - ETA: 0s - loss: 0.1832 - acc: 0.918 - ETA: 0s - loss: 0.1882 - acc: 0.917 - ETA: 0s - loss: 0.1913 - acc: 0.916 - ETA: 0s - loss: 0.1929 - acc: 0.915 - ETA: 0s - loss: 0.1927 - acc: 0.915 - ETA: 0s - loss: 0.1930 - acc: 0.915 - ETA: 0s - loss: 0.1921 - acc: 0.915 - 1s 50us/step - loss: 0.1920 - acc: 0.9157 - val_loss: 0.8930 - val_acc: 0.8304\n",
      "Epoch 399/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1701 - acc: 0.906 - ETA: 0s - loss: 0.2091 - acc: 0.905 - ETA: 0s - loss: 0.1915 - acc: 0.911 - ETA: 0s - loss: 0.1854 - acc: 0.916 - ETA: 0s - loss: 0.1879 - acc: 0.917 - ETA: 0s - loss: 0.1854 - acc: 0.917 - ETA: 0s - loss: 0.1846 - acc: 0.918 - ETA: 0s - loss: 0.1849 - acc: 0.918 - ETA: 0s - loss: 0.1874 - acc: 0.917 - ETA: 0s - loss: 0.1896 - acc: 0.917 - ETA: 0s - loss: 0.1899 - acc: 0.917 - ETA: 0s - loss: 0.1917 - acc: 0.917 - ETA: 0s - loss: 0.1936 - acc: 0.916 - ETA: 0s - loss: 0.1938 - acc: 0.916 - ETA: 0s - loss: 0.1922 - acc: 0.916 - ETA: 0s - loss: 0.1924 - acc: 0.915 - ETA: 0s - loss: 0.1923 - acc: 0.915 - ETA: 0s - loss: 0.1929 - acc: 0.915 - 1s 50us/step - loss: 0.1926 - acc: 0.9147 - val_loss: 0.8897 - val_acc: 0.8272\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 400/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.1645 - acc: 0.921 - ETA: 0s - loss: 0.1759 - acc: 0.918 - ETA: 0s - loss: 0.1677 - acc: 0.918 - ETA: 0s - loss: 0.1715 - acc: 0.920 - ETA: 0s - loss: 0.1815 - acc: 0.918 - ETA: 0s - loss: 0.1858 - acc: 0.920 - ETA: 0s - loss: 0.1872 - acc: 0.919 - ETA: 0s - loss: 0.1930 - acc: 0.916 - ETA: 0s - loss: 0.1900 - acc: 0.917 - ETA: 0s - loss: 0.1895 - acc: 0.918 - ETA: 0s - loss: 0.1937 - acc: 0.915 - ETA: 0s - loss: 0.1944 - acc: 0.914 - ETA: 0s - loss: 0.1944 - acc: 0.915 - ETA: 0s - loss: 0.1969 - acc: 0.914 - ETA: 0s - loss: 0.1962 - acc: 0.914 - ETA: 0s - loss: 0.1976 - acc: 0.914 - ETA: 0s - loss: 0.1955 - acc: 0.914 - ETA: 0s - loss: 0.1951 - acc: 0.915 - 1s 50us/step - loss: 0.1943 - acc: 0.9155 - val_loss: 0.9060 - val_acc: 0.8279\n",
      "Epoch 401/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.3440 - acc: 0.968 - ETA: 0s - loss: 0.1974 - acc: 0.916 - ETA: 0s - loss: 0.1913 - acc: 0.912 - ETA: 0s - loss: 0.1801 - acc: 0.917 - ETA: 0s - loss: 0.1864 - acc: 0.916 - ETA: 0s - loss: 0.1915 - acc: 0.914 - ETA: 0s - loss: 0.1920 - acc: 0.915 - ETA: 0s - loss: 0.1961 - acc: 0.914 - ETA: 0s - loss: 0.1954 - acc: 0.914 - ETA: 0s - loss: 0.1933 - acc: 0.914 - ETA: 0s - loss: 0.1929 - acc: 0.914 - ETA: 0s - loss: 0.1942 - acc: 0.914 - ETA: 0s - loss: 0.1944 - acc: 0.914 - ETA: 0s - loss: 0.1962 - acc: 0.913 - ETA: 0s - loss: 0.1946 - acc: 0.914 - ETA: 0s - loss: 0.1929 - acc: 0.915 - ETA: 0s - loss: 0.1926 - acc: 0.915 - ETA: 0s - loss: 0.1939 - acc: 0.915 - 1s 49us/step - loss: 0.1942 - acc: 0.9158 - val_loss: 0.8644 - val_acc: 0.8309\n",
      "Epoch 402/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.1695 - acc: 0.921 - ETA: 0s - loss: 0.1997 - acc: 0.913 - ETA: 0s - loss: 0.1875 - acc: 0.917 - ETA: 0s - loss: 0.1861 - acc: 0.916 - ETA: 0s - loss: 0.1804 - acc: 0.921 - ETA: 0s - loss: 0.1820 - acc: 0.920 - ETA: 0s - loss: 0.1795 - acc: 0.921 - ETA: 0s - loss: 0.1869 - acc: 0.919 - ETA: 0s - loss: 0.1857 - acc: 0.919 - ETA: 0s - loss: 0.1897 - acc: 0.919 - ETA: 0s - loss: 0.1925 - acc: 0.917 - ETA: 0s - loss: 0.1917 - acc: 0.918 - ETA: 0s - loss: 0.1915 - acc: 0.917 - ETA: 0s - loss: 0.1914 - acc: 0.916 - ETA: 0s - loss: 0.1897 - acc: 0.917 - ETA: 0s - loss: 0.1911 - acc: 0.916 - ETA: 0s - loss: 0.1902 - acc: 0.916 - ETA: 0s - loss: 0.1904 - acc: 0.915 - 1s 50us/step - loss: 0.1912 - acc: 0.9150 - val_loss: 0.9349 - val_acc: 0.8204\n",
      "Epoch 403/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.3979 - acc: 0.906 - ETA: 0s - loss: 0.1808 - acc: 0.918 - ETA: 0s - loss: 0.1735 - acc: 0.920 - ETA: 0s - loss: 0.1892 - acc: 0.923 - ETA: 0s - loss: 0.1909 - acc: 0.918 - ETA: 0s - loss: 0.1964 - acc: 0.917 - ETA: 0s - loss: 0.1960 - acc: 0.917 - ETA: 0s - loss: 0.1950 - acc: 0.917 - ETA: 0s - loss: 0.1924 - acc: 0.918 - ETA: 0s - loss: 0.1885 - acc: 0.919 - ETA: 0s - loss: 0.1868 - acc: 0.919 - ETA: 0s - loss: 0.1881 - acc: 0.920 - ETA: 0s - loss: 0.1891 - acc: 0.920 - ETA: 0s - loss: 0.1864 - acc: 0.921 - ETA: 0s - loss: 0.1890 - acc: 0.920 - ETA: 0s - loss: 0.1898 - acc: 0.919 - ETA: 0s - loss: 0.1901 - acc: 0.918 - ETA: 0s - loss: 0.1912 - acc: 0.917 - 1s 50us/step - loss: 0.1908 - acc: 0.9179 - val_loss: 0.9288 - val_acc: 0.8289\n",
      "Epoch 404/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.1519 - acc: 0.953 - ETA: 0s - loss: 0.1786 - acc: 0.929 - ETA: 0s - loss: 0.1862 - acc: 0.924 - ETA: 0s - loss: 0.1825 - acc: 0.926 - ETA: 0s - loss: 0.1800 - acc: 0.923 - ETA: 0s - loss: 0.1831 - acc: 0.921 - ETA: 0s - loss: 0.1818 - acc: 0.920 - ETA: 0s - loss: 0.1831 - acc: 0.921 - ETA: 0s - loss: 0.1844 - acc: 0.921 - ETA: 0s - loss: 0.1862 - acc: 0.920 - ETA: 0s - loss: 0.1882 - acc: 0.918 - ETA: 0s - loss: 0.1889 - acc: 0.917 - ETA: 0s - loss: 0.1928 - acc: 0.916 - ETA: 0s - loss: 0.1926 - acc: 0.917 - ETA: 0s - loss: 0.1931 - acc: 0.917 - ETA: 0s - loss: 0.1932 - acc: 0.917 - ETA: 0s - loss: 0.1935 - acc: 0.917 - ETA: 0s - loss: 0.1938 - acc: 0.917 - 1s 49us/step - loss: 0.1933 - acc: 0.9176 - val_loss: 0.9799 - val_acc: 0.8177\n",
      "Epoch 405/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.2035 - acc: 0.890 - ETA: 0s - loss: 0.1747 - acc: 0.922 - ETA: 0s - loss: 0.1775 - acc: 0.925 - ETA: 0s - loss: 0.1846 - acc: 0.922 - ETA: 0s - loss: 0.1899 - acc: 0.919 - ETA: 0s - loss: 0.1884 - acc: 0.918 - ETA: 0s - loss: 0.1846 - acc: 0.919 - ETA: 0s - loss: 0.1881 - acc: 0.918 - ETA: 0s - loss: 0.1908 - acc: 0.916 - ETA: 0s - loss: 0.1915 - acc: 0.915 - ETA: 0s - loss: 0.1911 - acc: 0.916 - ETA: 0s - loss: 0.1899 - acc: 0.916 - ETA: 0s - loss: 0.1919 - acc: 0.917 - ETA: 0s - loss: 0.1924 - acc: 0.916 - ETA: 0s - loss: 0.1913 - acc: 0.917 - ETA: 0s - loss: 0.1919 - acc: 0.916 - ETA: 0s - loss: 0.1907 - acc: 0.916 - ETA: 0s - loss: 0.1905 - acc: 0.916 - 1s 50us/step - loss: 0.1905 - acc: 0.9162 - val_loss: 0.9164 - val_acc: 0.8287\n",
      "Epoch 406/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1392 - acc: 0.906 - ETA: 0s - loss: 0.1747 - acc: 0.921 - ETA: 0s - loss: 0.1934 - acc: 0.919 - ETA: 0s - loss: 0.1889 - acc: 0.921 - ETA: 0s - loss: 0.1914 - acc: 0.917 - ETA: 0s - loss: 0.1931 - acc: 0.913 - ETA: 0s - loss: 0.1941 - acc: 0.913 - ETA: 0s - loss: 0.1936 - acc: 0.913 - ETA: 0s - loss: 0.1935 - acc: 0.914 - ETA: 0s - loss: 0.1958 - acc: 0.915 - ETA: 0s - loss: 0.1938 - acc: 0.915 - ETA: 0s - loss: 0.1923 - acc: 0.916 - ETA: 0s - loss: 0.1909 - acc: 0.916 - ETA: 0s - loss: 0.1907 - acc: 0.917 - ETA: 0s - loss: 0.1901 - acc: 0.917 - ETA: 0s - loss: 0.1909 - acc: 0.916 - ETA: 0s - loss: 0.1920 - acc: 0.915 - ETA: 0s - loss: 0.1910 - acc: 0.915 - 1s 49us/step - loss: 0.1906 - acc: 0.9160 - val_loss: 0.9185 - val_acc: 0.8207\n",
      "Epoch 407/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1144 - acc: 0.953 - ETA: 0s - loss: 0.1900 - acc: 0.917 - ETA: 0s - loss: 0.1842 - acc: 0.925 - ETA: 0s - loss: 0.1870 - acc: 0.920 - ETA: 0s - loss: 0.1984 - acc: 0.918 - ETA: 0s - loss: 0.1971 - acc: 0.916 - ETA: 0s - loss: 0.1962 - acc: 0.916 - ETA: 0s - loss: 0.1966 - acc: 0.916 - ETA: 0s - loss: 0.1950 - acc: 0.915 - ETA: 0s - loss: 0.1937 - acc: 0.915 - ETA: 0s - loss: 0.1935 - acc: 0.914 - ETA: 0s - loss: 0.1941 - acc: 0.914 - ETA: 0s - loss: 0.1963 - acc: 0.913 - ETA: 0s - loss: 0.1947 - acc: 0.913 - ETA: 0s - loss: 0.1941 - acc: 0.913 - ETA: 0s - loss: 0.1929 - acc: 0.914 - ETA: 0s - loss: 0.1920 - acc: 0.914 - ETA: 0s - loss: 0.1927 - acc: 0.914 - 1s 50us/step - loss: 0.1924 - acc: 0.9147 - val_loss: 0.9137 - val_acc: 0.8255\n",
      "Epoch 408/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.3069 - acc: 0.906 - ETA: 0s - loss: 0.2076 - acc: 0.917 - ETA: 0s - loss: 0.1999 - acc: 0.913 - ETA: 0s - loss: 0.1911 - acc: 0.917 - ETA: 0s - loss: 0.1886 - acc: 0.916 - ETA: 0s - loss: 0.1850 - acc: 0.917 - ETA: 0s - loss: 0.1945 - acc: 0.913 - ETA: 0s - loss: 0.1913 - acc: 0.914 - ETA: 0s - loss: 0.1895 - acc: 0.914 - ETA: 0s - loss: 0.1889 - acc: 0.915 - ETA: 0s - loss: 0.1872 - acc: 0.916 - ETA: 0s - loss: 0.1893 - acc: 0.916 - ETA: 0s - loss: 0.1897 - acc: 0.916 - ETA: 0s - loss: 0.1899 - acc: 0.915 - ETA: 0s - loss: 0.1912 - acc: 0.915 - ETA: 0s - loss: 0.1913 - acc: 0.914 - ETA: 0s - loss: 0.1925 - acc: 0.914 - ETA: 0s - loss: 0.1917 - acc: 0.915 - 1s 50us/step - loss: 0.1913 - acc: 0.9159 - val_loss: 0.9467 - val_acc: 0.8236\n",
      "Epoch 409/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.3093 - acc: 0.859 - ETA: 0s - loss: 0.1709 - acc: 0.921 - ETA: 0s - loss: 0.1735 - acc: 0.925 - ETA: 0s - loss: 0.1838 - acc: 0.921 - ETA: 0s - loss: 0.1859 - acc: 0.920 - ETA: 0s - loss: 0.1845 - acc: 0.920 - ETA: 0s - loss: 0.1831 - acc: 0.919 - ETA: 0s - loss: 0.1871 - acc: 0.917 - ETA: 0s - loss: 0.1913 - acc: 0.914 - ETA: 0s - loss: 0.1925 - acc: 0.914 - ETA: 0s - loss: 0.1934 - acc: 0.914 - ETA: 0s - loss: 0.1920 - acc: 0.915 - ETA: 0s - loss: 0.1913 - acc: 0.915 - ETA: 0s - loss: 0.1920 - acc: 0.915 - ETA: 0s - loss: 0.1904 - acc: 0.916 - ETA: 0s - loss: 0.1884 - acc: 0.917 - ETA: 0s - loss: 0.1892 - acc: 0.916 - ETA: 0s - loss: 0.1896 - acc: 0.916 - 1s 51us/step - loss: 0.1915 - acc: 0.9152 - val_loss: 0.9214 - val_acc: 0.8254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 410/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1646 - acc: 0.953 - ETA: 0s - loss: 0.2128 - acc: 0.917 - ETA: 0s - loss: 0.1985 - acc: 0.916 - ETA: 0s - loss: 0.1926 - acc: 0.917 - ETA: 0s - loss: 0.1980 - acc: 0.914 - ETA: 0s - loss: 0.1944 - acc: 0.915 - ETA: 0s - loss: 0.1919 - acc: 0.917 - ETA: 0s - loss: 0.1886 - acc: 0.918 - ETA: 0s - loss: 0.1901 - acc: 0.917 - ETA: 0s - loss: 0.1888 - acc: 0.918 - ETA: 0s - loss: 0.1879 - acc: 0.919 - ETA: 0s - loss: 0.1900 - acc: 0.918 - ETA: 0s - loss: 0.1899 - acc: 0.919 - ETA: 0s - loss: 0.1887 - acc: 0.919 - ETA: 0s - loss: 0.1907 - acc: 0.918 - ETA: 0s - loss: 0.1924 - acc: 0.918 - ETA: 0s - loss: 0.1926 - acc: 0.917 - ETA: 0s - loss: 0.1944 - acc: 0.916 - 1s 49us/step - loss: 0.1946 - acc: 0.9162 - val_loss: 0.9049 - val_acc: 0.8210\n",
      "Epoch 411/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.1572 - acc: 0.937 - ETA: 0s - loss: 0.1784 - acc: 0.922 - ETA: 0s - loss: 0.1771 - acc: 0.923 - ETA: 0s - loss: 0.1737 - acc: 0.925 - ETA: 0s - loss: 0.1782 - acc: 0.921 - ETA: 0s - loss: 0.1757 - acc: 0.923 - ETA: 0s - loss: 0.1857 - acc: 0.918 - ETA: 0s - loss: 0.1842 - acc: 0.918 - ETA: 0s - loss: 0.1868 - acc: 0.918 - ETA: 0s - loss: 0.1884 - acc: 0.916 - ETA: 0s - loss: 0.1876 - acc: 0.916 - ETA: 0s - loss: 0.1894 - acc: 0.917 - ETA: 0s - loss: 0.1887 - acc: 0.916 - ETA: 0s - loss: 0.1892 - acc: 0.916 - ETA: 0s - loss: 0.1899 - acc: 0.917 - ETA: 0s - loss: 0.1912 - acc: 0.917 - ETA: 0s - loss: 0.1924 - acc: 0.916 - ETA: 0s - loss: 0.1947 - acc: 0.916 - 1s 49us/step - loss: 0.1948 - acc: 0.9159 - val_loss: 0.8881 - val_acc: 0.8254\n",
      "Epoch 412/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1534 - acc: 0.953 - ETA: 0s - loss: 0.1564 - acc: 0.935 - ETA: 0s - loss: 0.1654 - acc: 0.927 - ETA: 0s - loss: 0.1777 - acc: 0.922 - ETA: 0s - loss: 0.1879 - acc: 0.921 - ETA: 0s - loss: 0.1888 - acc: 0.922 - ETA: 0s - loss: 0.1861 - acc: 0.924 - ETA: 0s - loss: 0.1868 - acc: 0.921 - ETA: 0s - loss: 0.1859 - acc: 0.921 - ETA: 0s - loss: 0.1890 - acc: 0.919 - ETA: 0s - loss: 0.1902 - acc: 0.919 - ETA: 0s - loss: 0.1890 - acc: 0.920 - ETA: 0s - loss: 0.1890 - acc: 0.919 - ETA: 0s - loss: 0.1881 - acc: 0.919 - ETA: 0s - loss: 0.1907 - acc: 0.918 - ETA: 0s - loss: 0.1911 - acc: 0.917 - ETA: 0s - loss: 0.1910 - acc: 0.917 - ETA: 0s - loss: 0.1919 - acc: 0.916 - 1s 49us/step - loss: 0.1923 - acc: 0.9167 - val_loss: 0.9185 - val_acc: 0.8173\n",
      "Epoch 413/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.2047 - acc: 0.890 - ETA: 0s - loss: 0.1986 - acc: 0.908 - ETA: 0s - loss: 0.1825 - acc: 0.915 - ETA: 0s - loss: 0.1930 - acc: 0.912 - ETA: 0s - loss: 0.1932 - acc: 0.913 - ETA: 0s - loss: 0.1908 - acc: 0.914 - ETA: 0s - loss: 0.1908 - acc: 0.915 - ETA: 0s - loss: 0.1899 - acc: 0.916 - ETA: 0s - loss: 0.1882 - acc: 0.916 - ETA: 0s - loss: 0.1858 - acc: 0.917 - ETA: 0s - loss: 0.1870 - acc: 0.916 - ETA: 0s - loss: 0.1863 - acc: 0.916 - ETA: 0s - loss: 0.1867 - acc: 0.917 - ETA: 0s - loss: 0.1880 - acc: 0.916 - ETA: 0s - loss: 0.1918 - acc: 0.915 - ETA: 0s - loss: 0.1912 - acc: 0.915 - ETA: 0s - loss: 0.1897 - acc: 0.917 - ETA: 0s - loss: 0.1926 - acc: 0.916 - 1s 50us/step - loss: 0.1940 - acc: 0.9156 - val_loss: 0.9253 - val_acc: 0.8169\n",
      "Epoch 414/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1977 - acc: 0.921 - ETA: 0s - loss: 0.1990 - acc: 0.921 - ETA: 0s - loss: 0.1911 - acc: 0.920 - ETA: 0s - loss: 0.1945 - acc: 0.916 - ETA: 0s - loss: 0.1938 - acc: 0.915 - ETA: 0s - loss: 0.1948 - acc: 0.913 - ETA: 0s - loss: 0.1953 - acc: 0.911 - ETA: 0s - loss: 0.1917 - acc: 0.914 - ETA: 0s - loss: 0.1928 - acc: 0.913 - ETA: 0s - loss: 0.1901 - acc: 0.914 - ETA: 0s - loss: 0.1917 - acc: 0.914 - ETA: 0s - loss: 0.1948 - acc: 0.913 - ETA: 0s - loss: 0.1954 - acc: 0.913 - ETA: 0s - loss: 0.1939 - acc: 0.914 - ETA: 0s - loss: 0.1936 - acc: 0.914 - ETA: 0s - loss: 0.1947 - acc: 0.914 - ETA: 0s - loss: 0.1940 - acc: 0.915 - ETA: 0s - loss: 0.1945 - acc: 0.915 - 1s 50us/step - loss: 0.1945 - acc: 0.9154 - val_loss: 0.8603 - val_acc: 0.8270\n",
      "Epoch 415/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.1203 - acc: 0.953 - ETA: 0s - loss: 0.1839 - acc: 0.910 - ETA: 0s - loss: 0.1760 - acc: 0.917 - ETA: 0s - loss: 0.1798 - acc: 0.916 - ETA: 0s - loss: 0.1856 - acc: 0.914 - ETA: 0s - loss: 0.1814 - acc: 0.915 - ETA: 0s - loss: 0.1906 - acc: 0.913 - ETA: 0s - loss: 0.1904 - acc: 0.914 - ETA: 0s - loss: 0.1880 - acc: 0.915 - ETA: 0s - loss: 0.1844 - acc: 0.917 - ETA: 0s - loss: 0.1841 - acc: 0.918 - ETA: 0s - loss: 0.1879 - acc: 0.917 - ETA: 0s - loss: 0.1869 - acc: 0.917 - ETA: 0s - loss: 0.1907 - acc: 0.916 - ETA: 0s - loss: 0.1914 - acc: 0.915 - ETA: 0s - loss: 0.1939 - acc: 0.915 - ETA: 0s - loss: 0.1926 - acc: 0.915 - ETA: 0s - loss: 0.1931 - acc: 0.915 - 1s 50us/step - loss: 0.1933 - acc: 0.9156 - val_loss: 0.9044 - val_acc: 0.8275\n",
      "Epoch 416/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.3972 - acc: 0.890 - ETA: 0s - loss: 0.1925 - acc: 0.930 - ETA: 0s - loss: 0.1998 - acc: 0.919 - ETA: 0s - loss: 0.1939 - acc: 0.918 - ETA: 0s - loss: 0.1959 - acc: 0.917 - ETA: 0s - loss: 0.1932 - acc: 0.918 - ETA: 0s - loss: 0.1936 - acc: 0.918 - ETA: 0s - loss: 0.1913 - acc: 0.918 - ETA: 0s - loss: 0.1894 - acc: 0.918 - ETA: 0s - loss: 0.1911 - acc: 0.917 - ETA: 0s - loss: 0.1906 - acc: 0.916 - ETA: 0s - loss: 0.1914 - acc: 0.916 - ETA: 0s - loss: 0.1922 - acc: 0.916 - ETA: 0s - loss: 0.1951 - acc: 0.916 - ETA: 0s - loss: 0.1945 - acc: 0.915 - ETA: 0s - loss: 0.1944 - acc: 0.914 - ETA: 0s - loss: 0.1962 - acc: 0.915 - ETA: 0s - loss: 0.1955 - acc: 0.915 - 1s 50us/step - loss: 0.1942 - acc: 0.9153 - val_loss: 0.9360 - val_acc: 0.8251\n",
      "Epoch 417/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.2034 - acc: 0.890 - ETA: 0s - loss: 0.2000 - acc: 0.924 - ETA: 0s - loss: 0.2021 - acc: 0.921 - ETA: 0s - loss: 0.1907 - acc: 0.924 - ETA: 0s - loss: 0.1853 - acc: 0.925 - ETA: 0s - loss: 0.1849 - acc: 0.924 - ETA: 0s - loss: 0.1874 - acc: 0.922 - ETA: 0s - loss: 0.1901 - acc: 0.919 - ETA: 0s - loss: 0.1910 - acc: 0.920 - ETA: 0s - loss: 0.1924 - acc: 0.918 - ETA: 0s - loss: 0.1923 - acc: 0.918 - ETA: 0s - loss: 0.1898 - acc: 0.918 - ETA: 0s - loss: 0.1919 - acc: 0.918 - ETA: 0s - loss: 0.1940 - acc: 0.918 - ETA: 0s - loss: 0.1939 - acc: 0.918 - ETA: 0s - loss: 0.1936 - acc: 0.918 - ETA: 0s - loss: 0.1924 - acc: 0.917 - ETA: 0s - loss: 0.1936 - acc: 0.917 - 1s 50us/step - loss: 0.1951 - acc: 0.9162 - val_loss: 1.0123 - val_acc: 0.8113\n",
      "Epoch 418/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.3127 - acc: 0.906 - ETA: 0s - loss: 0.1563 - acc: 0.932 - ETA: 0s - loss: 0.1699 - acc: 0.926 - ETA: 0s - loss: 0.1766 - acc: 0.921 - ETA: 0s - loss: 0.1831 - acc: 0.919 - ETA: 0s - loss: 0.1876 - acc: 0.917 - ETA: 0s - loss: 0.1879 - acc: 0.916 - ETA: 0s - loss: 0.1899 - acc: 0.916 - ETA: 0s - loss: 0.1920 - acc: 0.916 - ETA: 0s - loss: 0.1944 - acc: 0.915 - ETA: 0s - loss: 0.1968 - acc: 0.914 - ETA: 0s - loss: 0.1957 - acc: 0.914 - ETA: 0s - loss: 0.1948 - acc: 0.915 - ETA: 0s - loss: 0.1959 - acc: 0.914 - ETA: 0s - loss: 0.1955 - acc: 0.914 - ETA: 0s - loss: 0.1944 - acc: 0.915 - ETA: 0s - loss: 0.1939 - acc: 0.915 - ETA: 0s - loss: 0.1955 - acc: 0.915 - 1s 50us/step - loss: 0.1949 - acc: 0.9160 - val_loss: 0.9430 - val_acc: 0.8235\n",
      "Epoch 419/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.1124 - acc: 0.937 - ETA: 0s - loss: 0.1899 - acc: 0.917 - ETA: 0s - loss: 0.1859 - acc: 0.916 - ETA: 0s - loss: 0.1830 - acc: 0.918 - ETA: 0s - loss: 0.1790 - acc: 0.919 - ETA: 0s - loss: 0.1805 - acc: 0.918 - ETA: 0s - loss: 0.1804 - acc: 0.918 - ETA: 0s - loss: 0.1824 - acc: 0.918 - ETA: 0s - loss: 0.1836 - acc: 0.919 - ETA: 0s - loss: 0.1843 - acc: 0.918 - ETA: 0s - loss: 0.1856 - acc: 0.918 - ETA: 0s - loss: 0.1860 - acc: 0.918 - ETA: 0s - loss: 0.1859 - acc: 0.918 - ETA: 0s - loss: 0.1858 - acc: 0.917 - ETA: 0s - loss: 0.1853 - acc: 0.917 - ETA: 0s - loss: 0.1870 - acc: 0.917 - ETA: 0s - loss: 0.1892 - acc: 0.917 - ETA: 0s - loss: 0.1901 - acc: 0.916 - 1s 49us/step - loss: 0.1906 - acc: 0.9162 - val_loss: 0.9755 - val_acc: 0.8156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 420/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.1858 - acc: 0.906 - ETA: 0s - loss: 0.1619 - acc: 0.926 - ETA: 0s - loss: 0.1652 - acc: 0.923 - ETA: 0s - loss: 0.1761 - acc: 0.917 - ETA: 0s - loss: 0.1751 - acc: 0.919 - ETA: 0s - loss: 0.1834 - acc: 0.917 - ETA: 0s - loss: 0.1861 - acc: 0.916 - ETA: 0s - loss: 0.1920 - acc: 0.915 - ETA: 0s - loss: 0.1929 - acc: 0.915 - ETA: 0s - loss: 0.1900 - acc: 0.915 - ETA: 0s - loss: 0.1894 - acc: 0.916 - ETA: 0s - loss: 0.1873 - acc: 0.916 - ETA: 0s - loss: 0.1889 - acc: 0.916 - ETA: 0s - loss: 0.1898 - acc: 0.916 - ETA: 0s - loss: 0.1903 - acc: 0.916 - ETA: 0s - loss: 0.1911 - acc: 0.915 - ETA: 0s - loss: 0.1908 - acc: 0.915 - ETA: 0s - loss: 0.1901 - acc: 0.915 - 1s 49us/step - loss: 0.1910 - acc: 0.9155 - val_loss: 0.9380 - val_acc: 0.8161\n",
      "Epoch 421/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1315 - acc: 0.921 - ETA: 0s - loss: 0.1711 - acc: 0.925 - ETA: 0s - loss: 0.1808 - acc: 0.921 - ETA: 0s - loss: 0.1830 - acc: 0.920 - ETA: 0s - loss: 0.1770 - acc: 0.921 - ETA: 0s - loss: 0.1796 - acc: 0.920 - ETA: 0s - loss: 0.1809 - acc: 0.919 - ETA: 0s - loss: 0.1833 - acc: 0.919 - ETA: 0s - loss: 0.1836 - acc: 0.920 - ETA: 0s - loss: 0.1866 - acc: 0.918 - ETA: 0s - loss: 0.1896 - acc: 0.917 - ETA: 0s - loss: 0.1886 - acc: 0.917 - ETA: 0s - loss: 0.1877 - acc: 0.918 - ETA: 0s - loss: 0.1887 - acc: 0.918 - ETA: 0s - loss: 0.1900 - acc: 0.917 - ETA: 0s - loss: 0.1910 - acc: 0.917 - ETA: 0s - loss: 0.1935 - acc: 0.916 - ETA: 0s - loss: 0.1928 - acc: 0.916 - 1s 50us/step - loss: 0.1944 - acc: 0.9160 - val_loss: 0.9184 - val_acc: 0.8253\n",
      "Epoch 422/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.1971 - acc: 0.906 - ETA: 0s - loss: 0.1674 - acc: 0.918 - ETA: 0s - loss: 0.1719 - acc: 0.916 - ETA: 0s - loss: 0.1815 - acc: 0.915 - ETA: 0s - loss: 0.1892 - acc: 0.910 - ETA: 0s - loss: 0.1939 - acc: 0.910 - ETA: 0s - loss: 0.1931 - acc: 0.913 - ETA: 0s - loss: 0.1917 - acc: 0.914 - ETA: 0s - loss: 0.1912 - acc: 0.914 - ETA: 0s - loss: 0.1903 - acc: 0.914 - ETA: 0s - loss: 0.1897 - acc: 0.914 - ETA: 0s - loss: 0.1888 - acc: 0.915 - ETA: 0s - loss: 0.1897 - acc: 0.916 - ETA: 0s - loss: 0.1886 - acc: 0.916 - ETA: 0s - loss: 0.1885 - acc: 0.916 - ETA: 0s - loss: 0.1902 - acc: 0.916 - ETA: 0s - loss: 0.1908 - acc: 0.916 - ETA: 0s - loss: 0.1927 - acc: 0.915 - ETA: 0s - loss: 0.1931 - acc: 0.915 - 1s 52us/step - loss: 0.1935 - acc: 0.9155 - val_loss: 0.9223 - val_acc: 0.8263\n",
      "Epoch 423/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.1613 - acc: 0.937 - ETA: 0s - loss: 0.1626 - acc: 0.929 - ETA: 0s - loss: 0.1685 - acc: 0.924 - ETA: 0s - loss: 0.1745 - acc: 0.923 - ETA: 0s - loss: 0.1694 - acc: 0.925 - ETA: 0s - loss: 0.1716 - acc: 0.924 - ETA: 0s - loss: 0.1778 - acc: 0.922 - ETA: 0s - loss: 0.1788 - acc: 0.922 - ETA: 0s - loss: 0.1801 - acc: 0.921 - ETA: 0s - loss: 0.1819 - acc: 0.921 - ETA: 0s - loss: 0.1846 - acc: 0.920 - ETA: 0s - loss: 0.1843 - acc: 0.920 - ETA: 0s - loss: 0.1843 - acc: 0.919 - ETA: 0s - loss: 0.1869 - acc: 0.918 - ETA: 0s - loss: 0.1877 - acc: 0.917 - ETA: 0s - loss: 0.1891 - acc: 0.917 - ETA: 0s - loss: 0.1898 - acc: 0.916 - ETA: 0s - loss: 0.1901 - acc: 0.916 - 1s 51us/step - loss: 0.1894 - acc: 0.9162 - val_loss: 0.9311 - val_acc: 0.8223\n",
      "Epoch 424/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.2651 - acc: 0.843 - ETA: 0s - loss: 0.1445 - acc: 0.927 - ETA: 0s - loss: 0.1594 - acc: 0.925 - ETA: 0s - loss: 0.1675 - acc: 0.925 - ETA: 0s - loss: 0.1720 - acc: 0.921 - ETA: 0s - loss: 0.1850 - acc: 0.916 - ETA: 0s - loss: 0.1877 - acc: 0.915 - ETA: 0s - loss: 0.1862 - acc: 0.916 - ETA: 0s - loss: 0.1825 - acc: 0.918 - ETA: 0s - loss: 0.1845 - acc: 0.917 - ETA: 0s - loss: 0.1869 - acc: 0.916 - ETA: 0s - loss: 0.1854 - acc: 0.917 - ETA: 0s - loss: 0.1893 - acc: 0.916 - ETA: 0s - loss: 0.1891 - acc: 0.917 - ETA: 0s - loss: 0.1889 - acc: 0.917 - ETA: 0s - loss: 0.1890 - acc: 0.916 - ETA: 0s - loss: 0.1887 - acc: 0.916 - ETA: 0s - loss: 0.1893 - acc: 0.916 - ETA: 0s - loss: 0.1890 - acc: 0.916 - 1s 52us/step - loss: 0.1901 - acc: 0.9161 - val_loss: 0.8797 - val_acc: 0.8267\n",
      "Epoch 425/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.2121 - acc: 0.890 - ETA: 0s - loss: 0.1911 - acc: 0.907 - ETA: 0s - loss: 0.1804 - acc: 0.917 - ETA: 0s - loss: 0.1810 - acc: 0.915 - ETA: 0s - loss: 0.1845 - acc: 0.917 - ETA: 0s - loss: 0.1852 - acc: 0.915 - ETA: 0s - loss: 0.1875 - acc: 0.916 - ETA: 0s - loss: 0.1963 - acc: 0.915 - ETA: 0s - loss: 0.1955 - acc: 0.914 - ETA: 0s - loss: 0.1953 - acc: 0.914 - ETA: 0s - loss: 0.1955 - acc: 0.914 - ETA: 0s - loss: 0.1968 - acc: 0.913 - ETA: 0s - loss: 0.1957 - acc: 0.914 - ETA: 0s - loss: 0.1970 - acc: 0.913 - ETA: 0s - loss: 0.1976 - acc: 0.912 - ETA: 0s - loss: 0.1963 - acc: 0.913 - ETA: 0s - loss: 0.1940 - acc: 0.913 - ETA: 0s - loss: 0.1917 - acc: 0.914 - 1s 51us/step - loss: 0.1915 - acc: 0.9146 - val_loss: 0.9200 - val_acc: 0.8261\n",
      "Epoch 426/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.2321 - acc: 0.875 - ETA: 0s - loss: 0.1819 - acc: 0.925 - ETA: 0s - loss: 0.1846 - acc: 0.921 - ETA: 0s - loss: 0.1875 - acc: 0.916 - ETA: 0s - loss: 0.1911 - acc: 0.916 - ETA: 0s - loss: 0.1922 - acc: 0.915 - ETA: 0s - loss: 0.1906 - acc: 0.918 - ETA: 0s - loss: 0.1959 - acc: 0.917 - ETA: 0s - loss: 0.2017 - acc: 0.915 - ETA: 0s - loss: 0.2006 - acc: 0.916 - ETA: 0s - loss: 0.2013 - acc: 0.916 - ETA: 0s - loss: 0.2002 - acc: 0.916 - ETA: 0s - loss: 0.2011 - acc: 0.916 - ETA: 0s - loss: 0.1983 - acc: 0.917 - ETA: 0s - loss: 0.1990 - acc: 0.916 - ETA: 0s - loss: 0.2003 - acc: 0.916 - ETA: 0s - loss: 0.1993 - acc: 0.917 - ETA: 0s - loss: 0.2009 - acc: 0.916 - ETA: 0s - loss: 0.1998 - acc: 0.916 - 1s 51us/step - loss: 0.1995 - acc: 0.9167 - val_loss: 0.9924 - val_acc: 0.8144\n",
      "Epoch 427/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.1479 - acc: 0.937 - ETA: 0s - loss: 0.1870 - acc: 0.911 - ETA: 0s - loss: 0.1854 - acc: 0.911 - ETA: 0s - loss: 0.1862 - acc: 0.915 - ETA: 0s - loss: 0.1852 - acc: 0.915 - ETA: 0s - loss: 0.1850 - acc: 0.915 - ETA: 0s - loss: 0.1870 - acc: 0.916 - ETA: 0s - loss: 0.1912 - acc: 0.915 - ETA: 0s - loss: 0.1902 - acc: 0.915 - ETA: 0s - loss: 0.1919 - acc: 0.914 - ETA: 0s - loss: 0.1917 - acc: 0.915 - ETA: 0s - loss: 0.1924 - acc: 0.915 - ETA: 0s - loss: 0.1910 - acc: 0.916 - ETA: 0s - loss: 0.1898 - acc: 0.917 - ETA: 0s - loss: 0.1907 - acc: 0.916 - ETA: 0s - loss: 0.1922 - acc: 0.916 - ETA: 0s - loss: 0.1926 - acc: 0.916 - ETA: 0s - loss: 0.1919 - acc: 0.916 - 1s 51us/step - loss: 0.1924 - acc: 0.9165 - val_loss: 0.8867 - val_acc: 0.8312\n",
      "Epoch 428/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.2000 - acc: 0.953 - ETA: 0s - loss: 0.1724 - acc: 0.918 - ETA: 0s - loss: 0.1794 - acc: 0.911 - ETA: 0s - loss: 0.1779 - acc: 0.915 - ETA: 0s - loss: 0.1865 - acc: 0.914 - ETA: 0s - loss: 0.1856 - acc: 0.917 - ETA: 0s - loss: 0.1891 - acc: 0.916 - ETA: 0s - loss: 0.1961 - acc: 0.915 - ETA: 0s - loss: 0.1997 - acc: 0.915 - ETA: 0s - loss: 0.1970 - acc: 0.916 - ETA: 0s - loss: 0.1977 - acc: 0.914 - ETA: 0s - loss: 0.1953 - acc: 0.915 - ETA: 0s - loss: 0.1953 - acc: 0.915 - ETA: 0s - loss: 0.1958 - acc: 0.914 - ETA: 0s - loss: 0.1947 - acc: 0.915 - ETA: 0s - loss: 0.1957 - acc: 0.914 - ETA: 0s - loss: 0.1947 - acc: 0.915 - ETA: 0s - loss: 0.1941 - acc: 0.915 - 1s 50us/step - loss: 0.1933 - acc: 0.9153 - val_loss: 0.8810 - val_acc: 0.8228\n",
      "Epoch 429/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.2240 - acc: 0.890 - ETA: 0s - loss: 0.1864 - acc: 0.925 - ETA: 0s - loss: 0.1864 - acc: 0.921 - ETA: 0s - loss: 0.1859 - acc: 0.918 - ETA: 0s - loss: 0.1935 - acc: 0.915 - ETA: 0s - loss: 0.1961 - acc: 0.915 - ETA: 0s - loss: 0.1940 - acc: 0.916 - ETA: 0s - loss: 0.1929 - acc: 0.915 - ETA: 0s - loss: 0.1941 - acc: 0.916 - ETA: 0s - loss: 0.1906 - acc: 0.916 - ETA: 0s - loss: 0.1900 - acc: 0.917 - ETA: 0s - loss: 0.1886 - acc: 0.917 - ETA: 0s - loss: 0.1910 - acc: 0.917 - ETA: 0s - loss: 0.1922 - acc: 0.916 - ETA: 0s - loss: 0.1906 - acc: 0.916 - ETA: 0s - loss: 0.1904 - acc: 0.916 - ETA: 0s - loss: 0.1916 - acc: 0.915 - ETA: 0s - loss: 0.1904 - acc: 0.916 - 1s 51us/step - loss: 0.1911 - acc: 0.9162 - val_loss: 0.9356 - val_acc: 0.8237\n",
      "Epoch 430/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.2197 - acc: 0.890 - ETA: 0s - loss: 0.1823 - acc: 0.919 - ETA: 0s - loss: 0.1911 - acc: 0.918 - ETA: 0s - loss: 0.1958 - acc: 0.913 - ETA: 0s - loss: 0.1949 - acc: 0.914 - ETA: 0s - loss: 0.1916 - acc: 0.917 - ETA: 0s - loss: 0.1879 - acc: 0.918 - ETA: 0s - loss: 0.1919 - acc: 0.916 - ETA: 0s - loss: 0.1940 - acc: 0.914 - ETA: 0s - loss: 0.1941 - acc: 0.914 - ETA: 0s - loss: 0.1943 - acc: 0.914 - ETA: 0s - loss: 0.1933 - acc: 0.915 - ETA: 0s - loss: 0.1941 - acc: 0.914 - ETA: 0s - loss: 0.1921 - acc: 0.915 - ETA: 0s - loss: 0.1908 - acc: 0.917 - ETA: 0s - loss: 0.1906 - acc: 0.917 - ETA: 0s - loss: 0.1899 - acc: 0.916 - ETA: 0s - loss: 0.1918 - acc: 0.915 - 1s 50us/step - loss: 0.1907 - acc: 0.9162 - val_loss: 0.9643 - val_acc: 0.8243\n",
      "Epoch 431/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.1897 - acc: 0.953 - ETA: 0s - loss: 0.1592 - acc: 0.934 - ETA: 0s - loss: 0.1858 - acc: 0.922 - ETA: 0s - loss: 0.1833 - acc: 0.919 - ETA: 0s - loss: 0.1836 - acc: 0.918 - ETA: 0s - loss: 0.1905 - acc: 0.915 - ETA: 0s - loss: 0.1904 - acc: 0.915 - ETA: 0s - loss: 0.1904 - acc: 0.917 - ETA: 0s - loss: 0.1892 - acc: 0.918 - ETA: 0s - loss: 0.1904 - acc: 0.917 - ETA: 0s - loss: 0.1917 - acc: 0.915 - ETA: 0s - loss: 0.1907 - acc: 0.915 - ETA: 0s - loss: 0.1901 - acc: 0.915 - ETA: 0s - loss: 0.1910 - acc: 0.914 - ETA: 0s - loss: 0.1895 - acc: 0.915 - ETA: 0s - loss: 0.1891 - acc: 0.915 - ETA: 0s - loss: 0.1899 - acc: 0.914 - ETA: 0s - loss: 0.1912 - acc: 0.914 - 1s 49us/step - loss: 0.1911 - acc: 0.9145 - val_loss: 0.8916 - val_acc: 0.8190\n",
      "Epoch 432/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.2126 - acc: 0.921 - ETA: 0s - loss: 0.1872 - acc: 0.917 - ETA: 0s - loss: 0.1897 - acc: 0.918 - ETA: 0s - loss: 0.1836 - acc: 0.919 - ETA: 0s - loss: 0.1842 - acc: 0.921 - ETA: 0s - loss: 0.1827 - acc: 0.921 - ETA: 0s - loss: 0.1853 - acc: 0.921 - ETA: 0s - loss: 0.1832 - acc: 0.922 - ETA: 0s - loss: 0.1861 - acc: 0.920 - ETA: 0s - loss: 0.1855 - acc: 0.920 - ETA: 0s - loss: 0.1872 - acc: 0.919 - ETA: 0s - loss: 0.1887 - acc: 0.918 - ETA: 0s - loss: 0.1892 - acc: 0.918 - ETA: 0s - loss: 0.1907 - acc: 0.917 - ETA: 0s - loss: 0.1899 - acc: 0.918 - ETA: 0s - loss: 0.1896 - acc: 0.918 - ETA: 0s - loss: 0.1905 - acc: 0.917 - ETA: 0s - loss: 0.1915 - acc: 0.917 - 1s 49us/step - loss: 0.1914 - acc: 0.9175 - val_loss: 1.0019 - val_acc: 0.8123\n",
      "Epoch 433/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1398 - acc: 0.953 - ETA: 0s - loss: 0.1819 - acc: 0.926 - ETA: 0s - loss: 0.1857 - acc: 0.918 - ETA: 0s - loss: 0.1788 - acc: 0.921 - ETA: 0s - loss: 0.1767 - acc: 0.922 - ETA: 0s - loss: 0.1761 - acc: 0.921 - ETA: 0s - loss: 0.1775 - acc: 0.921 - ETA: 0s - loss: 0.1822 - acc: 0.920 - ETA: 0s - loss: 0.1851 - acc: 0.920 - ETA: 0s - loss: 0.1854 - acc: 0.919 - ETA: 0s - loss: 0.1876 - acc: 0.919 - ETA: 0s - loss: 0.1890 - acc: 0.917 - ETA: 0s - loss: 0.1885 - acc: 0.917 - ETA: 0s - loss: 0.1871 - acc: 0.917 - ETA: 0s - loss: 0.1874 - acc: 0.917 - ETA: 0s - loss: 0.1873 - acc: 0.917 - ETA: 0s - loss: 0.1866 - acc: 0.918 - ETA: 0s - loss: 0.1873 - acc: 0.917 - 1s 51us/step - loss: 0.1880 - acc: 0.9174 - val_loss: 0.9067 - val_acc: 0.8228\n",
      "Epoch 434/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1426 - acc: 0.953 - ETA: 0s - loss: 0.1629 - acc: 0.921 - ETA: 0s - loss: 0.1607 - acc: 0.921 - ETA: 0s - loss: 0.1661 - acc: 0.923 - ETA: 0s - loss: 0.1784 - acc: 0.918 - ETA: 0s - loss: 0.1766 - acc: 0.919 - ETA: 0s - loss: 0.1785 - acc: 0.918 - ETA: 0s - loss: 0.1768 - acc: 0.919 - ETA: 0s - loss: 0.1792 - acc: 0.918 - ETA: 0s - loss: 0.1832 - acc: 0.917 - ETA: 0s - loss: 0.1842 - acc: 0.916 - ETA: 0s - loss: 0.1837 - acc: 0.916 - ETA: 0s - loss: 0.1851 - acc: 0.916 - ETA: 0s - loss: 0.1854 - acc: 0.916 - ETA: 0s - loss: 0.1858 - acc: 0.917 - ETA: 0s - loss: 0.1883 - acc: 0.916 - ETA: 0s - loss: 0.1891 - acc: 0.916 - ETA: 0s - loss: 0.1890 - acc: 0.916 - 1s 50us/step - loss: 0.1887 - acc: 0.9168 - val_loss: 0.9241 - val_acc: 0.8277\n",
      "Epoch 435/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.1817 - acc: 0.906 - ETA: 0s - loss: 0.1614 - acc: 0.921 - ETA: 0s - loss: 0.1820 - acc: 0.917 - ETA: 0s - loss: 0.1789 - acc: 0.919 - ETA: 0s - loss: 0.1819 - acc: 0.919 - ETA: 0s - loss: 0.1972 - acc: 0.915 - ETA: 0s - loss: 0.2005 - acc: 0.915 - ETA: 0s - loss: 0.1946 - acc: 0.917 - ETA: 0s - loss: 0.1922 - acc: 0.917 - ETA: 0s - loss: 0.1928 - acc: 0.917 - ETA: 0s - loss: 0.1930 - acc: 0.917 - ETA: 0s - loss: 0.1931 - acc: 0.916 - ETA: 0s - loss: 0.1905 - acc: 0.917 - ETA: 0s - loss: 0.1906 - acc: 0.918 - ETA: 0s - loss: 0.1909 - acc: 0.917 - ETA: 0s - loss: 0.1904 - acc: 0.917 - ETA: 0s - loss: 0.1903 - acc: 0.917 - ETA: 0s - loss: 0.1911 - acc: 0.916 - 1s 51us/step - loss: 0.1907 - acc: 0.9167 - val_loss: 0.9187 - val_acc: 0.8267\n",
      "Epoch 436/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1687 - acc: 0.937 - ETA: 0s - loss: 0.1869 - acc: 0.915 - ETA: 0s - loss: 0.1989 - acc: 0.917 - ETA: 0s - loss: 0.1960 - acc: 0.918 - ETA: 0s - loss: 0.1937 - acc: 0.916 - ETA: 0s - loss: 0.1869 - acc: 0.919 - ETA: 0s - loss: 0.1885 - acc: 0.918 - ETA: 0s - loss: 0.1862 - acc: 0.919 - ETA: 0s - loss: 0.1889 - acc: 0.918 - ETA: 0s - loss: 0.1895 - acc: 0.918 - ETA: 0s - loss: 0.1919 - acc: 0.917 - ETA: 0s - loss: 0.1902 - acc: 0.917 - ETA: 0s - loss: 0.1886 - acc: 0.918 - ETA: 0s - loss: 0.1866 - acc: 0.919 - ETA: 0s - loss: 0.1860 - acc: 0.918 - ETA: 0s - loss: 0.1863 - acc: 0.918 - ETA: 0s - loss: 0.1867 - acc: 0.917 - ETA: 0s - loss: 0.1870 - acc: 0.917 - ETA: 0s - loss: 0.1883 - acc: 0.917 - 1s 51us/step - loss: 0.1883 - acc: 0.9171 - val_loss: 0.9714 - val_acc: 0.8193\n",
      "Epoch 437/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.1903 - acc: 0.921 - ETA: 0s - loss: 0.1997 - acc: 0.920 - ETA: 0s - loss: 0.1928 - acc: 0.919 - ETA: 0s - loss: 0.1953 - acc: 0.915 - ETA: 0s - loss: 0.1936 - acc: 0.915 - ETA: 0s - loss: 0.1906 - acc: 0.918 - ETA: 0s - loss: 0.1895 - acc: 0.917 - ETA: 0s - loss: 0.1883 - acc: 0.916 - ETA: 0s - loss: 0.1883 - acc: 0.918 - ETA: 0s - loss: 0.1892 - acc: 0.918 - ETA: 0s - loss: 0.1905 - acc: 0.918 - ETA: 0s - loss: 0.1910 - acc: 0.918 - ETA: 0s - loss: 0.1922 - acc: 0.919 - ETA: 0s - loss: 0.1915 - acc: 0.918 - ETA: 0s - loss: 0.1924 - acc: 0.918 - ETA: 0s - loss: 0.1936 - acc: 0.917 - ETA: 0s - loss: 0.1951 - acc: 0.916 - ETA: 0s - loss: 0.1947 - acc: 0.916 - 1s 50us/step - loss: 0.1943 - acc: 0.9162 - val_loss: 0.9815 - val_acc: 0.8180\n",
      "Epoch 438/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.1165 - acc: 0.968 - ETA: 0s - loss: 0.1541 - acc: 0.939 - ETA: 0s - loss: 0.1759 - acc: 0.924 - ETA: 0s - loss: 0.1869 - acc: 0.924 - ETA: 0s - loss: 0.1894 - acc: 0.922 - ETA: 0s - loss: 0.1866 - acc: 0.920 - ETA: 0s - loss: 0.1862 - acc: 0.919 - ETA: 0s - loss: 0.1860 - acc: 0.918 - ETA: 0s - loss: 0.1822 - acc: 0.920 - ETA: 0s - loss: 0.1817 - acc: 0.920 - ETA: 0s - loss: 0.1825 - acc: 0.919 - ETA: 0s - loss: 0.1849 - acc: 0.918 - ETA: 0s - loss: 0.1868 - acc: 0.918 - ETA: 0s - loss: 0.1863 - acc: 0.918 - ETA: 0s - loss: 0.1870 - acc: 0.918 - ETA: 0s - loss: 0.1906 - acc: 0.917 - ETA: 0s - loss: 0.1916 - acc: 0.917 - ETA: 0s - loss: 0.1918 - acc: 0.917 - 1s 51us/step - loss: 0.1914 - acc: 0.9175 - val_loss: 0.9437 - val_acc: 0.8248\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 439/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.1161 - acc: 0.921 - ETA: 0s - loss: 0.1905 - acc: 0.920 - ETA: 0s - loss: 0.1892 - acc: 0.918 - ETA: 0s - loss: 0.1858 - acc: 0.918 - ETA: 0s - loss: 0.1843 - acc: 0.919 - ETA: 0s - loss: 0.1814 - acc: 0.919 - ETA: 0s - loss: 0.1788 - acc: 0.919 - ETA: 0s - loss: 0.1804 - acc: 0.919 - ETA: 0s - loss: 0.1838 - acc: 0.918 - ETA: 0s - loss: 0.1826 - acc: 0.918 - ETA: 0s - loss: 0.1862 - acc: 0.917 - ETA: 0s - loss: 0.1918 - acc: 0.916 - ETA: 0s - loss: 0.1919 - acc: 0.916 - ETA: 0s - loss: 0.1911 - acc: 0.916 - ETA: 0s - loss: 0.1907 - acc: 0.916 - ETA: 0s - loss: 0.1902 - acc: 0.916 - ETA: 0s - loss: 0.1931 - acc: 0.916 - ETA: 0s - loss: 0.1917 - acc: 0.916 - ETA: 0s - loss: 0.1917 - acc: 0.917 - 1s 51us/step - loss: 0.1915 - acc: 0.9175 - val_loss: 0.9216 - val_acc: 0.8248\n",
      "Epoch 440/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.2770 - acc: 0.875 - ETA: 0s - loss: 0.1784 - acc: 0.916 - ETA: 0s - loss: 0.1679 - acc: 0.927 - ETA: 0s - loss: 0.1744 - acc: 0.924 - ETA: 0s - loss: 0.1786 - acc: 0.922 - ETA: 0s - loss: 0.1781 - acc: 0.922 - ETA: 0s - loss: 0.1839 - acc: 0.919 - ETA: 0s - loss: 0.1876 - acc: 0.919 - ETA: 0s - loss: 0.1888 - acc: 0.917 - ETA: 0s - loss: 0.1879 - acc: 0.918 - ETA: 0s - loss: 0.1858 - acc: 0.918 - ETA: 0s - loss: 0.1861 - acc: 0.918 - ETA: 0s - loss: 0.1857 - acc: 0.918 - ETA: 0s - loss: 0.1848 - acc: 0.918 - ETA: 0s - loss: 0.1837 - acc: 0.919 - ETA: 0s - loss: 0.1854 - acc: 0.919 - ETA: 0s - loss: 0.1883 - acc: 0.918 - ETA: 0s - loss: 0.1898 - acc: 0.917 - 1s 50us/step - loss: 0.1909 - acc: 0.9171 - val_loss: 0.8836 - val_acc: 0.8286\n",
      "Epoch 441/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1926 - acc: 0.859 - ETA: 0s - loss: 0.1648 - acc: 0.925 - ETA: 0s - loss: 0.1631 - acc: 0.930 - ETA: 0s - loss: 0.1757 - acc: 0.924 - ETA: 0s - loss: 0.1811 - acc: 0.922 - ETA: 0s - loss: 0.1823 - acc: 0.921 - ETA: 0s - loss: 0.1813 - acc: 0.921 - ETA: 0s - loss: 0.1834 - acc: 0.919 - ETA: 0s - loss: 0.1864 - acc: 0.918 - ETA: 0s - loss: 0.1899 - acc: 0.915 - ETA: 0s - loss: 0.1918 - acc: 0.915 - ETA: 0s - loss: 0.1913 - acc: 0.916 - ETA: 0s - loss: 0.1922 - acc: 0.916 - ETA: 0s - loss: 0.1909 - acc: 0.916 - ETA: 0s - loss: 0.1904 - acc: 0.916 - ETA: 0s - loss: 0.1892 - acc: 0.917 - ETA: 0s - loss: 0.1893 - acc: 0.917 - ETA: 0s - loss: 0.1905 - acc: 0.916 - 1s 51us/step - loss: 0.1903 - acc: 0.9161 - val_loss: 0.9129 - val_acc: 0.8265\n",
      "Epoch 442/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1963 - acc: 0.937 - ETA: 0s - loss: 0.1850 - acc: 0.911 - ETA: 0s - loss: 0.1786 - acc: 0.912 - ETA: 0s - loss: 0.1902 - acc: 0.909 - ETA: 0s - loss: 0.1874 - acc: 0.912 - ETA: 0s - loss: 0.1918 - acc: 0.915 - ETA: 0s - loss: 0.1843 - acc: 0.918 - ETA: 0s - loss: 0.1838 - acc: 0.918 - ETA: 0s - loss: 0.1876 - acc: 0.917 - ETA: 0s - loss: 0.1869 - acc: 0.917 - ETA: 0s - loss: 0.1895 - acc: 0.916 - ETA: 0s - loss: 0.1926 - acc: 0.916 - ETA: 0s - loss: 0.1920 - acc: 0.916 - ETA: 0s - loss: 0.1928 - acc: 0.916 - ETA: 0s - loss: 0.1944 - acc: 0.916 - ETA: 0s - loss: 0.1930 - acc: 0.916 - ETA: 0s - loss: 0.1919 - acc: 0.916 - ETA: 0s - loss: 0.1918 - acc: 0.916 - 1s 51us/step - loss: 0.1936 - acc: 0.9159 - val_loss: 0.9235 - val_acc: 0.8198\n",
      "Epoch 443/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.3538 - acc: 0.843 - ETA: 0s - loss: 0.2180 - acc: 0.917 - ETA: 0s - loss: 0.2058 - acc: 0.912 - ETA: 0s - loss: 0.1943 - acc: 0.917 - ETA: 0s - loss: 0.1935 - acc: 0.917 - ETA: 0s - loss: 0.1907 - acc: 0.916 - ETA: 0s - loss: 0.1928 - acc: 0.916 - ETA: 0s - loss: 0.1934 - acc: 0.915 - ETA: 0s - loss: 0.1960 - acc: 0.913 - ETA: 0s - loss: 0.1939 - acc: 0.915 - ETA: 0s - loss: 0.1904 - acc: 0.916 - ETA: 0s - loss: 0.1928 - acc: 0.916 - ETA: 0s - loss: 0.1950 - acc: 0.914 - ETA: 0s - loss: 0.1946 - acc: 0.914 - ETA: 0s - loss: 0.1960 - acc: 0.914 - ETA: 0s - loss: 0.1946 - acc: 0.914 - ETA: 0s - loss: 0.1930 - acc: 0.915 - ETA: 0s - loss: 0.1917 - acc: 0.915 - 1s 51us/step - loss: 0.1916 - acc: 0.9160 - val_loss: 0.9517 - val_acc: 0.8165\n",
      "Epoch 444/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.2043 - acc: 0.906 - ETA: 0s - loss: 0.1912 - acc: 0.913 - ETA: 0s - loss: 0.1868 - acc: 0.918 - ETA: 0s - loss: 0.1869 - acc: 0.920 - ETA: 0s - loss: 0.1854 - acc: 0.920 - ETA: 0s - loss: 0.1859 - acc: 0.918 - ETA: 0s - loss: 0.1853 - acc: 0.917 - ETA: 0s - loss: 0.1849 - acc: 0.916 - ETA: 0s - loss: 0.1852 - acc: 0.917 - ETA: 0s - loss: 0.1851 - acc: 0.917 - ETA: 0s - loss: 0.1870 - acc: 0.917 - ETA: 0s - loss: 0.1910 - acc: 0.916 - ETA: 0s - loss: 0.1920 - acc: 0.915 - ETA: 0s - loss: 0.1904 - acc: 0.916 - ETA: 0s - loss: 0.1904 - acc: 0.917 - ETA: 0s - loss: 0.1892 - acc: 0.917 - ETA: 0s - loss: 0.1899 - acc: 0.917 - ETA: 0s - loss: 0.1894 - acc: 0.917 - ETA: 0s - loss: 0.1904 - acc: 0.917 - 1s 52us/step - loss: 0.1904 - acc: 0.9172 - val_loss: 0.9174 - val_acc: 0.8234\n",
      "Epoch 445/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.1895 - acc: 0.953 - ETA: 0s - loss: 0.2040 - acc: 0.914 - ETA: 0s - loss: 0.1981 - acc: 0.913 - ETA: 0s - loss: 0.1950 - acc: 0.916 - ETA: 0s - loss: 0.1945 - acc: 0.917 - ETA: 0s - loss: 0.1971 - acc: 0.914 - ETA: 0s - loss: 0.1940 - acc: 0.915 - ETA: 0s - loss: 0.1934 - acc: 0.915 - ETA: 0s - loss: 0.1931 - acc: 0.916 - ETA: 0s - loss: 0.1920 - acc: 0.914 - ETA: 0s - loss: 0.1913 - acc: 0.914 - ETA: 0s - loss: 0.1913 - acc: 0.915 - ETA: 0s - loss: 0.1933 - acc: 0.915 - ETA: 0s - loss: 0.1933 - acc: 0.915 - ETA: 0s - loss: 0.1902 - acc: 0.916 - ETA: 0s - loss: 0.1910 - acc: 0.915 - ETA: 0s - loss: 0.1907 - acc: 0.915 - ETA: 0s - loss: 0.1908 - acc: 0.915 - 1s 50us/step - loss: 0.1900 - acc: 0.9156 - val_loss: 0.9248 - val_acc: 0.8236\n",
      "Epoch 446/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.2211 - acc: 0.890 - ETA: 0s - loss: 0.2205 - acc: 0.909 - ETA: 0s - loss: 0.2052 - acc: 0.916 - ETA: 0s - loss: 0.2082 - acc: 0.914 - ETA: 0s - loss: 0.2046 - acc: 0.915 - ETA: 0s - loss: 0.1998 - acc: 0.916 - ETA: 0s - loss: 0.1973 - acc: 0.918 - ETA: 0s - loss: 0.1964 - acc: 0.918 - ETA: 0s - loss: 0.1921 - acc: 0.920 - ETA: 0s - loss: 0.1928 - acc: 0.920 - ETA: 0s - loss: 0.1976 - acc: 0.917 - ETA: 0s - loss: 0.1970 - acc: 0.917 - ETA: 0s - loss: 0.1966 - acc: 0.917 - ETA: 0s - loss: 0.1997 - acc: 0.915 - ETA: 0s - loss: 0.1963 - acc: 0.916 - ETA: 0s - loss: 0.1953 - acc: 0.916 - ETA: 0s - loss: 0.1945 - acc: 0.916 - ETA: 0s - loss: 0.1945 - acc: 0.916 - 1s 51us/step - loss: 0.1942 - acc: 0.9163 - val_loss: 0.9513 - val_acc: 0.8187\n",
      "Epoch 447/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.4750 - acc: 0.843 - ETA: 0s - loss: 0.2033 - acc: 0.925 - ETA: 0s - loss: 0.2024 - acc: 0.921 - ETA: 0s - loss: 0.2034 - acc: 0.918 - ETA: 0s - loss: 0.2073 - acc: 0.914 - ETA: 0s - loss: 0.2035 - acc: 0.913 - ETA: 0s - loss: 0.1984 - acc: 0.916 - ETA: 0s - loss: 0.1957 - acc: 0.917 - ETA: 0s - loss: 0.1943 - acc: 0.919 - ETA: 0s - loss: 0.1931 - acc: 0.919 - ETA: 0s - loss: 0.1924 - acc: 0.918 - ETA: 0s - loss: 0.1924 - acc: 0.917 - ETA: 0s - loss: 0.1914 - acc: 0.917 - ETA: 0s - loss: 0.1929 - acc: 0.917 - ETA: 0s - loss: 0.1943 - acc: 0.916 - ETA: 0s - loss: 0.1921 - acc: 0.917 - ETA: 0s - loss: 0.1941 - acc: 0.916 - ETA: 0s - loss: 0.1934 - acc: 0.916 - ETA: 0s - loss: 0.1953 - acc: 0.916 - 1s 52us/step - loss: 0.1953 - acc: 0.9161 - val_loss: 0.9312 - val_acc: 0.8246\n",
      "Epoch 448/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.1061 - acc: 0.968 - ETA: 0s - loss: 0.1653 - acc: 0.932 - ETA: 0s - loss: 0.1648 - acc: 0.931 - ETA: 0s - loss: 0.1675 - acc: 0.930 - ETA: 0s - loss: 0.1686 - acc: 0.930 - ETA: 0s - loss: 0.1724 - acc: 0.929 - ETA: 0s - loss: 0.1755 - acc: 0.928 - ETA: 0s - loss: 0.1739 - acc: 0.927 - ETA: 0s - loss: 0.1757 - acc: 0.925 - ETA: 0s - loss: 0.1762 - acc: 0.926 - ETA: 0s - loss: 0.1807 - acc: 0.924 - ETA: 0s - loss: 0.1829 - acc: 0.923 - ETA: 0s - loss: 0.1822 - acc: 0.923 - ETA: 0s - loss: 0.1810 - acc: 0.923 - ETA: 0s - loss: 0.1854 - acc: 0.921 - ETA: 0s - loss: 0.1878 - acc: 0.919 - ETA: 0s - loss: 0.1888 - acc: 0.920 - ETA: 0s - loss: 0.1893 - acc: 0.918 - ETA: 0s - loss: 0.1895 - acc: 0.918 - 1s 52us/step - loss: 0.1907 - acc: 0.9183 - val_loss: 0.9016 - val_acc: 0.8218\n",
      "Epoch 449/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.4071 - acc: 0.937 - ETA: 0s - loss: 0.1947 - acc: 0.933 - ETA: 0s - loss: 0.1858 - acc: 0.925 - ETA: 0s - loss: 0.1855 - acc: 0.923 - ETA: 0s - loss: 0.1841 - acc: 0.923 - ETA: 0s - loss: 0.1831 - acc: 0.921 - ETA: 0s - loss: 0.1858 - acc: 0.918 - ETA: 0s - loss: 0.1844 - acc: 0.918 - ETA: 0s - loss: 0.1892 - acc: 0.916 - ETA: 0s - loss: 0.1902 - acc: 0.916 - ETA: 0s - loss: 0.1888 - acc: 0.917 - ETA: 0s - loss: 0.1895 - acc: 0.915 - ETA: 0s - loss: 0.1924 - acc: 0.915 - ETA: 0s - loss: 0.1901 - acc: 0.916 - ETA: 0s - loss: 0.1887 - acc: 0.916 - ETA: 0s - loss: 0.1890 - acc: 0.916 - ETA: 0s - loss: 0.1898 - acc: 0.916 - ETA: 0s - loss: 0.1917 - acc: 0.915 - 1s 49us/step - loss: 0.1920 - acc: 0.9160 - val_loss: 0.9960 - val_acc: 0.8152\n",
      "Epoch 450/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.2584 - acc: 0.859 - ETA: 0s - loss: 0.1645 - acc: 0.925 - ETA: 0s - loss: 0.1800 - acc: 0.919 - ETA: 0s - loss: 0.1843 - acc: 0.918 - ETA: 0s - loss: 0.1906 - acc: 0.918 - ETA: 0s - loss: 0.1865 - acc: 0.919 - ETA: 0s - loss: 0.1863 - acc: 0.920 - ETA: 0s - loss: 0.1854 - acc: 0.920 - ETA: 0s - loss: 0.1880 - acc: 0.919 - ETA: 0s - loss: 0.1850 - acc: 0.920 - ETA: 0s - loss: 0.1833 - acc: 0.920 - ETA: 0s - loss: 0.1824 - acc: 0.921 - ETA: 0s - loss: 0.1862 - acc: 0.919 - ETA: 0s - loss: 0.1866 - acc: 0.919 - ETA: 0s - loss: 0.1893 - acc: 0.918 - ETA: 0s - loss: 0.1892 - acc: 0.917 - ETA: 0s - loss: 0.1880 - acc: 0.918 - ETA: 0s - loss: 0.1893 - acc: 0.917 - 1s 50us/step - loss: 0.1900 - acc: 0.9174 - val_loss: 0.9076 - val_acc: 0.8267\n",
      "Epoch 451/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1880 - acc: 0.937 - ETA: 0s - loss: 0.1877 - acc: 0.932 - ETA: 0s - loss: 0.2145 - acc: 0.919 - ETA: 0s - loss: 0.2045 - acc: 0.920 - ETA: 0s - loss: 0.2022 - acc: 0.918 - ETA: 0s - loss: 0.1947 - acc: 0.919 - ETA: 0s - loss: 0.1922 - acc: 0.919 - ETA: 0s - loss: 0.1926 - acc: 0.919 - ETA: 0s - loss: 0.1882 - acc: 0.920 - ETA: 0s - loss: 0.1896 - acc: 0.920 - ETA: 0s - loss: 0.1884 - acc: 0.920 - ETA: 0s - loss: 0.1875 - acc: 0.920 - ETA: 0s - loss: 0.1882 - acc: 0.919 - ETA: 0s - loss: 0.1869 - acc: 0.919 - ETA: 0s - loss: 0.1885 - acc: 0.919 - ETA: 0s - loss: 0.1885 - acc: 0.918 - ETA: 0s - loss: 0.1906 - acc: 0.918 - ETA: 0s - loss: 0.1923 - acc: 0.917 - 1s 50us/step - loss: 0.1931 - acc: 0.9172 - val_loss: 0.9515 - val_acc: 0.8256\n",
      "Epoch 452/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.2423 - acc: 0.828 - ETA: 0s - loss: 0.1767 - acc: 0.914 - ETA: 0s - loss: 0.1685 - acc: 0.919 - ETA: 0s - loss: 0.1746 - acc: 0.919 - ETA: 0s - loss: 0.1711 - acc: 0.923 - ETA: 0s - loss: 0.1756 - acc: 0.922 - ETA: 0s - loss: 0.1786 - acc: 0.924 - ETA: 0s - loss: 0.1789 - acc: 0.924 - ETA: 0s - loss: 0.1808 - acc: 0.923 - ETA: 0s - loss: 0.1821 - acc: 0.921 - ETA: 0s - loss: 0.1839 - acc: 0.920 - ETA: 0s - loss: 0.1856 - acc: 0.919 - ETA: 0s - loss: 0.1879 - acc: 0.918 - ETA: 0s - loss: 0.1885 - acc: 0.917 - ETA: 0s - loss: 0.1880 - acc: 0.917 - ETA: 0s - loss: 0.1894 - acc: 0.916 - ETA: 0s - loss: 0.1884 - acc: 0.916 - ETA: 0s - loss: 0.1891 - acc: 0.916 - 1s 50us/step - loss: 0.1911 - acc: 0.9161 - val_loss: 0.8315 - val_acc: 0.8337\n",
      "Epoch 453/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.2363 - acc: 0.937 - ETA: 0s - loss: 0.1833 - acc: 0.912 - ETA: 0s - loss: 0.1857 - acc: 0.914 - ETA: 0s - loss: 0.1850 - acc: 0.914 - ETA: 0s - loss: 0.1844 - acc: 0.914 - ETA: 0s - loss: 0.1823 - acc: 0.916 - ETA: 0s - loss: 0.1855 - acc: 0.915 - ETA: 0s - loss: 0.1825 - acc: 0.915 - ETA: 0s - loss: 0.1840 - acc: 0.917 - ETA: 0s - loss: 0.1837 - acc: 0.917 - ETA: 0s - loss: 0.1830 - acc: 0.917 - ETA: 0s - loss: 0.1841 - acc: 0.917 - ETA: 0s - loss: 0.1850 - acc: 0.917 - ETA: 0s - loss: 0.1834 - acc: 0.918 - ETA: 0s - loss: 0.1863 - acc: 0.917 - ETA: 0s - loss: 0.1865 - acc: 0.917 - ETA: 0s - loss: 0.1874 - acc: 0.917 - ETA: 0s - loss: 0.1888 - acc: 0.916 - 1s 50us/step - loss: 0.1904 - acc: 0.9165 - val_loss: 0.8960 - val_acc: 0.8256\n",
      "Epoch 454/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.2953 - acc: 0.843 - ETA: 0s - loss: 0.1950 - acc: 0.912 - ETA: 0s - loss: 0.1871 - acc: 0.912 - ETA: 0s - loss: 0.1860 - acc: 0.913 - ETA: 0s - loss: 0.1815 - acc: 0.915 - ETA: 0s - loss: 0.1819 - acc: 0.915 - ETA: 0s - loss: 0.1832 - acc: 0.913 - ETA: 0s - loss: 0.1843 - acc: 0.913 - ETA: 0s - loss: 0.1883 - acc: 0.913 - ETA: 0s - loss: 0.1878 - acc: 0.914 - ETA: 0s - loss: 0.1880 - acc: 0.914 - ETA: 0s - loss: 0.1898 - acc: 0.914 - ETA: 0s - loss: 0.1890 - acc: 0.915 - ETA: 0s - loss: 0.1906 - acc: 0.915 - ETA: 0s - loss: 0.1923 - acc: 0.914 - ETA: 0s - loss: 0.1913 - acc: 0.915 - ETA: 0s - loss: 0.1907 - acc: 0.915 - ETA: 0s - loss: 0.1894 - acc: 0.917 - 1s 52us/step - loss: 0.1891 - acc: 0.9171 - val_loss: 0.9392 - val_acc: 0.8185\n",
      "Epoch 455/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1837 - acc: 0.906 - ETA: 0s - loss: 0.1798 - acc: 0.928 - ETA: 0s - loss: 0.1638 - acc: 0.932 - ETA: 0s - loss: 0.1761 - acc: 0.923 - ETA: 0s - loss: 0.1789 - acc: 0.922 - ETA: 0s - loss: 0.1811 - acc: 0.922 - ETA: 0s - loss: 0.1774 - acc: 0.923 - ETA: 0s - loss: 0.1795 - acc: 0.921 - ETA: 0s - loss: 0.1826 - acc: 0.920 - ETA: 0s - loss: 0.1818 - acc: 0.920 - ETA: 0s - loss: 0.1849 - acc: 0.919 - ETA: 0s - loss: 0.1862 - acc: 0.918 - ETA: 0s - loss: 0.1871 - acc: 0.919 - ETA: 0s - loss: 0.1862 - acc: 0.918 - ETA: 0s - loss: 0.1865 - acc: 0.918 - ETA: 0s - loss: 0.1872 - acc: 0.918 - ETA: 0s - loss: 0.1896 - acc: 0.918 - ETA: 0s - loss: 0.1887 - acc: 0.918 - 1s 50us/step - loss: 0.1902 - acc: 0.9182 - val_loss: 0.9539 - val_acc: 0.8276\n",
      "Epoch 456/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.2220 - acc: 0.906 - ETA: 0s - loss: 0.2401 - acc: 0.916 - ETA: 0s - loss: 0.2049 - acc: 0.921 - ETA: 0s - loss: 0.1882 - acc: 0.927 - ETA: 0s - loss: 0.1850 - acc: 0.924 - ETA: 0s - loss: 0.1865 - acc: 0.922 - ETA: 0s - loss: 0.1841 - acc: 0.922 - ETA: 0s - loss: 0.1812 - acc: 0.923 - ETA: 0s - loss: 0.1814 - acc: 0.922 - ETA: 0s - loss: 0.1845 - acc: 0.921 - ETA: 0s - loss: 0.1848 - acc: 0.922 - ETA: 0s - loss: 0.1850 - acc: 0.921 - ETA: 0s - loss: 0.1879 - acc: 0.920 - ETA: 0s - loss: 0.1879 - acc: 0.919 - ETA: 0s - loss: 0.1873 - acc: 0.919 - ETA: 0s - loss: 0.1899 - acc: 0.917 - ETA: 0s - loss: 0.1917 - acc: 0.916 - ETA: 0s - loss: 0.1904 - acc: 0.917 - 1s 51us/step - loss: 0.1895 - acc: 0.9172 - val_loss: 0.9575 - val_acc: 0.8227\n",
      "Epoch 457/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.1740 - acc: 0.937 - ETA: 0s - loss: 0.1842 - acc: 0.905 - ETA: 0s - loss: 0.1916 - acc: 0.911 - ETA: 0s - loss: 0.1834 - acc: 0.914 - ETA: 0s - loss: 0.1853 - acc: 0.916 - ETA: 0s - loss: 0.1880 - acc: 0.917 - ETA: 0s - loss: 0.1904 - acc: 0.915 - ETA: 0s - loss: 0.1866 - acc: 0.917 - ETA: 0s - loss: 0.1849 - acc: 0.918 - ETA: 0s - loss: 0.1874 - acc: 0.917 - ETA: 0s - loss: 0.1859 - acc: 0.918 - ETA: 0s - loss: 0.1850 - acc: 0.918 - ETA: 0s - loss: 0.1855 - acc: 0.917 - ETA: 0s - loss: 0.1862 - acc: 0.916 - ETA: 0s - loss: 0.1862 - acc: 0.916 - ETA: 0s - loss: 0.1871 - acc: 0.916 - ETA: 0s - loss: 0.1872 - acc: 0.915 - ETA: 0s - loss: 0.1875 - acc: 0.915 - 1s 50us/step - loss: 0.1898 - acc: 0.9153 - val_loss: 0.9795 - val_acc: 0.8167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 458/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1690 - acc: 0.890 - ETA: 0s - loss: 0.1483 - acc: 0.918 - ETA: 0s - loss: 0.1726 - acc: 0.919 - ETA: 0s - loss: 0.1715 - acc: 0.919 - ETA: 0s - loss: 0.1757 - acc: 0.919 - ETA: 0s - loss: 0.1799 - acc: 0.920 - ETA: 0s - loss: 0.1791 - acc: 0.918 - ETA: 0s - loss: 0.1824 - acc: 0.916 - ETA: 0s - loss: 0.1825 - acc: 0.916 - ETA: 0s - loss: 0.1816 - acc: 0.916 - ETA: 0s - loss: 0.1852 - acc: 0.916 - ETA: 0s - loss: 0.1875 - acc: 0.915 - ETA: 0s - loss: 0.1881 - acc: 0.914 - ETA: 0s - loss: 0.1885 - acc: 0.914 - ETA: 0s - loss: 0.1894 - acc: 0.915 - ETA: 0s - loss: 0.1920 - acc: 0.915 - ETA: 0s - loss: 0.1911 - acc: 0.916 - ETA: 0s - loss: 0.1924 - acc: 0.916 - 1s 49us/step - loss: 0.1921 - acc: 0.9162 - val_loss: 0.9023 - val_acc: 0.8221\n",
      "Epoch 459/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1641 - acc: 0.921 - ETA: 0s - loss: 0.1704 - acc: 0.938 - ETA: 0s - loss: 0.1744 - acc: 0.931 - ETA: 0s - loss: 0.1792 - acc: 0.927 - ETA: 0s - loss: 0.1788 - acc: 0.926 - ETA: 0s - loss: 0.1777 - acc: 0.926 - ETA: 0s - loss: 0.1782 - acc: 0.924 - ETA: 0s - loss: 0.1770 - acc: 0.924 - ETA: 0s - loss: 0.1805 - acc: 0.922 - ETA: 0s - loss: 0.1808 - acc: 0.921 - ETA: 0s - loss: 0.1824 - acc: 0.920 - ETA: 0s - loss: 0.1827 - acc: 0.920 - ETA: 0s - loss: 0.1839 - acc: 0.919 - ETA: 0s - loss: 0.1845 - acc: 0.919 - ETA: 0s - loss: 0.1855 - acc: 0.918 - ETA: 0s - loss: 0.1878 - acc: 0.917 - ETA: 0s - loss: 0.1867 - acc: 0.916 - ETA: 0s - loss: 0.1865 - acc: 0.917 - 1s 50us/step - loss: 0.1866 - acc: 0.9170 - val_loss: 0.8881 - val_acc: 0.8251\n",
      "Epoch 460/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1488 - acc: 0.937 - ETA: 0s - loss: 0.1713 - acc: 0.928 - ETA: 0s - loss: 0.1953 - acc: 0.917 - ETA: 0s - loss: 0.2043 - acc: 0.915 - ETA: 0s - loss: 0.1955 - acc: 0.917 - ETA: 0s - loss: 0.1990 - acc: 0.915 - ETA: 0s - loss: 0.1962 - acc: 0.916 - ETA: 0s - loss: 0.1928 - acc: 0.917 - ETA: 0s - loss: 0.1919 - acc: 0.918 - ETA: 0s - loss: 0.1904 - acc: 0.918 - ETA: 0s - loss: 0.1913 - acc: 0.918 - ETA: 0s - loss: 0.1923 - acc: 0.918 - ETA: 0s - loss: 0.1925 - acc: 0.918 - ETA: 0s - loss: 0.1904 - acc: 0.919 - ETA: 0s - loss: 0.1909 - acc: 0.919 - ETA: 0s - loss: 0.1921 - acc: 0.918 - ETA: 0s - loss: 0.1930 - acc: 0.918 - ETA: 0s - loss: 0.1933 - acc: 0.917 - 1s 50us/step - loss: 0.1926 - acc: 0.9176 - val_loss: 0.9665 - val_acc: 0.8119\n",
      "Epoch 461/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1720 - acc: 0.937 - ETA: 0s - loss: 0.1490 - acc: 0.930 - ETA: 0s - loss: 0.1648 - acc: 0.923 - ETA: 0s - loss: 0.1841 - acc: 0.921 - ETA: 0s - loss: 0.1818 - acc: 0.922 - ETA: 0s - loss: 0.1837 - acc: 0.922 - ETA: 0s - loss: 0.1838 - acc: 0.921 - ETA: 0s - loss: 0.1829 - acc: 0.921 - ETA: 0s - loss: 0.1869 - acc: 0.921 - ETA: 0s - loss: 0.1871 - acc: 0.920 - ETA: 0s - loss: 0.1886 - acc: 0.920 - ETA: 0s - loss: 0.1877 - acc: 0.919 - ETA: 0s - loss: 0.1860 - acc: 0.920 - ETA: 0s - loss: 0.1843 - acc: 0.920 - ETA: 0s - loss: 0.1842 - acc: 0.919 - ETA: 0s - loss: 0.1857 - acc: 0.918 - ETA: 0s - loss: 0.1850 - acc: 0.919 - ETA: 0s - loss: 0.1877 - acc: 0.919 - 1s 50us/step - loss: 0.1879 - acc: 0.9188 - val_loss: 0.9399 - val_acc: 0.8268\n",
      "Epoch 462/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.2021 - acc: 0.906 - ETA: 0s - loss: 0.1722 - acc: 0.923 - ETA: 0s - loss: 0.1768 - acc: 0.920 - ETA: 0s - loss: 0.1818 - acc: 0.914 - ETA: 0s - loss: 0.1788 - acc: 0.917 - ETA: 0s - loss: 0.1801 - acc: 0.914 - ETA: 0s - loss: 0.1779 - acc: 0.917 - ETA: 0s - loss: 0.1810 - acc: 0.917 - ETA: 0s - loss: 0.1820 - acc: 0.917 - ETA: 0s - loss: 0.1848 - acc: 0.916 - ETA: 0s - loss: 0.1867 - acc: 0.916 - ETA: 0s - loss: 0.1875 - acc: 0.916 - ETA: 0s - loss: 0.1887 - acc: 0.917 - ETA: 0s - loss: 0.1893 - acc: 0.917 - ETA: 0s - loss: 0.1893 - acc: 0.917 - ETA: 0s - loss: 0.1873 - acc: 0.918 - ETA: 0s - loss: 0.1859 - acc: 0.919 - ETA: 0s - loss: 0.1869 - acc: 0.918 - 1s 51us/step - loss: 0.1875 - acc: 0.9184 - val_loss: 0.9294 - val_acc: 0.8178\n",
      "Epoch 463/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.1375 - acc: 0.953 - ETA: 0s - loss: 0.1750 - acc: 0.922 - ETA: 0s - loss: 0.1809 - acc: 0.915 - ETA: 0s - loss: 0.2076 - acc: 0.915 - ETA: 0s - loss: 0.2075 - acc: 0.912 - ETA: 0s - loss: 0.1975 - acc: 0.915 - ETA: 0s - loss: 0.1950 - acc: 0.915 - ETA: 0s - loss: 0.1965 - acc: 0.916 - ETA: 0s - loss: 0.1955 - acc: 0.917 - ETA: 0s - loss: 0.1941 - acc: 0.917 - ETA: 0s - loss: 0.1920 - acc: 0.917 - ETA: 0s - loss: 0.1964 - acc: 0.915 - ETA: 0s - loss: 0.1934 - acc: 0.916 - ETA: 0s - loss: 0.1953 - acc: 0.916 - ETA: 0s - loss: 0.1967 - acc: 0.915 - ETA: 0s - loss: 0.1961 - acc: 0.915 - ETA: 0s - loss: 0.1942 - acc: 0.915 - ETA: 0s - loss: 0.1938 - acc: 0.916 - 1s 51us/step - loss: 0.1936 - acc: 0.9161 - val_loss: 0.9408 - val_acc: 0.8310\n",
      "Epoch 464/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.4080 - acc: 0.843 - ETA: 0s - loss: 0.1923 - acc: 0.909 - ETA: 0s - loss: 0.1856 - acc: 0.916 - ETA: 0s - loss: 0.1788 - acc: 0.921 - ETA: 0s - loss: 0.1814 - acc: 0.919 - ETA: 0s - loss: 0.1848 - acc: 0.919 - ETA: 0s - loss: 0.1878 - acc: 0.919 - ETA: 0s - loss: 0.1870 - acc: 0.919 - ETA: 0s - loss: 0.1881 - acc: 0.918 - ETA: 0s - loss: 0.1877 - acc: 0.919 - ETA: 0s - loss: 0.1890 - acc: 0.918 - ETA: 0s - loss: 0.1912 - acc: 0.917 - ETA: 0s - loss: 0.1918 - acc: 0.917 - ETA: 0s - loss: 0.1903 - acc: 0.918 - ETA: 0s - loss: 0.1911 - acc: 0.918 - ETA: 0s - loss: 0.1907 - acc: 0.917 - ETA: 0s - loss: 0.1910 - acc: 0.917 - ETA: 0s - loss: 0.1919 - acc: 0.916 - ETA: 0s - loss: 0.1919 - acc: 0.916 - 1s 52us/step - loss: 0.1916 - acc: 0.9169 - val_loss: 0.9852 - val_acc: 0.8177\n",
      "Epoch 465/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.1877 - acc: 0.906 - ETA: 0s - loss: 0.1862 - acc: 0.913 - ETA: 0s - loss: 0.1803 - acc: 0.915 - ETA: 0s - loss: 0.1841 - acc: 0.918 - ETA: 0s - loss: 0.1835 - acc: 0.919 - ETA: 0s - loss: 0.1860 - acc: 0.920 - ETA: 0s - loss: 0.1872 - acc: 0.917 - ETA: 0s - loss: 0.1913 - acc: 0.916 - ETA: 0s - loss: 0.1895 - acc: 0.917 - ETA: 0s - loss: 0.1901 - acc: 0.918 - ETA: 0s - loss: 0.1936 - acc: 0.917 - ETA: 0s - loss: 0.1894 - acc: 0.918 - ETA: 0s - loss: 0.1894 - acc: 0.918 - ETA: 0s - loss: 0.1916 - acc: 0.918 - ETA: 0s - loss: 0.1916 - acc: 0.918 - ETA: 0s - loss: 0.1921 - acc: 0.918 - ETA: 0s - loss: 0.1922 - acc: 0.917 - ETA: 0s - loss: 0.1920 - acc: 0.918 - 1s 51us/step - loss: 0.1923 - acc: 0.9177 - val_loss: 0.9586 - val_acc: 0.8243\n",
      "Epoch 466/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1336 - acc: 0.921 - ETA: 0s - loss: 0.2082 - acc: 0.916 - ETA: 0s - loss: 0.2199 - acc: 0.917 - ETA: 0s - loss: 0.2104 - acc: 0.917 - ETA: 0s - loss: 0.1978 - acc: 0.919 - ETA: 0s - loss: 0.1931 - acc: 0.918 - ETA: 0s - loss: 0.1916 - acc: 0.918 - ETA: 0s - loss: 0.1865 - acc: 0.919 - ETA: 0s - loss: 0.1879 - acc: 0.920 - ETA: 0s - loss: 0.1882 - acc: 0.919 - ETA: 0s - loss: 0.1896 - acc: 0.919 - ETA: 0s - loss: 0.1898 - acc: 0.919 - ETA: 0s - loss: 0.1878 - acc: 0.919 - ETA: 0s - loss: 0.1901 - acc: 0.918 - ETA: 0s - loss: 0.1907 - acc: 0.917 - ETA: 0s - loss: 0.1917 - acc: 0.917 - ETA: 0s - loss: 0.1901 - acc: 0.917 - ETA: 0s - loss: 0.1904 - acc: 0.917 - 1s 51us/step - loss: 0.1891 - acc: 0.9177 - val_loss: 0.9421 - val_acc: 0.8204\n",
      "Epoch 467/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.2553 - acc: 0.875 - ETA: 0s - loss: 0.2034 - acc: 0.915 - ETA: 0s - loss: 0.1934 - acc: 0.920 - ETA: 0s - loss: 0.1954 - acc: 0.918 - ETA: 0s - loss: 0.2016 - acc: 0.915 - ETA: 0s - loss: 0.2038 - acc: 0.917 - ETA: 0s - loss: 0.1998 - acc: 0.918 - ETA: 0s - loss: 0.1944 - acc: 0.919 - ETA: 0s - loss: 0.1926 - acc: 0.918 - ETA: 0s - loss: 0.1904 - acc: 0.919 - ETA: 0s - loss: 0.1857 - acc: 0.920 - ETA: 0s - loss: 0.1870 - acc: 0.920 - ETA: 0s - loss: 0.1876 - acc: 0.919 - ETA: 0s - loss: 0.1905 - acc: 0.919 - ETA: 0s - loss: 0.1901 - acc: 0.918 - ETA: 0s - loss: 0.1902 - acc: 0.918 - ETA: 0s - loss: 0.1904 - acc: 0.918 - ETA: 0s - loss: 0.1922 - acc: 0.917 - 1s 51us/step - loss: 0.1935 - acc: 0.9163 - val_loss: 0.9387 - val_acc: 0.8180\n",
      "Epoch 468/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1841 - acc: 0.921 - ETA: 0s - loss: 0.1644 - acc: 0.928 - ETA: 0s - loss: 0.1757 - acc: 0.925 - ETA: 0s - loss: 0.1756 - acc: 0.924 - ETA: 0s - loss: 0.1792 - acc: 0.922 - ETA: 0s - loss: 0.1796 - acc: 0.922 - ETA: 0s - loss: 0.1788 - acc: 0.923 - ETA: 0s - loss: 0.1764 - acc: 0.923 - ETA: 0s - loss: 0.1802 - acc: 0.921 - ETA: 0s - loss: 0.1824 - acc: 0.920 - ETA: 0s - loss: 0.1838 - acc: 0.919 - ETA: 0s - loss: 0.1835 - acc: 0.920 - ETA: 0s - loss: 0.1841 - acc: 0.919 - ETA: 0s - loss: 0.1853 - acc: 0.919 - ETA: 0s - loss: 0.1865 - acc: 0.918 - ETA: 0s - loss: 0.1884 - acc: 0.917 - ETA: 0s - loss: 0.1881 - acc: 0.918 - ETA: 0s - loss: 0.1902 - acc: 0.917 - 1s 50us/step - loss: 0.1908 - acc: 0.9172 - val_loss: 0.9704 - val_acc: 0.8180\n",
      "Epoch 469/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1850 - acc: 0.906 - ETA: 0s - loss: 0.2088 - acc: 0.911 - ETA: 0s - loss: 0.1945 - acc: 0.913 - ETA: 0s - loss: 0.2008 - acc: 0.915 - ETA: 0s - loss: 0.2004 - acc: 0.915 - ETA: 0s - loss: 0.2096 - acc: 0.911 - ETA: 0s - loss: 0.2075 - acc: 0.913 - ETA: 0s - loss: 0.2052 - acc: 0.914 - ETA: 0s - loss: 0.2049 - acc: 0.913 - ETA: 0s - loss: 0.2024 - acc: 0.914 - ETA: 0s - loss: 0.1993 - acc: 0.915 - ETA: 0s - loss: 0.1973 - acc: 0.915 - ETA: 0s - loss: 0.1977 - acc: 0.915 - ETA: 0s - loss: 0.1998 - acc: 0.914 - ETA: 0s - loss: 0.1989 - acc: 0.914 - ETA: 0s - loss: 0.1967 - acc: 0.914 - ETA: 0s - loss: 0.1946 - acc: 0.916 - ETA: 0s - loss: 0.1938 - acc: 0.916 - ETA: 0s - loss: 0.1958 - acc: 0.916 - 1s 52us/step - loss: 0.1953 - acc: 0.9164 - val_loss: 0.9380 - val_acc: 0.8246\n",
      "Epoch 470/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.0906 - acc: 0.968 - ETA: 0s - loss: 0.1671 - acc: 0.921 - ETA: 0s - loss: 0.1837 - acc: 0.916 - ETA: 0s - loss: 0.1867 - acc: 0.912 - ETA: 0s - loss: 0.1886 - acc: 0.913 - ETA: 0s - loss: 0.1876 - acc: 0.914 - ETA: 0s - loss: 0.1894 - acc: 0.914 - ETA: 0s - loss: 0.1908 - acc: 0.914 - ETA: 0s - loss: 0.1910 - acc: 0.914 - ETA: 0s - loss: 0.1890 - acc: 0.915 - ETA: 0s - loss: 0.1884 - acc: 0.915 - ETA: 0s - loss: 0.1880 - acc: 0.915 - ETA: 0s - loss: 0.1884 - acc: 0.915 - ETA: 0s - loss: 0.1877 - acc: 0.916 - ETA: 0s - loss: 0.1908 - acc: 0.915 - ETA: 0s - loss: 0.1913 - acc: 0.915 - ETA: 0s - loss: 0.1904 - acc: 0.915 - ETA: 0s - loss: 0.1887 - acc: 0.916 - ETA: 0s - loss: 0.1890 - acc: 0.917 - 1s 52us/step - loss: 0.1889 - acc: 0.9175 - val_loss: 0.9181 - val_acc: 0.8281\n",
      "Epoch 471/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.2499 - acc: 0.859 - ETA: 0s - loss: 0.1655 - acc: 0.926 - ETA: 0s - loss: 0.1696 - acc: 0.926 - ETA: 0s - loss: 0.1806 - acc: 0.918 - ETA: 0s - loss: 0.1872 - acc: 0.919 - ETA: 0s - loss: 0.1846 - acc: 0.919 - ETA: 0s - loss: 0.1842 - acc: 0.920 - ETA: 0s - loss: 0.1879 - acc: 0.919 - ETA: 0s - loss: 0.1857 - acc: 0.920 - ETA: 0s - loss: 0.1837 - acc: 0.921 - ETA: 0s - loss: 0.1820 - acc: 0.922 - ETA: 0s - loss: 0.1833 - acc: 0.921 - ETA: 0s - loss: 0.1830 - acc: 0.921 - ETA: 0s - loss: 0.1839 - acc: 0.921 - ETA: 0s - loss: 0.1830 - acc: 0.921 - ETA: 0s - loss: 0.1853 - acc: 0.921 - ETA: 0s - loss: 0.1853 - acc: 0.920 - ETA: 0s - loss: 0.1868 - acc: 0.920 - 1s 50us/step - loss: 0.1902 - acc: 0.9188 - val_loss: 0.8920 - val_acc: 0.8186\n",
      "Epoch 472/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.1629 - acc: 0.921 - ETA: 0s - loss: 0.1844 - acc: 0.927 - ETA: 0s - loss: 0.1738 - acc: 0.928 - ETA: 0s - loss: 0.1821 - acc: 0.926 - ETA: 0s - loss: 0.1809 - acc: 0.922 - ETA: 0s - loss: 0.1852 - acc: 0.921 - ETA: 0s - loss: 0.1879 - acc: 0.920 - ETA: 0s - loss: 0.1917 - acc: 0.918 - ETA: 0s - loss: 0.1938 - acc: 0.916 - ETA: 0s - loss: 0.1930 - acc: 0.917 - ETA: 0s - loss: 0.1923 - acc: 0.917 - ETA: 0s - loss: 0.1924 - acc: 0.918 - ETA: 0s - loss: 0.1928 - acc: 0.917 - ETA: 0s - loss: 0.1922 - acc: 0.917 - ETA: 0s - loss: 0.1935 - acc: 0.917 - ETA: 0s - loss: 0.1972 - acc: 0.916 - ETA: 0s - loss: 0.1974 - acc: 0.916 - 1s 48us/step - loss: 0.1967 - acc: 0.9159 - val_loss: 0.9624 - val_acc: 0.8221\n",
      "Epoch 473/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.2747 - acc: 0.875 - ETA: 0s - loss: 0.1832 - acc: 0.917 - ETA: 0s - loss: 0.1745 - acc: 0.917 - ETA: 0s - loss: 0.1751 - acc: 0.921 - ETA: 0s - loss: 0.1794 - acc: 0.919 - ETA: 0s - loss: 0.1774 - acc: 0.919 - ETA: 0s - loss: 0.1782 - acc: 0.919 - ETA: 0s - loss: 0.1819 - acc: 0.919 - ETA: 0s - loss: 0.1863 - acc: 0.919 - ETA: 0s - loss: 0.1874 - acc: 0.918 - ETA: 0s - loss: 0.1873 - acc: 0.918 - ETA: 0s - loss: 0.1864 - acc: 0.918 - ETA: 0s - loss: 0.1882 - acc: 0.918 - ETA: 0s - loss: 0.1876 - acc: 0.919 - ETA: 0s - loss: 0.1893 - acc: 0.918 - ETA: 0s - loss: 0.1912 - acc: 0.917 - ETA: 0s - loss: 0.1889 - acc: 0.918 - ETA: 0s - loss: 0.1892 - acc: 0.917 - 1s 49us/step - loss: 0.1896 - acc: 0.9175 - val_loss: 0.9122 - val_acc: 0.8243\n",
      "Epoch 474/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.2181 - acc: 0.875 - ETA: 0s - loss: 0.1743 - acc: 0.933 - ETA: 0s - loss: 0.1805 - acc: 0.922 - ETA: 0s - loss: 0.1847 - acc: 0.921 - ETA: 0s - loss: 0.1825 - acc: 0.921 - ETA: 0s - loss: 0.1885 - acc: 0.921 - ETA: 0s - loss: 0.1909 - acc: 0.920 - ETA: 0s - loss: 0.1889 - acc: 0.920 - ETA: 0s - loss: 0.1848 - acc: 0.921 - ETA: 0s - loss: 0.1864 - acc: 0.921 - ETA: 0s - loss: 0.1855 - acc: 0.920 - ETA: 0s - loss: 0.1867 - acc: 0.919 - ETA: 0s - loss: 0.1879 - acc: 0.918 - ETA: 0s - loss: 0.1862 - acc: 0.918 - ETA: 0s - loss: 0.1853 - acc: 0.918 - ETA: 0s - loss: 0.1862 - acc: 0.918 - ETA: 0s - loss: 0.1864 - acc: 0.918 - ETA: 0s - loss: 0.1871 - acc: 0.917 - 1s 49us/step - loss: 0.1872 - acc: 0.9172 - val_loss: 1.0987 - val_acc: 0.8006\n",
      "Epoch 475/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.4852 - acc: 0.828 - ETA: 0s - loss: 0.1816 - acc: 0.932 - ETA: 0s - loss: 0.1770 - acc: 0.926 - ETA: 0s - loss: 0.1752 - acc: 0.926 - ETA: 0s - loss: 0.1872 - acc: 0.924 - ETA: 0s - loss: 0.1875 - acc: 0.924 - ETA: 0s - loss: 0.1904 - acc: 0.922 - ETA: 0s - loss: 0.1917 - acc: 0.921 - ETA: 0s - loss: 0.1901 - acc: 0.922 - ETA: 0s - loss: 0.1896 - acc: 0.922 - ETA: 0s - loss: 0.1900 - acc: 0.921 - ETA: 0s - loss: 0.1879 - acc: 0.921 - ETA: 0s - loss: 0.1879 - acc: 0.921 - ETA: 0s - loss: 0.1910 - acc: 0.919 - ETA: 0s - loss: 0.1902 - acc: 0.919 - ETA: 0s - loss: 0.1897 - acc: 0.919 - ETA: 0s - loss: 0.1894 - acc: 0.919 - ETA: 0s - loss: 0.1923 - acc: 0.918 - 1s 50us/step - loss: 0.1922 - acc: 0.9185 - val_loss: 0.9770 - val_acc: 0.8170\n",
      "Epoch 476/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.2375 - acc: 0.859 - ETA: 0s - loss: 0.1639 - acc: 0.926 - ETA: 0s - loss: 0.1707 - acc: 0.923 - ETA: 0s - loss: 0.1842 - acc: 0.921 - ETA: 0s - loss: 0.1859 - acc: 0.919 - ETA: 0s - loss: 0.1864 - acc: 0.916 - ETA: 0s - loss: 0.1854 - acc: 0.916 - ETA: 0s - loss: 0.1870 - acc: 0.915 - ETA: 0s - loss: 0.1856 - acc: 0.916 - ETA: 0s - loss: 0.1875 - acc: 0.916 - ETA: 0s - loss: 0.1872 - acc: 0.916 - ETA: 0s - loss: 0.1864 - acc: 0.917 - ETA: 0s - loss: 0.1884 - acc: 0.916 - ETA: 0s - loss: 0.1894 - acc: 0.915 - ETA: 0s - loss: 0.1906 - acc: 0.915 - ETA: 0s - loss: 0.1915 - acc: 0.915 - ETA: 0s - loss: 0.1929 - acc: 0.915 - ETA: 0s - loss: 0.1923 - acc: 0.916 - 1s 50us/step - loss: 0.1929 - acc: 0.9157 - val_loss: 0.9300 - val_acc: 0.8225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 477/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.1773 - acc: 0.890 - ETA: 0s - loss: 0.1659 - acc: 0.913 - ETA: 0s - loss: 0.1706 - acc: 0.912 - ETA: 0s - loss: 0.1681 - acc: 0.919 - ETA: 0s - loss: 0.1696 - acc: 0.919 - ETA: 0s - loss: 0.1745 - acc: 0.918 - ETA: 0s - loss: 0.1723 - acc: 0.920 - ETA: 0s - loss: 0.1726 - acc: 0.921 - ETA: 0s - loss: 0.1751 - acc: 0.921 - ETA: 0s - loss: 0.1759 - acc: 0.920 - ETA: 0s - loss: 0.1807 - acc: 0.919 - ETA: 0s - loss: 0.1811 - acc: 0.919 - ETA: 0s - loss: 0.1808 - acc: 0.919 - ETA: 0s - loss: 0.1862 - acc: 0.918 - ETA: 0s - loss: 0.1862 - acc: 0.918 - ETA: 0s - loss: 0.1875 - acc: 0.918 - ETA: 0s - loss: 0.1899 - acc: 0.917 - ETA: 0s - loss: 0.1903 - acc: 0.917 - 1s 50us/step - loss: 0.1891 - acc: 0.9178 - val_loss: 0.9205 - val_acc: 0.8270\n",
      "Epoch 478/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.1522 - acc: 0.875 - ETA: 0s - loss: 0.1933 - acc: 0.921 - ETA: 0s - loss: 0.1935 - acc: 0.920 - ETA: 0s - loss: 0.1880 - acc: 0.919 - ETA: 0s - loss: 0.1855 - acc: 0.919 - ETA: 0s - loss: 0.1832 - acc: 0.920 - ETA: 0s - loss: 0.1900 - acc: 0.920 - ETA: 0s - loss: 0.1905 - acc: 0.920 - ETA: 0s - loss: 0.1908 - acc: 0.920 - ETA: 0s - loss: 0.1889 - acc: 0.919 - ETA: 0s - loss: 0.1897 - acc: 0.918 - ETA: 0s - loss: 0.1919 - acc: 0.917 - ETA: 0s - loss: 0.1912 - acc: 0.918 - ETA: 0s - loss: 0.1943 - acc: 0.917 - ETA: 0s - loss: 0.1934 - acc: 0.918 - ETA: 0s - loss: 0.1935 - acc: 0.918 - ETA: 0s - loss: 0.1940 - acc: 0.917 - ETA: 0s - loss: 0.1940 - acc: 0.918 - 1s 50us/step - loss: 0.1935 - acc: 0.9179 - val_loss: 1.0769 - val_acc: 0.8159\n",
      "Epoch 479/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.2802 - acc: 0.875 - ETA: 0s - loss: 0.2033 - acc: 0.910 - ETA: 0s - loss: 0.1878 - acc: 0.911 - ETA: 0s - loss: 0.1881 - acc: 0.915 - ETA: 0s - loss: 0.1891 - acc: 0.916 - ETA: 0s - loss: 0.1861 - acc: 0.918 - ETA: 0s - loss: 0.1861 - acc: 0.918 - ETA: 0s - loss: 0.1876 - acc: 0.917 - ETA: 0s - loss: 0.1925 - acc: 0.914 - ETA: 0s - loss: 0.1905 - acc: 0.916 - ETA: 0s - loss: 0.1928 - acc: 0.915 - ETA: 0s - loss: 0.1913 - acc: 0.916 - ETA: 0s - loss: 0.1921 - acc: 0.916 - ETA: 0s - loss: 0.1935 - acc: 0.915 - ETA: 0s - loss: 0.1925 - acc: 0.915 - ETA: 0s - loss: 0.1927 - acc: 0.916 - ETA: 0s - loss: 0.1926 - acc: 0.916 - ETA: 0s - loss: 0.1938 - acc: 0.916 - 1s 50us/step - loss: 0.1957 - acc: 0.9156 - val_loss: 0.9868 - val_acc: 0.8153\n",
      "Epoch 480/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1919 - acc: 0.906 - ETA: 0s - loss: 0.1799 - acc: 0.922 - ETA: 0s - loss: 0.1854 - acc: 0.918 - ETA: 0s - loss: 0.1851 - acc: 0.919 - ETA: 0s - loss: 0.1854 - acc: 0.919 - ETA: 0s - loss: 0.1844 - acc: 0.920 - ETA: 0s - loss: 0.1857 - acc: 0.919 - ETA: 0s - loss: 0.1876 - acc: 0.918 - ETA: 0s - loss: 0.1895 - acc: 0.918 - ETA: 0s - loss: 0.1874 - acc: 0.919 - ETA: 0s - loss: 0.1856 - acc: 0.920 - ETA: 0s - loss: 0.1894 - acc: 0.917 - ETA: 0s - loss: 0.1907 - acc: 0.917 - ETA: 0s - loss: 0.1908 - acc: 0.917 - ETA: 0s - loss: 0.1907 - acc: 0.917 - ETA: 0s - loss: 0.1922 - acc: 0.917 - ETA: 0s - loss: 0.1907 - acc: 0.917 - ETA: 0s - loss: 0.1902 - acc: 0.917 - 1s 50us/step - loss: 0.1899 - acc: 0.9169 - val_loss: 0.9456 - val_acc: 0.8264\n",
      "Epoch 481/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.1950 - acc: 0.906 - ETA: 0s - loss: 0.1974 - acc: 0.916 - ETA: 0s - loss: 0.1849 - acc: 0.916 - ETA: 0s - loss: 0.1808 - acc: 0.920 - ETA: 0s - loss: 0.1821 - acc: 0.921 - ETA: 0s - loss: 0.1821 - acc: 0.921 - ETA: 0s - loss: 0.1803 - acc: 0.922 - ETA: 0s - loss: 0.1884 - acc: 0.918 - ETA: 0s - loss: 0.1910 - acc: 0.918 - ETA: 0s - loss: 0.1948 - acc: 0.918 - ETA: 0s - loss: 0.1912 - acc: 0.920 - ETA: 0s - loss: 0.1895 - acc: 0.920 - ETA: 0s - loss: 0.1894 - acc: 0.920 - ETA: 0s - loss: 0.1907 - acc: 0.920 - ETA: 0s - loss: 0.1923 - acc: 0.920 - ETA: 0s - loss: 0.1917 - acc: 0.920 - ETA: 0s - loss: 0.1910 - acc: 0.920 - ETA: 0s - loss: 0.1925 - acc: 0.920 - 1s 49us/step - loss: 0.1925 - acc: 0.9197 - val_loss: 0.9664 - val_acc: 0.8227\n",
      "Epoch 482/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1883 - acc: 0.906 - ETA: 0s - loss: 0.1596 - acc: 0.917 - ETA: 0s - loss: 0.1759 - acc: 0.923 - ETA: 0s - loss: 0.1823 - acc: 0.922 - ETA: 0s - loss: 0.1845 - acc: 0.922 - ETA: 0s - loss: 0.1863 - acc: 0.922 - ETA: 0s - loss: 0.1833 - acc: 0.923 - ETA: 0s - loss: 0.1819 - acc: 0.922 - ETA: 0s - loss: 0.1840 - acc: 0.920 - ETA: 0s - loss: 0.1822 - acc: 0.920 - ETA: 0s - loss: 0.1874 - acc: 0.919 - ETA: 0s - loss: 0.1863 - acc: 0.919 - ETA: 0s - loss: 0.1878 - acc: 0.919 - ETA: 0s - loss: 0.1879 - acc: 0.918 - ETA: 0s - loss: 0.1877 - acc: 0.918 - ETA: 0s - loss: 0.1902 - acc: 0.917 - ETA: 0s - loss: 0.1890 - acc: 0.917 - ETA: 0s - loss: 0.1903 - acc: 0.917 - 1s 50us/step - loss: 0.1911 - acc: 0.9175 - val_loss: 0.8919 - val_acc: 0.8234\n",
      "Epoch 483/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.1339 - acc: 0.890 - ETA: 0s - loss: 0.2080 - acc: 0.909 - ETA: 0s - loss: 0.1876 - acc: 0.919 - ETA: 0s - loss: 0.1851 - acc: 0.917 - ETA: 0s - loss: 0.1917 - acc: 0.918 - ETA: 0s - loss: 0.1872 - acc: 0.919 - ETA: 0s - loss: 0.1835 - acc: 0.919 - ETA: 0s - loss: 0.1867 - acc: 0.917 - ETA: 0s - loss: 0.1851 - acc: 0.918 - ETA: 0s - loss: 0.1859 - acc: 0.917 - ETA: 0s - loss: 0.1871 - acc: 0.916 - ETA: 0s - loss: 0.1897 - acc: 0.916 - ETA: 0s - loss: 0.1923 - acc: 0.915 - ETA: 0s - loss: 0.1945 - acc: 0.914 - ETA: 0s - loss: 0.1940 - acc: 0.915 - ETA: 0s - loss: 0.1939 - acc: 0.915 - ETA: 0s - loss: 0.1952 - acc: 0.915 - ETA: 0s - loss: 0.1953 - acc: 0.915 - 1s 50us/step - loss: 0.1961 - acc: 0.9152 - val_loss: 0.9646 - val_acc: 0.8169\n",
      "Epoch 484/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.1420 - acc: 0.921 - ETA: 0s - loss: 0.1893 - acc: 0.912 - ETA: 0s - loss: 0.1899 - acc: 0.909 - ETA: 0s - loss: 0.2049 - acc: 0.911 - ETA: 0s - loss: 0.2009 - acc: 0.911 - ETA: 0s - loss: 0.1948 - acc: 0.915 - ETA: 0s - loss: 0.2023 - acc: 0.913 - ETA: 0s - loss: 0.1950 - acc: 0.915 - ETA: 0s - loss: 0.1920 - acc: 0.916 - ETA: 0s - loss: 0.1892 - acc: 0.917 - ETA: 0s - loss: 0.1918 - acc: 0.916 - ETA: 0s - loss: 0.1910 - acc: 0.916 - ETA: 0s - loss: 0.1911 - acc: 0.917 - ETA: 0s - loss: 0.1918 - acc: 0.916 - ETA: 0s - loss: 0.1930 - acc: 0.916 - ETA: 0s - loss: 0.1926 - acc: 0.916 - ETA: 0s - loss: 0.1943 - acc: 0.916 - ETA: 0s - loss: 0.1935 - acc: 0.916 - 1s 51us/step - loss: 0.1944 - acc: 0.9165 - val_loss: 0.9161 - val_acc: 0.8229\n",
      "Epoch 485/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.2263 - acc: 0.906 - ETA: 0s - loss: 0.2109 - acc: 0.919 - ETA: 0s - loss: 0.1894 - acc: 0.918 - ETA: 0s - loss: 0.1894 - acc: 0.916 - ETA: 0s - loss: 0.1866 - acc: 0.920 - ETA: 0s - loss: 0.1829 - acc: 0.922 - ETA: 0s - loss: 0.1844 - acc: 0.919 - ETA: 0s - loss: 0.1826 - acc: 0.920 - ETA: 0s - loss: 0.1816 - acc: 0.919 - ETA: 0s - loss: 0.1850 - acc: 0.918 - ETA: 0s - loss: 0.1860 - acc: 0.917 - ETA: 0s - loss: 0.1874 - acc: 0.916 - ETA: 0s - loss: 0.1875 - acc: 0.916 - ETA: 0s - loss: 0.1875 - acc: 0.916 - ETA: 0s - loss: 0.1875 - acc: 0.916 - ETA: 0s - loss: 0.1903 - acc: 0.916 - ETA: 0s - loss: 0.1893 - acc: 0.916 - ETA: 0s - loss: 0.1893 - acc: 0.917 - 1s 50us/step - loss: 0.1897 - acc: 0.9175 - val_loss: 0.9755 - val_acc: 0.8132\n",
      "Epoch 486/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.1310 - acc: 0.921 - ETA: 0s - loss: 0.1615 - acc: 0.935 - ETA: 0s - loss: 0.1733 - acc: 0.926 - ETA: 0s - loss: 0.1775 - acc: 0.924 - ETA: 0s - loss: 0.1774 - acc: 0.923 - ETA: 0s - loss: 0.1848 - acc: 0.920 - ETA: 0s - loss: 0.1832 - acc: 0.921 - ETA: 0s - loss: 0.1879 - acc: 0.921 - ETA: 0s - loss: 0.1872 - acc: 0.920 - ETA: 0s - loss: 0.1879 - acc: 0.919 - ETA: 0s - loss: 0.1889 - acc: 0.918 - ETA: 0s - loss: 0.1914 - acc: 0.917 - ETA: 0s - loss: 0.1922 - acc: 0.918 - ETA: 0s - loss: 0.1911 - acc: 0.917 - ETA: 0s - loss: 0.1912 - acc: 0.917 - ETA: 0s - loss: 0.1911 - acc: 0.916 - ETA: 0s - loss: 0.1925 - acc: 0.916 - ETA: 0s - loss: 0.1921 - acc: 0.916 - 1s 51us/step - loss: 0.1926 - acc: 0.9163 - val_loss: 0.9633 - val_acc: 0.8105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 487/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.4061 - acc: 0.937 - ETA: 0s - loss: 0.1942 - acc: 0.918 - ETA: 0s - loss: 0.1794 - acc: 0.927 - ETA: 0s - loss: 0.1847 - acc: 0.924 - ETA: 0s - loss: 0.1835 - acc: 0.922 - ETA: 0s - loss: 0.1868 - acc: 0.920 - ETA: 0s - loss: 0.1903 - acc: 0.917 - ETA: 0s - loss: 0.1863 - acc: 0.918 - ETA: 0s - loss: 0.1863 - acc: 0.920 - ETA: 0s - loss: 0.1841 - acc: 0.919 - ETA: 0s - loss: 0.1863 - acc: 0.918 - ETA: 0s - loss: 0.1859 - acc: 0.918 - ETA: 0s - loss: 0.1834 - acc: 0.919 - ETA: 0s - loss: 0.1840 - acc: 0.919 - ETA: 0s - loss: 0.1872 - acc: 0.917 - ETA: 0s - loss: 0.1870 - acc: 0.917 - ETA: 0s - loss: 0.1884 - acc: 0.916 - ETA: 0s - loss: 0.1890 - acc: 0.917 - 1s 50us/step - loss: 0.1885 - acc: 0.9174 - val_loss: 0.9510 - val_acc: 0.8190\n",
      "Epoch 488/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.1541 - acc: 0.921 - ETA: 0s - loss: 0.1786 - acc: 0.924 - ETA: 0s - loss: 0.1674 - acc: 0.927 - ETA: 0s - loss: 0.1618 - acc: 0.927 - ETA: 0s - loss: 0.1654 - acc: 0.927 - ETA: 0s - loss: 0.1758 - acc: 0.924 - ETA: 0s - loss: 0.1809 - acc: 0.922 - ETA: 0s - loss: 0.1821 - acc: 0.920 - ETA: 0s - loss: 0.1837 - acc: 0.919 - ETA: 0s - loss: 0.1876 - acc: 0.919 - ETA: 0s - loss: 0.1873 - acc: 0.919 - ETA: 0s - loss: 0.1878 - acc: 0.918 - ETA: 0s - loss: 0.1900 - acc: 0.917 - ETA: 0s - loss: 0.1908 - acc: 0.917 - ETA: 0s - loss: 0.1915 - acc: 0.917 - ETA: 0s - loss: 0.1959 - acc: 0.916 - ETA: 0s - loss: 0.1954 - acc: 0.916 - ETA: 0s - loss: 0.1952 - acc: 0.916 - 1s 49us/step - loss: 0.1951 - acc: 0.9165 - val_loss: 0.8993 - val_acc: 0.8161\n",
      "Epoch 489/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1851 - acc: 0.921 - ETA: 0s - loss: 0.1796 - acc: 0.914 - ETA: 0s - loss: 0.1853 - acc: 0.917 - ETA: 0s - loss: 0.1821 - acc: 0.918 - ETA: 0s - loss: 0.1898 - acc: 0.916 - ETA: 0s - loss: 0.1852 - acc: 0.919 - ETA: 0s - loss: 0.1843 - acc: 0.919 - ETA: 0s - loss: 0.1847 - acc: 0.919 - ETA: 0s - loss: 0.1849 - acc: 0.919 - ETA: 0s - loss: 0.1895 - acc: 0.917 - ETA: 0s - loss: 0.1903 - acc: 0.917 - ETA: 0s - loss: 0.1902 - acc: 0.918 - ETA: 0s - loss: 0.1904 - acc: 0.918 - ETA: 0s - loss: 0.1935 - acc: 0.918 - ETA: 0s - loss: 0.1942 - acc: 0.918 - ETA: 0s - loss: 0.1940 - acc: 0.917 - ETA: 0s - loss: 0.1927 - acc: 0.918 - ETA: 0s - loss: 0.1916 - acc: 0.918 - 1s 49us/step - loss: 0.1920 - acc: 0.9180 - val_loss: 0.9341 - val_acc: 0.8207\n",
      "Epoch 490/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.2154 - acc: 0.875 - ETA: 0s - loss: 0.2026 - acc: 0.904 - ETA: 0s - loss: 0.1999 - acc: 0.913 - ETA: 0s - loss: 0.1957 - acc: 0.913 - ETA: 0s - loss: 0.2001 - acc: 0.914 - ETA: 0s - loss: 0.1992 - acc: 0.915 - ETA: 0s - loss: 0.1929 - acc: 0.916 - ETA: 0s - loss: 0.1912 - acc: 0.917 - ETA: 0s - loss: 0.1882 - acc: 0.918 - ETA: 0s - loss: 0.1911 - acc: 0.917 - ETA: 0s - loss: 0.1926 - acc: 0.916 - ETA: 0s - loss: 0.1910 - acc: 0.917 - ETA: 0s - loss: 0.1920 - acc: 0.917 - ETA: 0s - loss: 0.1919 - acc: 0.918 - ETA: 0s - loss: 0.1922 - acc: 0.917 - ETA: 0s - loss: 0.1904 - acc: 0.918 - ETA: 0s - loss: 0.1917 - acc: 0.917 - ETA: 0s - loss: 0.1919 - acc: 0.916 - 1s 50us/step - loss: 0.1924 - acc: 0.9164 - val_loss: 0.8984 - val_acc: 0.8231\n",
      "Epoch 491/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.2385 - acc: 0.875 - ETA: 0s - loss: 0.1808 - acc: 0.926 - ETA: 0s - loss: 0.1945 - acc: 0.919 - ETA: 0s - loss: 0.1872 - acc: 0.920 - ETA: 0s - loss: 0.1852 - acc: 0.918 - ETA: 0s - loss: 0.1829 - acc: 0.918 - ETA: 0s - loss: 0.1880 - acc: 0.916 - ETA: 0s - loss: 0.1846 - acc: 0.916 - ETA: 0s - loss: 0.1843 - acc: 0.916 - ETA: 0s - loss: 0.1872 - acc: 0.916 - ETA: 0s - loss: 0.1872 - acc: 0.916 - ETA: 0s - loss: 0.1876 - acc: 0.916 - ETA: 0s - loss: 0.1866 - acc: 0.916 - ETA: 0s - loss: 0.1878 - acc: 0.915 - ETA: 0s - loss: 0.1911 - acc: 0.915 - ETA: 0s - loss: 0.1929 - acc: 0.915 - ETA: 0s - loss: 0.1917 - acc: 0.915 - ETA: 0s - loss: 0.1907 - acc: 0.916 - 1s 51us/step - loss: 0.1919 - acc: 0.9166 - val_loss: 0.9912 - val_acc: 0.8124\n",
      "Epoch 492/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1652 - acc: 0.937 - ETA: 0s - loss: 0.2068 - acc: 0.920 - ETA: 0s - loss: 0.1958 - acc: 0.919 - ETA: 0s - loss: 0.1968 - acc: 0.915 - ETA: 0s - loss: 0.1905 - acc: 0.917 - ETA: 0s - loss: 0.1869 - acc: 0.919 - ETA: 0s - loss: 0.1822 - acc: 0.919 - ETA: 0s - loss: 0.1822 - acc: 0.918 - ETA: 0s - loss: 0.1858 - acc: 0.917 - ETA: 0s - loss: 0.1857 - acc: 0.917 - ETA: 0s - loss: 0.1882 - acc: 0.919 - ETA: 0s - loss: 0.1863 - acc: 0.919 - ETA: 0s - loss: 0.1874 - acc: 0.919 - ETA: 0s - loss: 0.1917 - acc: 0.918 - ETA: 0s - loss: 0.1934 - acc: 0.917 - ETA: 0s - loss: 0.1955 - acc: 0.916 - ETA: 0s - loss: 0.1952 - acc: 0.916 - ETA: 0s - loss: 0.1946 - acc: 0.916 - 1s 49us/step - loss: 0.1950 - acc: 0.9160 - val_loss: 0.8905 - val_acc: 0.8264\n",
      "Epoch 493/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.1729 - acc: 0.921 - ETA: 0s - loss: 0.1543 - acc: 0.932 - ETA: 0s - loss: 0.1782 - acc: 0.922 - ETA: 0s - loss: 0.1850 - acc: 0.921 - ETA: 0s - loss: 0.1884 - acc: 0.920 - ETA: 0s - loss: 0.1918 - acc: 0.919 - ETA: 0s - loss: 0.1886 - acc: 0.921 - ETA: 0s - loss: 0.1940 - acc: 0.919 - ETA: 0s - loss: 0.1912 - acc: 0.920 - ETA: 0s - loss: 0.1902 - acc: 0.921 - ETA: 0s - loss: 0.1922 - acc: 0.919 - ETA: 0s - loss: 0.1940 - acc: 0.919 - ETA: 0s - loss: 0.1967 - acc: 0.918 - ETA: 0s - loss: 0.1961 - acc: 0.917 - ETA: 0s - loss: 0.1942 - acc: 0.918 - ETA: 0s - loss: 0.1936 - acc: 0.918 - ETA: 0s - loss: 0.1928 - acc: 0.917 - ETA: 0s - loss: 0.1937 - acc: 0.917 - 1s 51us/step - loss: 0.1943 - acc: 0.9175 - val_loss: 0.9181 - val_acc: 0.8237\n",
      "Epoch 494/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.2964 - acc: 0.828 - ETA: 0s - loss: 0.1697 - acc: 0.912 - ETA: 0s - loss: 0.1832 - acc: 0.912 - ETA: 0s - loss: 0.1807 - acc: 0.917 - ETA: 0s - loss: 0.1740 - acc: 0.922 - ETA: 0s - loss: 0.1749 - acc: 0.923 - ETA: 0s - loss: 0.1868 - acc: 0.919 - ETA: 0s - loss: 0.1829 - acc: 0.920 - ETA: 0s - loss: 0.1816 - acc: 0.921 - ETA: 0s - loss: 0.1806 - acc: 0.921 - ETA: 0s - loss: 0.1785 - acc: 0.921 - ETA: 0s - loss: 0.1811 - acc: 0.920 - ETA: 0s - loss: 0.1889 - acc: 0.918 - ETA: 0s - loss: 0.1889 - acc: 0.917 - ETA: 0s - loss: 0.1897 - acc: 0.917 - ETA: 0s - loss: 0.1903 - acc: 0.917 - ETA: 0s - loss: 0.1908 - acc: 0.917 - ETA: 0s - loss: 0.1908 - acc: 0.917 - 1s 50us/step - loss: 0.1907 - acc: 0.9177 - val_loss: 0.9386 - val_acc: 0.8246\n",
      "Epoch 495/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.1797 - acc: 0.906 - ETA: 0s - loss: 0.1559 - acc: 0.934 - ETA: 0s - loss: 0.1645 - acc: 0.932 - ETA: 0s - loss: 0.1844 - acc: 0.924 - ETA: 0s - loss: 0.1798 - acc: 0.924 - ETA: 0s - loss: 0.1786 - acc: 0.922 - ETA: 0s - loss: 0.1818 - acc: 0.922 - ETA: 0s - loss: 0.1847 - acc: 0.921 - ETA: 0s - loss: 0.1843 - acc: 0.920 - ETA: 0s - loss: 0.1868 - acc: 0.919 - ETA: 0s - loss: 0.1866 - acc: 0.920 - ETA: 0s - loss: 0.1893 - acc: 0.919 - ETA: 0s - loss: 0.1932 - acc: 0.918 - ETA: 0s - loss: 0.1926 - acc: 0.918 - ETA: 0s - loss: 0.1912 - acc: 0.918 - ETA: 0s - loss: 0.1897 - acc: 0.918 - ETA: 0s - loss: 0.1901 - acc: 0.919 - ETA: 0s - loss: 0.1914 - acc: 0.917 - 1s 49us/step - loss: 0.1918 - acc: 0.9178 - val_loss: 0.9377 - val_acc: 0.8248\n",
      "Epoch 496/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1998 - acc: 0.937 - ETA: 0s - loss: 0.1743 - acc: 0.916 - ETA: 0s - loss: 0.1883 - acc: 0.913 - ETA: 0s - loss: 0.1970 - acc: 0.912 - ETA: 0s - loss: 0.1906 - acc: 0.916 - ETA: 0s - loss: 0.1882 - acc: 0.916 - ETA: 0s - loss: 0.1855 - acc: 0.917 - ETA: 0s - loss: 0.1890 - acc: 0.917 - ETA: 0s - loss: 0.1941 - acc: 0.915 - ETA: 0s - loss: 0.1913 - acc: 0.917 - ETA: 0s - loss: 0.1914 - acc: 0.917 - ETA: 0s - loss: 0.1915 - acc: 0.916 - ETA: 0s - loss: 0.1932 - acc: 0.916 - ETA: 0s - loss: 0.1943 - acc: 0.915 - ETA: 0s - loss: 0.1949 - acc: 0.916 - ETA: 0s - loss: 0.1948 - acc: 0.916 - ETA: 0s - loss: 0.1944 - acc: 0.916 - ETA: 0s - loss: 0.1928 - acc: 0.916 - 1s 49us/step - loss: 0.1928 - acc: 0.9164 - val_loss: 0.9765 - val_acc: 0.8187\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 497/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.1680 - acc: 0.906 - ETA: 0s - loss: 0.1666 - acc: 0.922 - ETA: 0s - loss: 0.1812 - acc: 0.918 - ETA: 0s - loss: 0.1880 - acc: 0.919 - ETA: 0s - loss: 0.1930 - acc: 0.918 - ETA: 0s - loss: 0.1930 - acc: 0.918 - ETA: 0s - loss: 0.1907 - acc: 0.920 - ETA: 0s - loss: 0.1892 - acc: 0.919 - ETA: 0s - loss: 0.1903 - acc: 0.919 - ETA: 0s - loss: 0.1892 - acc: 0.919 - ETA: 0s - loss: 0.1900 - acc: 0.918 - ETA: 0s - loss: 0.1918 - acc: 0.916 - ETA: 0s - loss: 0.1945 - acc: 0.916 - ETA: 0s - loss: 0.1938 - acc: 0.916 - ETA: 0s - loss: 0.1930 - acc: 0.916 - ETA: 0s - loss: 0.1919 - acc: 0.916 - ETA: 0s - loss: 0.1917 - acc: 0.916 - ETA: 0s - loss: 0.1922 - acc: 0.916 - 1s 50us/step - loss: 0.1928 - acc: 0.9160 - val_loss: 0.9605 - val_acc: 0.8219\n",
      "Epoch 498/500\n",
      "20513/20513 [==============================] - ETA: 1s - loss: 0.2155 - acc: 0.906 - ETA: 0s - loss: 0.2086 - acc: 0.912 - ETA: 0s - loss: 0.2051 - acc: 0.917 - ETA: 0s - loss: 0.2009 - acc: 0.915 - ETA: 0s - loss: 0.2042 - acc: 0.913 - ETA: 0s - loss: 0.1953 - acc: 0.916 - ETA: 0s - loss: 0.1923 - acc: 0.918 - ETA: 0s - loss: 0.1912 - acc: 0.917 - ETA: 0s - loss: 0.1950 - acc: 0.918 - ETA: 0s - loss: 0.1911 - acc: 0.919 - ETA: 0s - loss: 0.1888 - acc: 0.920 - ETA: 0s - loss: 0.1879 - acc: 0.919 - ETA: 0s - loss: 0.1887 - acc: 0.918 - ETA: 0s - loss: 0.1930 - acc: 0.917 - ETA: 0s - loss: 0.1928 - acc: 0.917 - ETA: 0s - loss: 0.1916 - acc: 0.917 - ETA: 0s - loss: 0.1911 - acc: 0.917 - ETA: 0s - loss: 0.1913 - acc: 0.917 - 1s 49us/step - loss: 0.1916 - acc: 0.9172 - val_loss: 0.9324 - val_acc: 0.8250\n",
      "Epoch 499/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.4018 - acc: 0.906 - ETA: 0s - loss: 0.1943 - acc: 0.915 - ETA: 0s - loss: 0.1940 - acc: 0.922 - ETA: 0s - loss: 0.1964 - acc: 0.920 - ETA: 0s - loss: 0.1953 - acc: 0.921 - ETA: 0s - loss: 0.1892 - acc: 0.923 - ETA: 0s - loss: 0.1951 - acc: 0.916 - ETA: 0s - loss: 0.1999 - acc: 0.917 - ETA: 0s - loss: 0.1973 - acc: 0.917 - ETA: 0s - loss: 0.1943 - acc: 0.918 - ETA: 0s - loss: 0.1978 - acc: 0.917 - ETA: 0s - loss: 0.1984 - acc: 0.916 - ETA: 0s - loss: 0.1963 - acc: 0.917 - ETA: 0s - loss: 0.1988 - acc: 0.917 - ETA: 0s - loss: 0.1994 - acc: 0.917 - ETA: 0s - loss: 0.1977 - acc: 0.917 - ETA: 0s - loss: 0.1956 - acc: 0.917 - ETA: 0s - loss: 0.1951 - acc: 0.918 - 1s 50us/step - loss: 0.1955 - acc: 0.9184 - val_loss: 0.9444 - val_acc: 0.8236\n",
      "Epoch 500/500\n",
      "20513/20513 [==============================] - ETA: 0s - loss: 0.2334 - acc: 0.921 - ETA: 0s - loss: 0.1919 - acc: 0.920 - ETA: 0s - loss: 0.1863 - acc: 0.925 - ETA: 0s - loss: 0.1864 - acc: 0.922 - ETA: 0s - loss: 0.1824 - acc: 0.923 - ETA: 0s - loss: 0.1876 - acc: 0.921 - ETA: 0s - loss: 0.1854 - acc: 0.921 - ETA: 0s - loss: 0.1895 - acc: 0.919 - ETA: 0s - loss: 0.1926 - acc: 0.918 - ETA: 0s - loss: 0.1913 - acc: 0.918 - ETA: 0s - loss: 0.1969 - acc: 0.917 - ETA: 0s - loss: 0.2000 - acc: 0.917 - ETA: 0s - loss: 0.2029 - acc: 0.916 - ETA: 0s - loss: 0.2014 - acc: 0.915 - ETA: 0s - loss: 0.1997 - acc: 0.916 - ETA: 0s - loss: 0.1966 - acc: 0.917 - ETA: 0s - loss: 0.1968 - acc: 0.917 - ETA: 0s - loss: 0.2006 - acc: 0.916 - 1s 50us/step - loss: 0.1994 - acc: 0.9167 - val_loss: 0.9382 - val_acc: 0.8239\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(X_td, y_td, batch_size=64, epochs=500, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "acc = hist.history['acc']\n",
    "val_acc = hist.history['val_acc']\n",
    "loss = hist.history['loss']\n",
    "val_loss = hist.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "# ‘bo’는 파란색 점을 의미합니다\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "# ‘b’는 파란색 실선을 의미합니다\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()   # 그래프를 초기화합니다\n",
    "acc = hist.history['acc']\n",
    "val_acc = hist.history['val_acc']\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 23444 samples, validate on 5861 samples\n",
      "Epoch 1/4\n",
      "23444/23444 [==============================] - 1s 36us/step - loss: 0.4846 - acc: 0.8058 - val_loss: 0.3553 - val_acc: 0.8377\n",
      "Epoch 2/4\n",
      "23444/23444 [==============================] - 0s 6us/step - loss: 0.3401 - acc: 0.8393 - val_loss: 0.3318 - val_acc: 0.8435\n",
      "Epoch 3/4\n",
      "23444/23444 [==============================] - 0s 6us/step - loss: 0.3283 - acc: 0.8447 - val_loss: 0.3252 - val_acc: 0.8470\n",
      "Epoch 4/4\n",
      "23444/23444 [==============================] - 0s 6us/step - loss: 0.3238 - acc: 0.8477 - val_loss: 0.3221 - val_acc: 0.8495\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2008ea40b70>"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, kernel_initializer='normal', activation='relu', input_shape=(11,)))\n",
    "model.add(layers.Dense(32, kernel_initializer='normal', activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer ='rmsprop',loss='binary_crossentropy', metrics =['accuracy'])\n",
    "model.fit(X_td, y_td, batch_size=512, epochs=4, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 29305 entries, 0 to 29304\n",
      "Data columns (total 20 columns):\n",
      "age                            29305 non-null int64\n",
      "education-num                  29305 non-null int64\n",
      "marital-status                 29305 non-null int64\n",
      "relationship                   29305 non-null int64\n",
      "capital-gain                   29305 non-null int64\n",
      "capital-loss                   29305 non-null int64\n",
      "hours-per-week                 29305 non-null int64\n",
      "workclass_ ?                   29305 non-null uint8\n",
      "workclass_ Federal-gov         29305 non-null uint8\n",
      "workclass_ Local-gov           29305 non-null uint8\n",
      "workclass_ Never-worked        29305 non-null uint8\n",
      "workclass_ Private             29305 non-null uint8\n",
      "workclass_ Self-emp-inc        29305 non-null uint8\n",
      "workclass_ Self-emp-not-inc    29305 non-null uint8\n",
      "workclass_ State-gov           29305 non-null uint8\n",
      "workclass_ Without-pay         29305 non-null uint8\n",
      "occupation_ Exec-managerial    29305 non-null uint8\n",
      "occupation_ Prof-specialty     29305 non-null uint8\n",
      "sex_ Female                    29305 non-null uint8\n",
      "sex_ Male                      29305 non-null uint8\n",
      "dtypes: int64(7), uint8(13)\n",
      "memory usage: 2.2 MB\n"
     ]
    }
   ],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=KFold(n_splits=6, random_state=1, shuffle=False),\n",
       "             error_score='raise-deprecating',\n",
       "             estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None,\n",
       "                                      colsample_bytree=1.0,\n",
       "                                      importance_type='split',\n",
       "                                      learning_rate=0.1, max_depth=-1,\n",
       "                                      min_child_samples=20,\n",
       "                                      min_child_weight=0.001,\n",
       "                                      min_split_gain=0.0, n_estimators=100,\n",
       "                                      n_jobs=-1, num_leaves=31, objective=N...\n",
       "                                      random_state=None, reg_alpha=0.0,\n",
       "                                      reg_lambda=0.0, silent=True,\n",
       "                                      subsample=1.0, subsample_for_bin=200000,\n",
       "                                      subsample_freq=0),\n",
       "             iid='warn', n_jobs=-1,\n",
       "             param_grid={'learning_rate': [0.1], 'max_depth': [6],\n",
       "                         'min_child_samples': [0], 'min_data_in_leaf': [40],\n",
       "                         'n_estimators': [200], 'num_leaves': [63],\n",
       "                         'reg_alpha': [0.05]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='f1', verbose=0)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lgbm = LGBMClassifier()\n",
    "param_grid = {\n",
    "    'learning_rate': [0.1],\n",
    "    'num_leaves': [63],\n",
    "    'max_depth': [6],\n",
    "    'reg_alpha': [0.05],\n",
    "    'min_child_samples': [0],\n",
    "    'min_data_in_leaf': [40],\n",
    "    'n_estimators':[200]\n",
    "    }\n",
    "cv=KFold(n_splits=6, random_state=1)\n",
    "gcv=GridSearchCV(model_lgbm, param_grid=param_grid, cv=cv, scoring='f1', n_jobs=-1)\n",
    "\n",
    "gcv.fit(X_tr, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
       "               importance_type='split', learning_rate=0.1, max_depth=6,\n",
       "               min_child_samples=0, min_child_weight=0.001, min_data_in_leaf=40,\n",
       "               min_split_gain=0.0, n_estimators=200, n_jobs=-1, num_leaves=63,\n",
       "               objective=None, random_state=None, reg_alpha=0.05,\n",
       "               reg_lambda=0.0, silent=True, subsample=1.0,\n",
       "               subsample_for_bin=200000, subsample_freq=0)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gcv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lgbm = gcv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8672922659768806"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(model_lgbm, X_tr, y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds.\n",
      "[1000]\tvalid_0's binary_logloss: 0.300157\n",
      "[2000]\tvalid_0's binary_logloss: 0.295805\n",
      "[3000]\tvalid_0's binary_logloss: 0.29371\n",
      "[4000]\tvalid_0's binary_logloss: 0.292434\n",
      "[5000]\tvalid_0's binary_logloss: 0.291526\n",
      "[6000]\tvalid_0's binary_logloss: 0.290836\n",
      "[7000]\tvalid_0's binary_logloss: 0.290254\n",
      "Early stopping, best iteration is:\n",
      "[7112]\tvalid_0's binary_logloss: 0.290189\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"max_bin\": 512,\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"objective\": \"binary\",\n",
    "    \"metric\": \"binary_logloss\",\n",
    "    \"num_leaves\": 2,\n",
    "    \"verbose\": -1,\n",
    "    \"min_data\": 100,\n",
    "    \"boost_from_average\": True\n",
    "}\n",
    "\n",
    "model_ind = lgb.train(params, d_train, 20000, valid_sets=[d_test], early_stopping_rounds=50, verbose_eval=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vecstack import stacking\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_tr, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23444, 98)\n",
      "(5861, 98)\n",
      "(23444,)\n",
      "(5861,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "models = [ ExtraTreesClassifier(random_state = 0, n_jobs = -1, n_estimators = 300, max_depth = 17), \n",
    "          RandomForestClassifier(criterion='gini', max_depth=13, max_features='auto', n_estimators=500), \n",
    "          XGBClassifier(booster='gbtree', colsample_bylevel=1, colsample_bytree=0.5, gamma=0, max_depth=5, min_child_weight=1, n_estimators=500, objective='binary:logistic',random_state=42, silent=True),\n",
    "          LogisticRegression(random_state=0, C=100.0, penalty='l2'),\n",
    "          AdaBoostClassifier(random_state=0, n_estimators=32),\n",
    "         ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task:         [classification]\n",
      "n_classes:    [2]\n",
      "metric:       [accuracy_score]\n",
      "mode:         [oof_pred_bag]\n",
      "n_models:     [5]\n",
      "\n",
      "model  0:     [ExtraTreesClassifier]\n",
      "    fold  0:  [0.83589219]\n",
      "    fold  1:  [0.84473639]\n",
      "    fold  2:  [0.84064153]\n",
      "    fold  3:  [0.83907850]\n",
      "    ----\n",
      "    MEAN:     [0.84008715] + [0.00318347]\n",
      "    FULL:     [0.84008702]\n",
      "\n",
      "model  1:     [RandomForestClassifier]\n",
      "    fold  0:  [0.85039236]\n",
      "    fold  1:  [0.85719161]\n",
      "    fold  2:  [0.86213957]\n",
      "    fold  3:  [0.85836177]\n",
      "    ----\n",
      "    MEAN:     [0.85702133] + [0.00424162]\n",
      "    FULL:     [0.85702099]\n",
      "\n",
      "model  2:     [XGBClassifier]\n",
      "    fold  0:  [0.86438076]\n",
      "    fold  1:  [0.86759939]\n",
      "    fold  2:  [0.87032930]\n",
      "    fold  3:  [0.86945392]\n",
      "    ----\n",
      "    MEAN:     [0.86794084] + [0.00227952]\n",
      "    FULL:     [0.86794062]\n",
      "\n",
      "model  3:     [LogisticRegression]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    fold  0:  [0.84425111]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    fold  1:  [0.84814878]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    fold  2:  [0.84763692]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    fold  3:  [0.84641638]\n",
      "    ----\n",
      "    MEAN:     [0.84661330] + [0.00150202]\n",
      "    FULL:     [0.84661321]\n",
      "\n",
      "model  4:     [AdaBoostClassifier]\n",
      "    fold  0:  [0.84749232]\n",
      "    fold  1:  [0.84866064]\n",
      "    fold  2:  [0.85753284]\n",
      "    fold  3:  [0.85597270]\n",
      "    ----\n",
      "    MEAN:     [0.85241463] + [0.00439254]\n",
      "    FULL:     [0.85241426]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "S_train, S_test = stacking(models, X_train, y_train, X_test,\n",
    "                           regression = False, metric = accuracy_score, n_folds = 4,\n",
    "                           stratified = True, shuffle = True, random_state = 0, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c152aff5941f4fe6b6cbbe0902ef6aea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "score_list = []\n",
    "lr_list = [0.6]\n",
    "est_list = [20]\n",
    "metric_list = ['l1', 'l2']\n",
    "for lr in tqdm_notebook(lr_list):\n",
    "    for est in est_list:\n",
    "        for mt in metric_list:\n",
    "            model = LGBMClassifier(seed = 0, metric=mt, learning_rate=lr, n_estimators=est, num_leaves = 38)\n",
    "            model = model.fit(S_train, y_train)\n",
    "            score = cross_val_score(model, S_test, y_test, cv=5).mean()\n",
    "            score_list.append([lr, est, mt, score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.6</td>\n",
       "      <td>20</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.868280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.6</td>\n",
       "      <td>20</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.868280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.6</td>\n",
       "      <td>30</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.868109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.6</td>\n",
       "      <td>30</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.868109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.6</td>\n",
       "      <td>10</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.867086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.6</td>\n",
       "      <td>10</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.867086</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1   2         3\n",
       "2  0.6  20  l1  0.868280\n",
       "3  0.6  20  l2  0.868280\n",
       "4  0.6  30  l1  0.868109\n",
       "5  0.6  30  l2  0.868109\n",
       "0  0.6  10  l1  0.867086\n",
       "1  0.6  10  l2  0.867086"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(score_list).sort_values(by=3, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task:         [classification]\n",
      "n_classes:    [2]\n",
      "metric:       [accuracy_score]\n",
      "mode:         [oof_pred_bag]\n",
      "n_models:     [5]\n",
      "\n",
      "model  0:     [ExtraTreesClassifier]\n",
      "    fold  0:  [0.83589219]\n",
      "    fold  1:  [0.84473639]\n",
      "    fold  2:  [0.84064153]\n",
      "    fold  3:  [0.83907850]\n",
      "    ----\n",
      "    MEAN:     [0.84008715] + [0.00318347]\n",
      "    FULL:     [0.84008702]\n",
      "\n",
      "model  1:     [RandomForestClassifier]\n",
      "    fold  0:  [0.85022177]\n",
      "    fold  1:  [0.85975090]\n",
      "    fold  2:  [0.86265142]\n",
      "    fold  3:  [0.85819113]\n",
      "    ----\n",
      "    MEAN:     [0.85770380] + [0.00460673]\n",
      "    FULL:     [0.85770346]\n",
      "\n",
      "model  2:     [XGBClassifier]\n",
      "    fold  0:  [0.86438076]\n",
      "    fold  1:  [0.86759939]\n",
      "    fold  2:  [0.87032930]\n",
      "    fold  3:  [0.86945392]\n",
      "    ----\n",
      "    MEAN:     [0.86794084] + [0.00227952]\n",
      "    FULL:     [0.86794062]\n",
      "\n",
      "model  3:     [LogisticRegression]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    fold  0:  [0.84425111]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    fold  1:  [0.84814878]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    fold  2:  [0.84763692]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bigBro\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    fold  3:  [0.84641638]\n",
      "    ----\n",
      "    MEAN:     [0.84661330] + [0.00150202]\n",
      "    FULL:     [0.84661321]\n",
      "\n",
      "model  4:     [AdaBoostClassifier]\n",
      "    fold  0:  [0.84749232]\n",
      "    fold  1:  [0.84866064]\n",
      "    fold  2:  [0.85753284]\n",
      "    fold  3:  [0.85597270]\n",
      "    ----\n",
      "    MEAN:     [0.85241463] + [0.00439254]\n",
      "    FULL:     [0.85241426]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "S_train, S_test = stacking(models, X_train, y_train, X_te,\n",
    "                           regression = False, metric = accuracy_score, n_folds = 4,\n",
    "                           stratified = True, shuffle = True, random_state = 0, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBClassifier(seed = 0, n_jobs = -1, learning_rate = 0.01, n_estimators = 400, max_depth = 2)\n",
    "model = model.fit(S_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LGBMClassifier(learning_rate = 0.6,metric = 'l1', n_estimators = 20, num_leaves = 38)\n",
    "model = model.fit(S_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(S_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 1, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    15725\n",
       "True      3812\n",
       "Name: income, dtype: int64"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model_lgbm.predict(X_te)\n",
    "y_pred = (y_pred > 0.5)\n",
    "y_pred = pd.DataFrame(y_pred, columns=['income'])\n",
    "y_pred.iloc[:, 0].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    15725\n",
       "True      3812\n",
       "Name: income, dtype: int64"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = pd.DataFrame(y_pred, columns=['income'])\n",
    "y_pred.iloc[:, 0].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if nessesary\n",
    "#y_pred = pd.DataFrame(y_pred.iloc[:,0], columns=['income'])\n",
    "y_pred['income'].replace(True, 1, inplace=True)\n",
    "y_pred['income'].replace(False, 0, inplace=True)\n",
    "y_pred = y_pred.astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.concat([pd.DataFrame(list(range(29306, 48843)), columns=['no']), y_pred], axis=1)\n",
    "result.to_csv('./submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
